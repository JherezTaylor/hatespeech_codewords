{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a dependency2vec model\n",
    "\n",
    "This notebook serves as a tutorial for creating a dependency2vec embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Introduction can be found at IDEA NAS/public/Presentation Slide/Senior Meeting/Dependency2vec.pdf\n",
    "- Syntactic Parser: I recommend Spacy https://spacy.io/\n",
    "- dependency2vec Paper: \"Dependency-Based Word Embeddings\", Omer Levy and Yoav Goldberg, 2014. *Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics*\n",
    "- dependency2vec source: https://bitbucket.org/yoavgo/word2vecf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jumping in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data format required to make a dep2vec embedding differs from traditional word embeddings.\n",
    "\n",
    "Whereas word2vec or fasttext expects one text sample per line:\n",
    "- user_mention user_mention user_mention i care obviously that's i'm making comment \n",
    "- he turn roadblocks super highways :) \n",
    "- what need know count me text #womensmarch\n",
    "\n",
    "dependency2vec expects data in [CoNLL-U Format](http://universaldependencies.org/format.html):\n",
    "\n",
    "Each line represents the format specified in CoNLL-U. The file format keeps track of the positions of each token in a given text sample and stores the required data for each token until we get to the end of that text sample. Each sample is separated by a new line. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1\tthen\tthen\tADV\tRB\t_\t2\tadvmod\t2:advmod\t_\n",
    "2\tgave\tgive\tVERB\tVBD\t_\t0\tROOT\t0:ROOT\t_\n",
    "3\tthe\tthe\tDET\tDT\t_\t5\tdet\t5:det\t_\n",
    "4\tole\tole\tADJ\tJJ\t_\t5\tamod\t5:amod\t_\n",
    "5\tuncle\tuncle\tNOUN\tNN\t_\t2\tdobj\t2:dobj\t_\n",
    "6\tcharlie\tcharlie\tPROPN\tNNP\t_\t5\tappos\t5:appos\t_\n",
    "7\tto\tto\tPART\tTO\t_\t8\taux\t8:aux\t_\n",
    "8\tmr\tmr\tVERB\tVB\t_\t5\trelcl\t5:relcl\t_\n",
    "9\tortiz\tortiz\tPROPN\tNNP\t_\t11\tcompound\t11:compound\t_\n",
    "10\t#nyy\t#nyy\tPROPN\tNNP\t_\t11\tcompound\t11:compound\t_\n",
    "11\tuser_mention\tuser_mention\tNOUN\tNN\t_\t8\tdobj\t8:dobj\t_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That looks complex, ain't nobody got time for dat\n",
    "\n",
    "Never fear, we have the technology. The full code is in my repository, I just extracted the key parts for brevity. This isn't tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def init_nlp_pipeline(parser, tokenizer=CustomTwokenizer):\n",
    "    \"\"\"Initialize spaCy nlp pipeline\n",
    "    The params are boolean values that determine if that feature should\n",
    "    be loaded with the pipeline.\n",
    "\n",
    "    Returns:\n",
    "        nlp: spaCy language model\n",
    "    \"\"\"\n",
    "    if parser is False:\n",
    "        nlp = spacy.load(settings.SPACY_EN_MODEL, create_make_doc=tokenizer,\n",
    "                         parser=False)\n",
    "    else:\n",
    "        nlp = spacy.load(settings.SPACY_EN_MODEL,\n",
    "                         create_make_doc=tokenizer)\n",
    "    return nlp\n",
    "\n",
    "\n",
    "def extract_lexical_features_test(nlp, tweet_list):\n",
    "    \"\"\"Provides tokenization, POS and dependency parsing\n",
    "    Args:\n",
    "        nlp  (spaCy model): Language processing pipeline\n",
    "    \"\"\"\n",
    "    staging = []\n",
    "    \n",
    "    docs = nlp.pipe(tweet_texts, batch_size=15000, n_threads=4)\n",
    "    for object_id, doc in zip(object_ids, docs):\n",
    "        parsed_doc = extract_conll_format(doc)\n",
    "        staging.append(parsed_doc)\n",
    "        \n",
    "        ## Send the docs somewhere in order to write to disk later\n",
    "    return staging\n",
    "\n",
    "\n",
    "def extract_conll_format(doc):\n",
    "    \"\"\"Return the document in CoNLL format\n",
    "    Args:\n",
    "        doc (spaCy Doc): A container for accessing linguistic annotations.\n",
    "    Returns:\n",
    "        result: List of lines storing the CoNLL format for each word in a sentence.\n",
    "    \"\"\"\n",
    "    # https://github.com/explosion/spaCy/issues/533#issuecomment-254774296\n",
    "    # http://universaldependencies.org/docs/format.html\n",
    "    result = []\n",
    "    conll = []\n",
    "    for sent in doc.sents:\n",
    "        for i, word in enumerate(sent):\n",
    "            if word.head is word:\n",
    "                head_idx = 0\n",
    "            else:\n",
    "                head_idx = word.head.i + 1\n",
    "            conll.extend((i + 1, word.lower_, word.lemma_, word.pos_, word.tag_,\n",
    "                          \"_\", head_idx, word.dep_, str(head_idx) + \":\" + word.dep_, \"_\"))\n",
    "            result.append(\"\\t\".join(str(x) for x in conll))\n",
    "            conll = []\n",
    "    return result\n",
    "\n",
    "def prep_conll_file(collection, filename):\n",
    "    \"\"\" Takes a MongoDB or other cursor\n",
    "    and writes the coNLL data to a text file.\n",
    "    Args:\n",
    "        collection (iterable)\n",
    "        filename (str)\n",
    "    \"\"\"\n",
    "\n",
    "    count = 0\n",
    "    with open(settings.CONLL_PATH + filename, \"a+\") as _f:\n",
    "        for doc in collection:\n",
    "            count += 1\n",
    "            for entry in doc[\"conllFormat\"]:\n",
    "                _f.write(entry + \"\\n\")\n",
    "            _f.write(\"\\n\")\n",
    "            settings.logger.debug(\"Count %s\", count)\n",
    "        _f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model\n",
    "\n",
    "The following function calls the underlying C++ code. The full instructions are in the readme for the dep2vec source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def dep2vec_model(input_data, filename, filter_count, min_count, dimensions):\n",
    "    \"\"\" Train a dependency2vec model.\n",
    "    Follows the same steps outlined in the dependency2vec readme\n",
    "    \"\"\"\n",
    "    time1 = notifiers.time()\n",
    "    # Step 1\n",
    "    output = subprocess.run(\"cut -f 2 hatespeech_core/data/conll_data/\" + input_data + \" | python hatespeech_core/data/conll_data/dependency2vec/scripts/vocab.py \" + str(\n",
    "        filter_count) + \" > \" + \"hatespeech_core/data/conll_data/dependency2vec/vocab_data/counted_vocabulary_\" + input_data, shell=True, check=True, stdout=subprocess.PIPE)\n",
    "    for line in output.stdout.splitlines():\n",
    "        print(line)\n",
    "    print(output)\n",
    "\n",
    "    # Step 2\n",
    "    output = subprocess.run(\"cat hatespeech_core/data/conll_data/\" + input_data + \" | python hatespeech_core/data/conll_data/dependency2vec/scripts/extract_deps.py hatespeech_core/data/conll_data/dependency2vec/vocab_data/counted_vocabulary_\" +\n",
    "                            input_data + \" \" + str(filter_count) + \" > \" + \"hatespeech_core/data/conll_data/dependency2vec/vocab_data/dep.contexts_\" + input_data, shell=True, check=True, stdout=subprocess.PIPE)\n",
    "    for line in output.stdout.splitlines():\n",
    "        print(line)\n",
    "\n",
    "    # Step 3\n",
    "    output = subprocess.run(\"hatespeech_core/data/conll_data/dependency2vec/\" + \"./count_and_filter -train \" + \"hatespeech_core/data/conll_data/dependency2vec/vocab_data/dep.contexts_\" + input_data + \" -cvocab \" +\n",
    "                            \"hatespeech_core/data/conll_data/dependency2vec/vocab_data/cv_\" + input_data + \" -wvocab \" + \"hatespeech_core/data/conll_data/dependency2vec/vocab_data/wv_\" + input_data + \" -min-count \" + str(min_count), shell=True, check=True, stdout=subprocess.PIPE)\n",
    "    for line in output.stdout.splitlines():\n",
    "        print(line)\n",
    "\n",
    "    # Step 4\n",
    "    output = subprocess.run(\"hatespeech_core/data/conll_data/dependency2vec/\" + \"./word2vecf -train \" + \"hatespeech_core/data/conll_data/dependency2vec/vocab_data/dep.contexts_\" + input_data + \" -cvocab \" +\n",
    "                            \"hatespeech_core/data/conll_data/dependency2vec/vocab_data/cv_\" + input_data + \" -wvocab \" + \"hatespeech_core/data/conll_data/dependency2vec/vocab_data/wv_\" + input_data + \" -size \" + str(dimensions) + \" -negative 15 -threads 10 -output hatespeech_core/data/persistence/word_embeddings/dim\" + str(dimensions) + \"vecs_\" + filename, shell=True, check=True, stdout=subprocess.PIPE)\n",
    "    for line in output.stdout.splitlines():\n",
    "        print(line)\n",
    "\n",
    "    time2 = notifiers.time()\n",
    "    notifiers.send_job_completion(\n",
    "        [time1, time2], [\"dependency2vec\", \"dependency2vec \" + filename])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from modules.preprocessing import neural_embeddings\n",
    "from modules.utils import file_ops, model_helpers, settings\n",
    "import glob\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embeddings(embedding_type, model_ids=None, load=False):\n",
    "    \"\"\" Helper function for loading embedding models\n",
    "    Args:\n",
    "        embedding_type (str): dep2vec,ft:keyedVectors w2v:word2vec\n",
    "        model_ids (list): List of ints referencing the models.\n",
    "    \"\"\"\n",
    "\n",
    "    model_format = \"kv\" if embedding_type == \"dep2vec\" or embedding_type == \"ft\" else \"w2v\"\n",
    "\n",
    "    if model_ids and load:\n",
    "        if embedding_type == \"dep2vec\":\n",
    "            embeddings_ref = sorted(file_ops.get_model_names(\n",
    "                glob.glob(settings.EMBEDDING_MODELS + \"dim*\")))\n",
    "        elif embedding_type == \"ft\":\n",
    "            embeddings_ref = sorted(file_ops.get_model_names(\n",
    "                glob.glob(settings.EMBEDDING_MODELS + \"*.vec\")))\n",
    "        elif embedding_type == \"w2v\":\n",
    "            embeddings_ref = sorted(file_ops.get_model_names(\n",
    "                glob.glob(settings.EMBEDDING_MODELS + \"word2vec_*\")))\n",
    "\n",
    "        for idx, ref in enumerate(embeddings_ref):\n",
    "            print(idx, ref)\n",
    "\n",
    "        loaded_models = []\n",
    "        for idx in model_ids:\n",
    "            loaded_models.append(load_embedding(\n",
    "                embeddings_ref[idx], model_format))\n",
    "        return loaded_models\n",
    "    else:\n",
    "        print(\"Embedding models not loaded\")\n",
    "\n",
    "def load_embedding(filename, embedding_type):\n",
    "    \"\"\" Load a fasttext or word2vec embedding\n",
    "    Args:\n",
    "        filename (str)\n",
    "        embedding_type (str): kv:keyedVectors w2v:word2vec\n",
    "    \"\"\"\n",
    "    if embedding_type == \"kv\":\n",
    "        return KeyedVectors.load_word2vec_format(settings.EMBEDDING_MODELS + filename, binary=False, unicode_errors=\"ignore\")\n",
    "    elif embedding_type == \"w2v\":\n",
    "        model = Word2Vec.load(settings.EMBEDDING_MODELS + filename)\n",
    "        word_vectors = model.wv\n",
    "        del model\n",
    "        return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 dim200vecs_core_combined_corpus\n",
      "1 dim200vecs_core_hate_corpus\n",
      "2 dim200vecs_core_tweets_clean\n",
      "3 dim200vecs_core_tweets_hs_keyword\n",
      "4 dim200vecs_dstormer_conll\n",
      "5 dim200vecs_inaug_conll\n",
      "6 dim200vecs_manch_conll\n",
      "7 dim200vecs_melvynhs_conll\n",
      "8 dim200vecs_twitter_conll\n",
      "9 dim200vecs_uselec_conll\n",
      "10 dim200vecs_ustream_conll\n"
     ]
    }
   ],
   "source": [
    "dep_model_ids = [8,4]\n",
    "loaded_embeddings = get_embeddings(\"dep2vec\", model_ids=dep_model_ids, load=True)\n",
    "dep_embeddings = {}\n",
    "dep_embeddings[\"twitter\"] = loaded_embeddings[0]\n",
    "dep_embeddings[\"dstormer\"] = loaded_embeddings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('blazer', 0.9450125694274902),\n",
      " ('windbreaker', 0.937454879283905),\n",
      " ('linen', 0.930109977722168),\n",
      " ('#shirt', 0.9292298555374146),\n",
      " ('hoody', 0.9283322095870972)]\n",
      "\n",
      "[('physician', 0.9836385846138),\n",
      " ('preacher', 0.9834705591201782),\n",
      " ('cleric', 0.9810529947280884),\n",
      " ('pensioner', 0.978823721408844),\n",
      " ('actress', 0.9784078001976013)]\n"
     ]
    }
   ],
   "source": [
    "target_word = \"bomber\"\n",
    "pprint(dep_embeddings[\"twitter\"].similar_by_word(target_word, topn=5))\n",
    "print()\n",
    "pprint(dep_embeddings[\"dstormer\"].similar_by_word(target_word, topn=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
