{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "\n",
    "This is a staging notebook for experiments related to the classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import models, similarities\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import joblib\n",
    "from modules.db import mongo_base\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import FeatureHasher, DictVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from modules.utils.CustomTwokenizer import CustomTwokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store collection as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_as_df(connection_params, projection):\n",
    "    client = mongo_base.connect()\n",
    "    db_name = connection_params[0]\n",
    "    connection_params.insert(0, client)\n",
    "    query = {}\n",
    "    query[\"filter\"] = {}\n",
    "    query[\"projection\"] = projection\n",
    "    query[\"limit\"] = 0\n",
    "    query[\"skip\"] = 0\n",
    "    query[\"no_cursor_timeout\"] = True\n",
    "    cursor = mongo_base.finder(connection_params, query, False)\n",
    "    df = pd.DataFrame(list(cursor))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge dataframes [CrowdFlower Dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connection_params_1 = [\"twitter\", \"crowdflower_features\"]\n",
    "connection_params_2 = [\"twitter\", \"crowdflower_features_emo\"]\n",
    "# df = fetch_as_df(connection_params_1, {})\n",
    "# df_emo = fetch_as_df(connection_params_2, {\"emotions\":1})\n",
    "# df = pd.DataFrame.merge(df, df_emo, on=\"_id\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle the raw feature collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacy_en_model = \"en_core_web_md\"\n",
    "spacy_glove_model = \"en_vectors_glove_md\"\n",
    "crowdflower_persistence_raw = 'data/persistence/df/crowdflower_features_raw.pkl.compressed'\n",
    "crowdflower_persistence = 'data/persistence/df/crowdflower_features.pkl.compressed'\n",
    "naacl_2016_persistence = 'data/persistence/df/naacl_2016.pkl.compressed'\n",
    "nlp_2016_persistence = 'data/persistence/df/nlp_2016.pkl.compressed'\n",
    "nlp = spacy.load(spacy_en_model, create_make_doc=CustomTwokenizer)\n",
    "# joblib.dump(df, crowdflower_persistence_raw, compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = joblib.load(crowdflower_persistence_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe with classifier features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feat_df = df[['_id', 'text', 'annotation_label', 'hs_keyword_matches', 'hs_keyword_count', 'unknown_words', 'unknown_words_count', 'comment_length', 'brown_cluster_ids', 'feat_dependency_contexts', 'feat_word_dep_root', 'feat_pos_dep_rootPos', 'feat_word_root_rootparent', 'feat_dep_unigrams', 'feat_dep_bigrams', 'feat_dep_trigrams']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>text</th>\n",
       "      <th>annotation_label</th>\n",
       "      <th>hs_keyword_matches</th>\n",
       "      <th>hs_keyword_count</th>\n",
       "      <th>unknown_words</th>\n",
       "      <th>unknown_words_count</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>brown_cluster_ids</th>\n",
       "      <th>feat_dependency_contexts</th>\n",
       "      <th>feat_word_dep_root</th>\n",
       "      <th>feat_pos_dep_rootPos</th>\n",
       "      <th>feat_word_root_rootparent</th>\n",
       "      <th>feat_dep_unigrams</th>\n",
       "      <th>feat_dep_bigrams</th>\n",
       "      <th>feat_dep_trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58c659be6541913eb7f119dd</td>\n",
       "      <td>Warning : penny boards will make you a faggot</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>[faggot]</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>[966, 228, 442, 4618, 602, 19]</td>\n",
       "      <td>[warning_:_punct, warning_make_acl, penny boar...</td>\n",
       "      <td>[warning_ROOT_warning, penny boards_nsubj_make...</td>\n",
       "      <td>[NN_ROOT_NN, NNS_nsubj_VB, MD_aux_VB, VB_acl_N...</td>\n",
       "      <td>[warning_warning_warning, penny boards_make_wa...</td>\n",
       "      <td>[warning_warning_ROOT_NN, penny boards_make_ns...</td>\n",
       "      <td>[warning_warning_ROOT_NN|penny boards_make_nsu...</td>\n",
       "      <td>[warning_warning_ROOT_NN|penny boards_make_nsu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58c659be6541913eb7f119de</td>\n",
       "      <td>Fuck dykes</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>[fuck_dykes_compoundINV, dykes_fuck_compound]</td>\n",
       "      <td>[fuck_compound_dykes, dykes_ROOT_dykes]</td>\n",
       "      <td>[NNP_compound_VBZ, VBZ_ROOT_VBZ]</td>\n",
       "      <td>[fuck_dykes_dykes, dykes_dykes_dykes]</td>\n",
       "      <td>[fuck_dykes_compound_NNP, dykes_dykes_ROOT_VBZ]</td>\n",
       "      <td>[fuck_dykes_compound_NNP|dykes_dykes_ROOT_VBZ]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58c659be6541913eb7f119df</td>\n",
       "      <td>user_mention user_mention user_mention user_me...</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[faggot]</td>\n",
       "      <td>1</td>\n",
       "      <td>[jefree]</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>[124, 3690, 966, 2442, 1684]</td>\n",
       "      <td>[user_mention user_mention user_mention user_m...</td>\n",
       "      <td>[user_mention user_mention user_mention user_m...</td>\n",
       "      <td>[NN_ROOT_NN, IN_advmod_JJS, JJS_advmod_VBP, PR...</td>\n",
       "      <td>[user_mention user_mention user_mention user_m...</td>\n",
       "      <td>[user_mention user_mention user_mention user_m...</td>\n",
       "      <td>[user_mention user_mention user_mention user_m...</td>\n",
       "      <td>[user_mention user_mention user_mention user_m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  58c659be6541913eb7f119dd   \n",
       "1  58c659be6541913eb7f119de   \n",
       "2  58c659be6541913eb7f119df   \n",
       "\n",
       "                                                text annotation_label  \\\n",
       "0     Warning : penny boards will make you a faggot     not_offensive   \n",
       "1                                        Fuck dykes        hatespeech   \n",
       "2  user_mention user_mention user_mention user_me...       hatespeech   \n",
       "\n",
       "  hs_keyword_matches  hs_keyword_count unknown_words  unknown_words_count  \\\n",
       "0           [faggot]                 1            []                    0   \n",
       "1                 []                 0            []                    0   \n",
       "2           [faggot]                 1      [jefree]                    1   \n",
       "\n",
       "   comment_length               brown_cluster_ids  \\\n",
       "0               9  [966, 228, 442, 4618, 602, 19]   \n",
       "1               2                              []   \n",
       "2              14    [124, 3690, 966, 2442, 1684]   \n",
       "\n",
       "                            feat_dependency_contexts  \\\n",
       "0  [warning_:_punct, warning_make_acl, penny boar...   \n",
       "1      [fuck_dykes_compoundINV, dykes_fuck_compound]   \n",
       "2  [user_mention user_mention user_mention user_m...   \n",
       "\n",
       "                                  feat_word_dep_root  \\\n",
       "0  [warning_ROOT_warning, penny boards_nsubj_make...   \n",
       "1            [fuck_compound_dykes, dykes_ROOT_dykes]   \n",
       "2  [user_mention user_mention user_mention user_m...   \n",
       "\n",
       "                                feat_pos_dep_rootPos  \\\n",
       "0  [NN_ROOT_NN, NNS_nsubj_VB, MD_aux_VB, VB_acl_N...   \n",
       "1                   [NNP_compound_VBZ, VBZ_ROOT_VBZ]   \n",
       "2  [NN_ROOT_NN, IN_advmod_JJS, JJS_advmod_VBP, PR...   \n",
       "\n",
       "                           feat_word_root_rootparent  \\\n",
       "0  [warning_warning_warning, penny boards_make_wa...   \n",
       "1              [fuck_dykes_dykes, dykes_dykes_dykes]   \n",
       "2  [user_mention user_mention user_mention user_m...   \n",
       "\n",
       "                                   feat_dep_unigrams  \\\n",
       "0  [warning_warning_ROOT_NN, penny boards_make_ns...   \n",
       "1    [fuck_dykes_compound_NNP, dykes_dykes_ROOT_VBZ]   \n",
       "2  [user_mention user_mention user_mention user_m...   \n",
       "\n",
       "                                    feat_dep_bigrams  \\\n",
       "0  [warning_warning_ROOT_NN|penny boards_make_nsu...   \n",
       "1     [fuck_dykes_compound_NNP|dykes_dykes_ROOT_VBZ]   \n",
       "2  [user_mention user_mention user_mention user_m...   \n",
       "\n",
       "                                   feat_dep_trigrams  \n",
       "0  [warning_warning_ROOT_NN|penny boards_make_nsu...  \n",
       "1                                                 []  \n",
       "2  [user_mention user_mention user_mention user_m...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joblib.dump(feat_df, crowdflower_persistence, compress=True)\n",
    "feat_df = joblib.load(crowdflower_persistence)\n",
    "feat_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch NAACL_SRW_2016 and NLP+CSS_2016 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection_params_3 = [\"twitter\", \"NAACL_SRW_2016\"]\n",
    "# connection_params_4 = [\"twitter\", \"NLP_CSS_2016_expert\"]\n",
    "# df_naacl = fetch_as_df(connection_params_3, {})\n",
    "# df_nlp = fetch_as_df(connection_params_4, {})\n",
    "# joblib.dump(df_naacl, naacl_2016_persistence, compress=True)\n",
    "# joblib.dump(df_nlp, nlp_2016_persistence, compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>annotation</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>591c29f465419158a43b735d</td>\n",
       "      <td>neither</td>\n",
       "      <td>597576902212063232</td>\n",
       "      <td>Cisco had to deal with a fat cash payout to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>591c29f465419158a43b735e</td>\n",
       "      <td>neither</td>\n",
       "      <td>565586175864610817</td>\n",
       "      <td>@MadamPlumpette I'm decent at editing, no worr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>591c29f465419158a43b735f</td>\n",
       "      <td>neither</td>\n",
       "      <td>563881580209246209</td>\n",
       "      <td>@girlziplocked will read. gotta go afk for a b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id annotation              id_str  \\\n",
       "0  591c29f465419158a43b735d    neither  597576902212063232   \n",
       "1  591c29f465419158a43b735e    neither  565586175864610817   \n",
       "2  591c29f465419158a43b735f    neither  563881580209246209   \n",
       "\n",
       "                                                text  \n",
       "0  Cisco had to deal with a fat cash payout to th...  \n",
       "1  @MadamPlumpette I'm decent at editing, no worr...  \n",
       "2  @girlziplocked will read. gotta go afk for a b...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_naacl = joblib.load(naacl_2016_persistence)\n",
    "df_nlp = joblib.load(nlp_2016_persistence)\n",
    "df_nlp.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup generic model experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(X, y, pipeline, process_name, num_expts=1):\n",
    "    scores = list()\n",
    "    for i in tqdm(range(num_expts)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, train_size=0.80)\n",
    "        model = pipeline.fit(X_train, y_train)  # train the classifier\n",
    "        y_prediction = model.predict(X_test)          # apply the model to the test data\n",
    "        report = classification_report(y_test, y_prediction)\n",
    "        score = accuracy_score(y_prediction, y_test)  # compare the results to the gold standard\n",
    "        scores.append(score)\n",
    "        print(\"Classification Report: \" + process_name)\n",
    "        print(report)\n",
    "        cm = confusion_matrix(y_test, y_prediction)\n",
    "#         print(\"Confusion matrix:\")\n",
    "#         print(cm)\n",
    "#     print(sum(scores) / num_expts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive baseline classification (countVectorizer: character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CrowdFlower dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the labels of the test set...\n",
      "14508 documents\n",
      "2 categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:06<00:00,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: CrowdFlower: CountVectorizer[character]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.47      0.43      0.45       460\n",
      "not_offensive       0.89      0.91      0.90      2442\n",
      "\n",
      "  avg / total       0.83      0.83      0.83      2902\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_set = feat_df.sample(n=int(len(feat_df)), random_state=1965)\n",
    "X = train_test_set['text']\n",
    "y = train_test_set['annotation_label']\n",
    "\n",
    "print(\"Predicting the labels of the test set...\")\n",
    "print(\"%d documents\" % len(X))\n",
    "print(\"%d categories\" % len(y.value_counts()))\n",
    "\n",
    "# our two ingredients: the ngram counter and the classifier\n",
    "nm = 5000\n",
    "vect = CountVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "ch2 = SelectKBest(chi2, k=nm)\n",
    "\n",
    "# There are just two steps to our process: extracting the ngrams and\n",
    "# putting them through the classifier. So our Pipeline looks like this:\n",
    "\n",
    "count_pipeline = Pipeline([\n",
    "    ('vect', vect),  # extract ngrams from tweet text\n",
    "    ('kBest', ch2),\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "\n",
    "# Run the classifcation\n",
    "run_experiment(X, y, count_pipeline, \"CrowdFlower: CountVectorizer[character]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NAACL 2016 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the labels of the test set...\n",
      "14508 documents\n",
      "3 categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:09<00:00,  9.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: NAACL2016: CountVectorizer [character]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       none       0.87      0.88      0.87      2048\n",
      "     racism       0.67      0.66      0.66       332\n",
      "     sexism       0.71      0.67      0.69       522\n",
      "\n",
      "avg / total       0.82      0.82      0.82      2902\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_set = df_naacl.sample(n=int(len(feat_df)), random_state=1965)\n",
    "X = train_test_set['text']\n",
    "y = train_test_set['annotation']\n",
    "\n",
    "print(\"Predicting the labels of the test set...\")\n",
    "print(\"%d documents\" % len(X))\n",
    "print(\"%d categories\" % len(y.value_counts()))\n",
    "\n",
    "nm = 5000\n",
    "vect = CountVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "ch2 = SelectKBest(chi2, k=nm)\n",
    "\n",
    "# There are just two steps to our process: extracting the ngrams and\n",
    "# putting them through the classifier. So our Pipeline looks like this:\n",
    "\n",
    "count_pipeline = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('kBest', ch2),\n",
    "    ('clf' , clf),\n",
    "])\n",
    "\n",
    "# Run the classifcation\n",
    "run_experiment(X, y, count_pipeline, \"NAACL2016: CountVectorizer [character]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive baseline classification (hashingVectorizer: character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CrowdFlower dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the labels of the test set...\n",
      "14508 documents\n",
      "2 categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:04<00:00,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: CrowdFlower: HashingVectorizer[character]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.55      0.36      0.43       478\n",
      "not_offensive       0.88      0.94      0.91      2424\n",
      "\n",
      "  avg / total       0.83      0.85      0.83      2902\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_set = feat_df.sample(n=int(len(feat_df)), random_state=1965)\n",
    "X = train_test_set['text']\n",
    "y = train_test_set['annotation_label']\n",
    "\n",
    "print(\"Predicting the labels of the test set...\")\n",
    "print(\"%d documents\" % len(X))\n",
    "print(\"%d categories\" % len(y.value_counts()))\n",
    "\n",
    "vect = HashingVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "\n",
    "hashing_pipeline = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('clf' , clf), \n",
    "])\n",
    "run_experiment(X, y, hashing_pipeline, \"CrowdFlower: HashingVectorizer[character]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NAACL 2016 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the labels of the test set...\n",
      "14508 documents\n",
      "3 categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:05<00:00,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: NAACL2016: HashingVectorizer[character]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       none       0.86      0.94      0.89      2000\n",
      "     racism       0.81      0.69      0.74       350\n",
      "     sexism       0.83      0.64      0.72       552\n",
      "\n",
      "avg / total       0.85      0.85      0.84      2902\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_set = df_naacl.sample(n=int(len(feat_df)), random_state=1965)\n",
    "X = train_test_set['text']\n",
    "y = train_test_set['annotation']\n",
    "\n",
    "print(\"Predicting the labels of the test set...\")\n",
    "print(\"%d documents\" % len(X))\n",
    "print(\"%d categories\" % len(y.value_counts()))\n",
    "\n",
    "vect = HashingVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "\n",
    "hashing_pipeline = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('clf' , clf), \n",
    "])\n",
    "run_experiment(X, y, hashing_pipeline, \"NAACL2016: HashingVectorizer[character]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive baseline classification (TfidfVectorizer: character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CrowdFlower dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the labels of the test set...\n",
      "14508 documents\n",
      "2 categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:04<00:00,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: CrowdFlower: TfidfVectorizer[character]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.59      0.37      0.46       468\n",
      "not_offensive       0.89      0.95      0.92      2434\n",
      "\n",
      "  avg / total       0.84      0.86      0.84      2902\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_set = feat_df.sample(n=int(len(feat_df)), random_state=1965)\n",
    "X = train_test_set['text']\n",
    "y = train_test_set['annotation_label']\n",
    "\n",
    "print(\"Predicting the labels of the test set...\")\n",
    "print(\"%d documents\" % len(X))\n",
    "print(\"%d categories\" % len(y.value_counts()))\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "\n",
    "tfidf_pipeline = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('clf' , clf),\n",
    "])\n",
    "run_experiment(X, y, tfidf_pipeline, \"CrowdFlower: TfidfVectorizer[character]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NAACL 2016 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the labels of the test set...\n",
      "14508 documents\n",
      "3 categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:07<00:00,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: NAACL2016: TfidfVectorizer[character]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       none       0.87      0.93      0.90      1994\n",
      "     racism       0.77      0.66      0.71       347\n",
      "     sexism       0.86      0.70      0.77       561\n",
      "\n",
      "avg / total       0.85      0.85      0.85      2902\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_set = df_naacl.sample(n=int(len(feat_df)), random_state=1965)\n",
    "X = train_test_set['text']\n",
    "y = train_test_set['annotation']\n",
    "\n",
    "print(\"Predicting the labels of the test set...\")\n",
    "print(\"%d documents\" % len(X))\n",
    "print(\"%d categories\" % len(y.value_counts()))\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "\n",
    "tfidf_pipeline = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('clf' , clf),\n",
    "])\n",
    "run_experiment(X, y, tfidf_pipeline, \"NAACL2016: TfidfVectorizer[character]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency tuple experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: CrowdFlower: FeatureHasher\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.54      0.19      0.28       497\n",
      "not_offensive       0.85      0.97      0.91      2405\n",
      "\n",
      "  avg / total       0.80      0.83      0.80      2902\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_set = feat_df.sample(n=int(len(feat_df)), random_state=1965)\n",
    "X = train_test_set['feat_word_root_rootparent']\n",
    "y = train_test_set['annotation_label']\n",
    "\n",
    "hasher = FeatureHasher(input_type='string', non_negative=True)\n",
    "clf = LinearSVC()\n",
    "\n",
    "featurer_hasher_pipeline = Pipeline([\n",
    "    ('hasher', hasher),\n",
    "    ('clf' , clf),\n",
    "])\n",
    "run_experiment(X, y, featurer_hasher_pipeline, \"CrowdFlower: FeatureHasher\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency context experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "class TextExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Adapted from code by @zacstewart \n",
    "       https://github.com/zacstewart/kaggle_seeclickfix/blob/master/estimator.py\n",
    "       Also see Zac Stewart's excellent blogpost on pipelines:\n",
    "       http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "       \"\"\"\n",
    "    \n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def transform(self, df):\n",
    "        # select the relevant column and return it as a numpy array\n",
    "        # set the array type to be string\n",
    "        return df[self.column_name].tolist()\n",
    "#         return np.asarray(df[self.column_name]).astype(str)\n",
    "        \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "\n",
    "class Apply(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies a function f element-wise to the numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fn):\n",
    "        self.fn = np.vectorize(fn)\n",
    "        \n",
    "    def transform(self, data):\n",
    "        # note: reshaping is necessary because otherwise sklearn\n",
    "        # interprets 1-d array as a single sample\n",
    "        return self.fn(data.reshape(data.size, 1))\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "class BooleanExtractor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def transform(self, df):\n",
    "        # select the relevant column and return it as a numpy array\n",
    "        # set the array type to be string\n",
    "        return np.asarray(df[self.column_name]).astype(np.int)\n",
    "                                                       \n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: TfidfVectorizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.57      0.30      0.39       475\n",
      "not_offensive       0.87      0.95      0.91      2427\n",
      "\n",
      "  avg / total       0.82      0.85      0.83      2902\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "X = train_test_set[['feat_dependency_contexts', 'hs_keyword_count']]\n",
    "y = train_test_set['annotation_label']\n",
    "clf = LinearSVC()\n",
    "empty_analyzer = lambda x: x\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "vect = TfidfVectorizer(analyzer=empty_analyzer)\n",
    "\n",
    "dependency_context_pipeline = Pipeline([\n",
    "    ('dep_extractor', TextExtractor('feat_dependency_contexts')), # extract names from df\n",
    "    ('vect', vect)\n",
    "])\n",
    "\n",
    "hs_keyword_count_pipeline = Pipeline([\n",
    "    ('count_extractor', BooleanExtractor('hs_keyword_count')),\n",
    "    ('identity', Apply(lambda x: x))\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('all_features', FeatureUnion([\n",
    "        ('dependency_context_pipeline', dependency_context_pipeline), # all text features\n",
    "        ('hs_keyword_count_pipeline', hs_keyword_count_pipeline),\n",
    "    ])),\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "\n",
    "run_experiment(X, y, pipeline, \"TfidfVectorizer\")\n",
    "\n",
    "# https://github.com/michelleful/SingaporeRoadnameOrigins/blob/master/notebooks/04%20Adding%20features%20with%20Pipelines.ipynb\n",
    "# https://github.com/amueller/kaggle_insults/blob/e4abac805be1d1e2b3201a978172bafd36cc01e3/features.py\n",
    "# http://www.markhneedham.com/blog/2015/03/02/python-scikit-learn-training-a-classifier-with-non-numeric-features/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
