{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "\n",
    "This is a staging notebook for experiments related to the classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import models, similarities\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import joblib\n",
    "from gensim import models, similarities\n",
    "from modules.db import mongo_base\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import FeatureHasher, DictVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from modules.utils.CustomTwokenizer import CustomTwokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store collection as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_as_df(connection_params, projection):\n",
    "    client = mongo_base.connect()\n",
    "    db_name = connection_params[0]\n",
    "    connection_params.insert(0, client)\n",
    "    query = {}\n",
    "    query[\"filter\"] = {}\n",
    "    query[\"projection\"] = projection\n",
    "    query[\"limit\"] = 0\n",
    "    query[\"skip\"] = 0\n",
    "    query[\"no_cursor_timeout\"] = True\n",
    "    cursor = mongo_base.finder(connection_params, query, False)\n",
    "    df = pd.DataFrame(list(cursor))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge dataframes [CrowdFlower Dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connection_params_1 = [\"twitter\", \"crowdflower_features\"]\n",
    "connection_params_2 = [\"twitter\", \"crowdflower_features_emo\"]\n",
    "# df = fetch_as_df(connection_params_1, {})\n",
    "# df_emo = fetch_as_df(connection_params_2, {\"emotions\":1})\n",
    "# df = pd.DataFrame.merge(df, df_emo, on=\"_id\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle the raw feature collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacy_en_model = \"en_core_web_md\"\n",
    "spacy_glove_model = \"en_vectors_glove_md\"\n",
    "crowdflower_persistence_raw = 'data/persistence/df/crowdflower_features_raw.pkl.compressed'\n",
    "crowdflower_persistence = 'data/persistence/df/crowdflower_features.pkl.compressed'\n",
    "naacl_2016_persistence = 'data/persistence/df/naacl_2016.pkl.compressed'\n",
    "nlp_2016_persistence = 'data/persistence/df/nlp_2016.pkl.compressed'\n",
    "# nlp = spacy.load(spacy_en_model, create_make_doc=CustomTwokenizer)\n",
    "# joblib.dump(df, crowdflower_persistence_raw, compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = joblib.load(crowdflower_persistence_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe with classifier features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feat_df = df[['_id', 'text', 'annotation_label', 'hs_keyword_matches', 'hs_keyword_count', 'unknown_words', 'unknown_words_count', 'comment_length', 'brown_cluster_ids', 'feat_dependency_contexts', 'feat_word_dep_root', 'feat_pos_dep_rootPos', 'feat_word_root_rootparent', 'feat_dep_unigrams', 'feat_dep_bigrams', 'feat_dep_trigrams']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>text</th>\n",
       "      <th>annotation_label</th>\n",
       "      <th>hs_keyword_matches</th>\n",
       "      <th>hs_keyword_count</th>\n",
       "      <th>unknown_words</th>\n",
       "      <th>unknown_words_count</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>brown_cluster_ids</th>\n",
       "      <th>feat_dependency_contexts</th>\n",
       "      <th>feat_word_dep_root</th>\n",
       "      <th>feat_pos_dep_rootPos</th>\n",
       "      <th>feat_word_root_rootparent</th>\n",
       "      <th>feat_dep_unigrams</th>\n",
       "      <th>feat_dep_bigrams</th>\n",
       "      <th>feat_dep_trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58c659be6541913eb7f119dd</td>\n",
       "      <td>Warning : penny boards will make you a faggot</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>[faggot]</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>[966, 228, 442, 4618, 602, 19]</td>\n",
       "      <td>[warning_:_punct, warning_make_acl, penny boar...</td>\n",
       "      <td>[warning_ROOT_warning, penny boards_nsubj_make...</td>\n",
       "      <td>[NN_ROOT_NN, NNS_nsubj_VB, MD_aux_VB, VB_acl_N...</td>\n",
       "      <td>[warning_warning_warning, penny boards_make_wa...</td>\n",
       "      <td>[warning_warning_ROOT_NN, penny boards_make_ns...</td>\n",
       "      <td>[warning_warning_ROOT_NN|penny boards_make_nsu...</td>\n",
       "      <td>[warning_warning_ROOT_NN|penny boards_make_nsu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58c659be6541913eb7f119de</td>\n",
       "      <td>Fuck dykes</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>[fuck_dykes_compoundINV, dykes_fuck_compound]</td>\n",
       "      <td>[fuck_compound_dykes, dykes_ROOT_dykes]</td>\n",
       "      <td>[NNP_compound_VBZ, VBZ_ROOT_VBZ]</td>\n",
       "      <td>[fuck_dykes_dykes, dykes_dykes_dykes]</td>\n",
       "      <td>[fuck_dykes_compound_NNP, dykes_dykes_ROOT_VBZ]</td>\n",
       "      <td>[fuck_dykes_compound_NNP|dykes_dykes_ROOT_VBZ]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58c659be6541913eb7f119df</td>\n",
       "      <td>user_mention user_mention user_mention user_me...</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[faggot]</td>\n",
       "      <td>1</td>\n",
       "      <td>[jefree]</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>[124, 3690, 966, 2442, 1684]</td>\n",
       "      <td>[user_mention user_mention user_mention user_m...</td>\n",
       "      <td>[user_mention user_mention user_mention user_m...</td>\n",
       "      <td>[NN_ROOT_NN, IN_advmod_JJS, JJS_advmod_VBP, PR...</td>\n",
       "      <td>[user_mention user_mention user_mention user_m...</td>\n",
       "      <td>[user_mention user_mention user_mention user_m...</td>\n",
       "      <td>[user_mention user_mention user_mention user_m...</td>\n",
       "      <td>[user_mention user_mention user_mention user_m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  58c659be6541913eb7f119dd   \n",
       "1  58c659be6541913eb7f119de   \n",
       "2  58c659be6541913eb7f119df   \n",
       "\n",
       "                                                text annotation_label  \\\n",
       "0     Warning : penny boards will make you a faggot     not_offensive   \n",
       "1                                        Fuck dykes        hatespeech   \n",
       "2  user_mention user_mention user_mention user_me...       hatespeech   \n",
       "\n",
       "  hs_keyword_matches  hs_keyword_count unknown_words  unknown_words_count  \\\n",
       "0           [faggot]                 1            []                    0   \n",
       "1                 []                 0            []                    0   \n",
       "2           [faggot]                 1      [jefree]                    1   \n",
       "\n",
       "   comment_length               brown_cluster_ids  \\\n",
       "0               9  [966, 228, 442, 4618, 602, 19]   \n",
       "1               2                              []   \n",
       "2              14    [124, 3690, 966, 2442, 1684]   \n",
       "\n",
       "                            feat_dependency_contexts  \\\n",
       "0  [warning_:_punct, warning_make_acl, penny boar...   \n",
       "1      [fuck_dykes_compoundINV, dykes_fuck_compound]   \n",
       "2  [user_mention user_mention user_mention user_m...   \n",
       "\n",
       "                                  feat_word_dep_root  \\\n",
       "0  [warning_ROOT_warning, penny boards_nsubj_make...   \n",
       "1            [fuck_compound_dykes, dykes_ROOT_dykes]   \n",
       "2  [user_mention user_mention user_mention user_m...   \n",
       "\n",
       "                                feat_pos_dep_rootPos  \\\n",
       "0  [NN_ROOT_NN, NNS_nsubj_VB, MD_aux_VB, VB_acl_N...   \n",
       "1                   [NNP_compound_VBZ, VBZ_ROOT_VBZ]   \n",
       "2  [NN_ROOT_NN, IN_advmod_JJS, JJS_advmod_VBP, PR...   \n",
       "\n",
       "                           feat_word_root_rootparent  \\\n",
       "0  [warning_warning_warning, penny boards_make_wa...   \n",
       "1              [fuck_dykes_dykes, dykes_dykes_dykes]   \n",
       "2  [user_mention user_mention user_mention user_m...   \n",
       "\n",
       "                                   feat_dep_unigrams  \\\n",
       "0  [warning_warning_ROOT_NN, penny boards_make_ns...   \n",
       "1    [fuck_dykes_compound_NNP, dykes_dykes_ROOT_VBZ]   \n",
       "2  [user_mention user_mention user_mention user_m...   \n",
       "\n",
       "                                    feat_dep_bigrams  \\\n",
       "0  [warning_warning_ROOT_NN|penny boards_make_nsu...   \n",
       "1     [fuck_dykes_compound_NNP|dykes_dykes_ROOT_VBZ]   \n",
       "2  [user_mention user_mention user_mention user_m...   \n",
       "\n",
       "                                   feat_dep_trigrams  \n",
       "0  [warning_warning_ROOT_NN|penny boards_make_nsu...  \n",
       "1                                                 []  \n",
       "2  [user_mention user_mention user_mention user_m...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joblib.dump(feat_df, crowdflower_persistence, compress=True)\n",
    "feat_df = joblib.load(crowdflower_persistence)\n",
    "feat_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch NAACL_SRW_2016 and NLP+CSS_2016 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connection_params_3 = [\"twitter\", \"NAACL_SRW_2016_features\"]\n",
    "# connection_params_4 = [\"twitter\", \"NLP_CSS_2016_expert_features\"]\n",
    "# df_naacl = fetch_as_df(connection_params_3, {})\n",
    "# df_nlp = fetch_as_df(connection_params_4, {})\n",
    "# joblib.dump(df_naacl, naacl_2016_persistence, compress=True)\n",
    "# joblib.dump(df_nlp, nlp_2016_persistence, compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>annotation</th>\n",
       "      <th>avg_token_length</th>\n",
       "      <th>brown_cluster_ids</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>feat_dep_bigrams</th>\n",
       "      <th>feat_dep_trigrams</th>\n",
       "      <th>feat_dep_unigrams</th>\n",
       "      <th>feat_dependency_contexts</th>\n",
       "      <th>feat_pos_dep_rootPos</th>\n",
       "      <th>feat_word_dep_root</th>\n",
       "      <th>feat_word_root_rootparent</th>\n",
       "      <th>has_hs_keywords</th>\n",
       "      <th>hs_keyword_count</th>\n",
       "      <th>hs_keyword_matches</th>\n",
       "      <th>text</th>\n",
       "      <th>unknown_words</th>\n",
       "      <th>unknown_words_count</th>\n",
       "      <th>uppercase_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>591c29f465419158a43b735d</td>\n",
       "      <td>neither</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[166, 1914, 12, 5829, 1020, 12, 5098, 20, 5098...</td>\n",
       "      <td>29</td>\n",
       "      <td>[cisco_had_nsubj_NNP|had_allow_aux_VBD, had_al...</td>\n",
       "      <td>[cisco_had_nsubj_NNP|had_allow_aux_VBD|to_deal...</td>\n",
       "      <td>[cisco_had_nsubj_NNP, had_allow_aux_VBD, to_de...</td>\n",
       "      <td>[cisco_had_nsubjINV, had_cisco_nsubj, had_deal...</td>\n",
       "      <td>[NNP_nsubj_VBD, VBD_aux_VB, TO_aux_VB, VB_xcom...</td>\n",
       "      <td>[cisco_nsubj_had, had_aux_allow, to_aux_deal, ...</td>\n",
       "      <td>[cisco_had_allow, had_allow_allow, to_deal_had...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Cisco had to deal with a fat cash payout to th...</td>\n",
       "      <td>[fsf, compliancy]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>591c29f465419158a43b735e</td>\n",
       "      <td>neither</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[551, 124, 3050, 4]</td>\n",
       "      <td>9</td>\n",
       "      <td>[user_mention_i'm_nsubj_NN|i'm_i'm_ROOT_VBZ, i...</td>\n",
       "      <td>[user_mention_i'm_nsubj_NN|i'm_i'm_ROOT_VBZ|de...</td>\n",
       "      <td>[user_mention_i'm_nsubj_NN, i'm_i'm_ROOT_VBZ, ...</td>\n",
       "      <td>[user_mention_i'm_nsubjINV, i'm_user_mention_n...</td>\n",
       "      <td>[NN_nsubj_VBZ, VBZ_ROOT_VBZ, JJ_acomp_VBZ, IN_...</td>\n",
       "      <td>[user_mention_nsubj_i'm, i'm_ROOT_i'm, decent_...</td>\n",
       "      <td>[user_mention_i'm_i'm, i'm_i'm_i'm, decent_i'm...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>user_mention I'm decent at editing , no worrie...</td>\n",
       "      <td>[^.^]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>591c29f465419158a43b735f</td>\n",
       "      <td>neither</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[442, 6314, 8, 5114, 3466, 508, 19, 853, 36, 1...</td>\n",
       "      <td>23</td>\n",
       "      <td>[user_mention_read_nsubj_NN|will_read_aux_MD, ...</td>\n",
       "      <td>[user_mention_read_nsubj_NN|will_read_aux_MD|r...</td>\n",
       "      <td>[user_mention_read_nsubj_NN, will_read_aux_MD,...</td>\n",
       "      <td>[user_mention_read_nsubjINV, will_read_auxINV,...</td>\n",
       "      <td>[NN_nsubj_VB, MD_aux_VB, VB_ROOT_VB, NN_aux_VB...</td>\n",
       "      <td>[user_mention_nsubj_read, will_aux_read, read_...</td>\n",
       "      <td>[user_mention_read_read, will_read_read, read_...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>user_mention will read . gotta go afk for a bi...</td>\n",
       "      <td>[afk]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id annotation  avg_token_length  \\\n",
       "0  591c29f465419158a43b735d    neither               4.0   \n",
       "1  591c29f465419158a43b735e    neither               5.0   \n",
       "2  591c29f465419158a43b735f    neither               4.0   \n",
       "\n",
       "                                   brown_cluster_ids  comment_length  \\\n",
       "0  [166, 1914, 12, 5829, 1020, 12, 5098, 20, 5098...              29   \n",
       "1                                [551, 124, 3050, 4]               9   \n",
       "2  [442, 6314, 8, 5114, 3466, 508, 19, 853, 36, 1...              23   \n",
       "\n",
       "                                    feat_dep_bigrams  \\\n",
       "0  [cisco_had_nsubj_NNP|had_allow_aux_VBD, had_al...   \n",
       "1  [user_mention_i'm_nsubj_NN|i'm_i'm_ROOT_VBZ, i...   \n",
       "2  [user_mention_read_nsubj_NN|will_read_aux_MD, ...   \n",
       "\n",
       "                                   feat_dep_trigrams  \\\n",
       "0  [cisco_had_nsubj_NNP|had_allow_aux_VBD|to_deal...   \n",
       "1  [user_mention_i'm_nsubj_NN|i'm_i'm_ROOT_VBZ|de...   \n",
       "2  [user_mention_read_nsubj_NN|will_read_aux_MD|r...   \n",
       "\n",
       "                                   feat_dep_unigrams  \\\n",
       "0  [cisco_had_nsubj_NNP, had_allow_aux_VBD, to_de...   \n",
       "1  [user_mention_i'm_nsubj_NN, i'm_i'm_ROOT_VBZ, ...   \n",
       "2  [user_mention_read_nsubj_NN, will_read_aux_MD,...   \n",
       "\n",
       "                            feat_dependency_contexts  \\\n",
       "0  [cisco_had_nsubjINV, had_cisco_nsubj, had_deal...   \n",
       "1  [user_mention_i'm_nsubjINV, i'm_user_mention_n...   \n",
       "2  [user_mention_read_nsubjINV, will_read_auxINV,...   \n",
       "\n",
       "                                feat_pos_dep_rootPos  \\\n",
       "0  [NNP_nsubj_VBD, VBD_aux_VB, TO_aux_VB, VB_xcom...   \n",
       "1  [NN_nsubj_VBZ, VBZ_ROOT_VBZ, JJ_acomp_VBZ, IN_...   \n",
       "2  [NN_nsubj_VB, MD_aux_VB, VB_ROOT_VB, NN_aux_VB...   \n",
       "\n",
       "                                  feat_word_dep_root  \\\n",
       "0  [cisco_nsubj_had, had_aux_allow, to_aux_deal, ...   \n",
       "1  [user_mention_nsubj_i'm, i'm_ROOT_i'm, decent_...   \n",
       "2  [user_mention_nsubj_read, will_aux_read, read_...   \n",
       "\n",
       "                           feat_word_root_rootparent  has_hs_keywords  \\\n",
       "0  [cisco_had_allow, had_allow_allow, to_deal_had...            False   \n",
       "1  [user_mention_i'm_i'm, i'm_i'm_i'm, decent_i'm...            False   \n",
       "2  [user_mention_read_read, will_read_read, read_...            False   \n",
       "\n",
       "   hs_keyword_count hs_keyword_matches  \\\n",
       "0                 0                 []   \n",
       "1                 0                 []   \n",
       "2                 0                 []   \n",
       "\n",
       "                                                text      unknown_words  \\\n",
       "0  Cisco had to deal with a fat cash payout to th...  [fsf, compliancy]   \n",
       "1  user_mention I'm decent at editing , no worrie...              [^.^]   \n",
       "2  user_mention will read . gotta go afk for a bi...              [afk]   \n",
       "\n",
       "   unknown_words_count  uppercase_token_count  \n",
       "0                    2                      2  \n",
       "1                    1                      0  \n",
       "2                    1                      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_naacl = joblib.load(naacl_2016_persistence)\n",
    "df_nlp = joblib.load(nlp_2016_persistence)\n",
    "df_nlp.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup generic model experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(X, y, pipeline, process_name, num_expts=1):\n",
    "    scores = list()\n",
    "    for i in tqdm(range(num_expts)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, train_size=0.80)\n",
    "        model = pipeline.fit(X_train, y_train)  # train the classifier\n",
    "        y_prediction = model.predict(X_test)          # apply the model to the test data\n",
    "        report = classification_report(y_test, y_prediction)\n",
    "        score = accuracy_score(y_prediction, y_test)  # compare the results to the gold standard\n",
    "        scores.append(score)\n",
    "        print(\"Classification Report: \" + process_name)\n",
    "        print(report)\n",
    "        cm = confusion_matrix(y_test, y_prediction)\n",
    "#         print(\"Confusion matrix:\")\n",
    "#         print(cm)\n",
    "#     print(sum(scores) / num_expts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup helpers [GridSearch, numFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_gridsearch_cv(pipeline, X, y, param_grid, n_jobs):\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, n_jobs=4)\n",
    "    grid_search.fit(X, y)\n",
    "    print(grid_search.best_params_)\n",
    "    print(grid_search.best_score_)\n",
    "    \n",
    "def get_num_features(vectorizer, X):\n",
    "    vect.fit(X)\n",
    "    # feature_names = [feature_names[i] for i in skb.get_support(indices=True)]\n",
    "    return len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive baseline classification (countVectorizer: character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CrowdFlower dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the labels of the test set...\n",
      "14508 documents\n",
      "2 categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:22<00:00, 22.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: CrowdFlower: CountVectorizer[character] LSA\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.59      0.34      0.43       470\n",
      "not_offensive       0.88      0.95      0.92      2432\n",
      "\n",
      "  avg / total       0.83      0.85      0.84      2902\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_set = feat_df.sample(n=int(len(feat_df)), random_state=1965)\n",
    "X = train_test_set['text']\n",
    "y = train_test_set['annotation_label']\n",
    "\n",
    "print(\"Predicting the labels of the test set...\")\n",
    "print(\"%d documents\" % len(X))\n",
    "print(\"%d categories\" % len(y.value_counts()))\n",
    "\n",
    "N_FEATURES_OPTIONS = [100000, 150000]\n",
    "# http://www.kdnuggets.com/2016/08/approaching-almost-any-machine-learning-problem.html/2\n",
    "N_COMPONENTS = [120]\n",
    "n_jobs = 2\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'skb__k': N_FEATURES_OPTIONS,\n",
    "        'svd__n_components': N_COMPONENTS\n",
    "    }\n",
    "]\n",
    "\n",
    "# Params learned through GridSearch\n",
    "k_features = 100000\n",
    "n_components = 120\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(3,5), analyzer='char', stop_words='english')\n",
    "# print(get_num_features(vect, X))\n",
    "\n",
    "clf = LinearSVC()\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "skb = SelectKBest(chi2, k=k_features)\n",
    "\n",
    "count_pipeline = Pipeline([\n",
    "    ('vect', vect),  # extract ngrams from tweet text\n",
    "    ('skb', skb),\n",
    "    ('svd', svd),\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "\n",
    "# run_gridsearch_cv(count_pipeline, X, y, param_grid, n_jobs)\n",
    "\n",
    "# Run the classifcation\n",
    "run_experiment(X, y, count_pipeline, \"CrowdFlower: CountVectorizer[character] LSA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NAACL 2016 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the labels of the test set...\n",
      "16187 documents\n",
      "3 categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:38<00:00, 38.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: NAACL2016: CountVectorizer [character]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       none       0.83      0.94      0.88      2214\n",
      "     racism       0.77      0.69      0.73       383\n",
      "     sexism       0.85      0.52      0.65       641\n",
      "\n",
      "avg / total       0.83      0.83      0.82      3238\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_set = df_naacl.sample(n=int(len(df_naacl)), random_state=1965)\n",
    "X = train_test_set['text']\n",
    "y = train_test_set['annotation']\n",
    "\n",
    "print(\"Predicting the labels of the test set...\")\n",
    "print(\"%d documents\" % len(X))\n",
    "print(\"%d categories\" % len(y.value_counts()))\n",
    "\n",
    "N_FEATURES_OPTIONS = [100000,130000]\n",
    "N_COMPONENTS = [120]\n",
    "n_jobs = 2\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'skb__k': N_FEATURES_OPTIONS,\n",
    "        'svd__n_components': N_COMPONENTS\n",
    "    }\n",
    "]\n",
    "\n",
    "# Params learned through GridSearch\n",
    "k_features = 130000\n",
    "n_components = 120\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(3,5), analyzer='char', stop_words='english')\n",
    "# print(get_num_features(vect, X))\n",
    "\n",
    "clf = LinearSVC()\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "skb = SelectKBest(chi2, k=k_features)\n",
    "\n",
    "count_pipeline = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('skb', skb),\n",
    "    ('svd', svd),\n",
    "    ('clf' , clf),\n",
    "])\n",
    "\n",
    "# run_gridsearch_cv(count_pipeline, X, y, param_grid, n_jobs)\n",
    "run_experiment(X, y, count_pipeline, \"NAACL2016: CountVectorizer [character]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive baseline classification (hashingVectorizer: character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CrowdFlower dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the labels of the test set...\n",
      "14508 documents\n",
      "2 categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:03<00:00,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: CrowdFlower: HashingVectorizer[character]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.56      0.38      0.45       471\n",
      "not_offensive       0.89      0.94      0.91      2431\n",
      "\n",
      "  avg / total       0.83      0.85      0.84      2902\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_set = feat_df.sample(n=int(len(feat_df)), random_state=1965)\n",
    "X = train_test_set['text']\n",
    "y = train_test_set['annotation_label']\n",
    "\n",
    "print(\"Predicting the labels of the test set...\")\n",
    "print(\"%d documents\" % len(X))\n",
    "print(\"%d categories\" % len(y.value_counts()))\n",
    "\n",
    "vect = HashingVectorizer(ngram_range=(3,5), analyzer='char', stop_words='english')\n",
    "clf = LinearSVC()\n",
    "\n",
    "hashing_pipeline = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('clf' , clf), \n",
    "])\n",
    "run_experiment(X, y, hashing_pipeline, \"CrowdFlower: HashingVectorizer[character]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NAACL 2016 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the labels of the test set...\n",
      "16187 documents\n",
      "3 categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:06<00:00,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: NAACL2016: HashingVectorizer[character]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       none       0.86      0.94      0.89      2197\n",
      "     racism       0.79      0.68      0.73       377\n",
      "     sexism       0.85      0.65      0.74       664\n",
      "\n",
      "avg / total       0.85      0.85      0.84      3238\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_set = df_naacl.sample(n=int(len(df_naacl)), random_state=1965)\n",
    "X = train_test_set['text']\n",
    "y = train_test_set['annotation']\n",
    "\n",
    "print(\"Predicting the labels of the test set...\")\n",
    "print(\"%d documents\" % len(X))\n",
    "print(\"%d categories\" % len(y.value_counts()))\n",
    "\n",
    "vect = HashingVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "\n",
    "hashing_pipeline = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('clf' , clf), \n",
    "])\n",
    "run_experiment(X, y, hashing_pipeline, \"NAACL2016: HashingVectorizer[character]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive baseline classification (TfidfVectorizer: character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CrowdFlower dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the labels of the test set...\n",
      "14508 documents\n",
      "2 categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:19<00:00, 19.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: CrowdFlower: LSA - TfidfVectorizer[character]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.62      0.25      0.36       517\n",
      "not_offensive       0.86      0.97      0.91      2385\n",
      "\n",
      "  avg / total       0.81      0.84      0.81      2902\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_set = feat_df.sample(n=int(len(feat_df)), random_state=1965)\n",
    "X = train_test_set['text']\n",
    "y = train_test_set['annotation_label']\n",
    "\n",
    "print(\"Predicting the labels of the test set...\")\n",
    "print(\"%d documents\" % len(X))\n",
    "print(\"%d categories\" % len(y.value_counts()))\n",
    "\n",
    "N_FEATURES_OPTIONS = [100000, 200000]\n",
    "N_COMPONENTS = [120]\n",
    "n_jobs = 2\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'skb__k': N_FEATURES_OPTIONS,\n",
    "        'svd__n_components': N_COMPONENTS\n",
    "    }\n",
    "]\n",
    "\n",
    "# Params learned through GridSearch\n",
    "k_features = 150000\n",
    "n_components = 120\n",
    "\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "skb = SelectKBest(chi2, k=k_features)\n",
    "vect = TfidfVectorizer(ngram_range=(3,5), analyzer='char', stop_words='english')\n",
    "# print(get_num_features(vect, X))\n",
    "\n",
    "clf = LinearSVC()\n",
    "\n",
    "tfidf_pipeline = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('skb', skb),\n",
    "    ('svd', svd),\n",
    "    ('clf' , clf),\n",
    "])\n",
    "\n",
    "# run_gridsearch_cv(tfidf_pipeline, X, y, param_grid, n_jobs)\n",
    "run_experiment(X, y, tfidf_pipeline, \"CrowdFlower: LSA - TfidfVectorizer[character]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NAACL 2016 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the labels of the test set...\n",
      "16187 documents\n",
      "3 categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:06<00:00,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: NAACL2016: TfidfVectorizer[character]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       none       0.87      0.92      0.89      2267\n",
      "     racism       0.74      0.70      0.72       369\n",
      "     sexism       0.80      0.67      0.73       602\n",
      "\n",
      "avg / total       0.84      0.85      0.84      3238\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_set = df_naacl.sample(n=int(len(df_naacl)), random_state=1965)\n",
    "X = train_test_set['text']\n",
    "y = train_test_set['annotation']\n",
    "\n",
    "print(\"Predicting the labels of the test set...\")\n",
    "print(\"%d documents\" % len(X))\n",
    "print(\"%d categories\" % len(y.value_counts()))\n",
    "\n",
    "N_FEATURES_OPTIONS = [100000, 200000]\n",
    "N_COMPONENTS = [120]\n",
    "n_jobs = 2\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'skb__k': N_FEATURES_OPTIONS,\n",
    "        'svd__n_components': N_COMPONENTS\n",
    "    }\n",
    "]\n",
    "\n",
    "# Params learned through GridSearch\n",
    "k_features = 150000\n",
    "n_components = 120\n",
    "\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "skb = SelectKBest(chi2, k=k_features)\n",
    "vect = TfidfVectorizer(ngram_range=(3,5), analyzer='char', stop_words='english')\n",
    "# print(get_num_features(vect, X))\n",
    "\n",
    "clf = LinearSVC()\n",
    "tfidf_pipeline = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('skb', skb),\n",
    "#     ('svd', svd),\n",
    "    ('clf' , clf),\n",
    "])\n",
    "\n",
    "# run_gridsearch_cv(tfidf_pipeline, X, y, param_grid, n_jobs)\n",
    "run_experiment(X, y, tfidf_pipeline, \"NAACL2016: TfidfVectorizer[character]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency tuple experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: CrowdFlower: FeatureHasher\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.51      0.22      0.30       454\n",
      "not_offensive       0.87      0.96      0.91      2448\n",
      "\n",
      "  avg / total       0.81      0.84      0.82      2902\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_set = feat_df.sample(n=int(len(feat_df)), random_state=1965)\n",
    "X = train_test_set['feat_word_root_rootparent']\n",
    "y = train_test_set['annotation_label']\n",
    "\n",
    "hasher = FeatureHasher(input_type='string', non_negative=True)\n",
    "clf = LinearSVC()\n",
    "\n",
    "featurer_hasher_pipeline = Pipeline([\n",
    "    ('hasher', hasher),\n",
    "    ('clf' , clf),\n",
    "])\n",
    "run_experiment(X, y, featurer_hasher_pipeline, \"CrowdFlower: FeatureHasher\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Feature Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update feature with top k similar words from Dependency2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hs_candidates_exp6_word = \"data/persistence/word_embeddings/dim200vecs_hs_candidates_exp6\"\n",
    "hs_candidates_exp6_model = models.KeyedVectors.load_word2vec_format(hs_candidates_exp6_word, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_top_k_similar(model,row,field_name,k):\n",
    "    similar_words = []\n",
    "    for word in row[field_name]:\n",
    "        if word in model.vocab:\n",
    "            matches = model.similar_by_word(word, topn=k, restrict_vocab=None)\n",
    "            for m in matches:\n",
    "                similar_words.append(m[0])\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_naacl['similar_hs_keywords'] = df_naacl.apply(lambda row: fetch_top_k_similar(hs_candidates_exp6_model, row, 'hs_keyword_matches', 5), axis=1)\n",
    "feat_df['similar_hs_keywords'] = feat_df.apply(lambda row: fetch_top_k_similar(hs_candidates_exp6_model, row, 'hs_keyword_matches', 5), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup column extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Adapted from code by @zacstewart \n",
    "       https://github.com/zacstewart/kaggle_seeclickfix/blob/master/estimator.py\n",
    "       Also see Zac Stewart's excellent blogpost on pipelines:\n",
    "       http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "       \"\"\"\n",
    "    \n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def transform(self, df):\n",
    "        # select the relevant column and return it as a numpy array\n",
    "        # set the array type to be string\n",
    "        return np.asarray(df[self.column_name]).astype(str)\n",
    "        \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "class TextListExtractor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df[self.column_name].tolist()\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self    \n",
    "\n",
    "class Apply(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies a function f element-wise to the numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fn):\n",
    "        self.fn = np.vectorize(fn)\n",
    "        \n",
    "    def transform(self, data):\n",
    "        # note: reshaping is necessary because otherwise sklearn\n",
    "        # interprets 1-d array as a single sample\n",
    "        return self.fn(data.reshape(data.size, 1))\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "class BooleanExtractor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def transform(self, df):\n",
    "        # select the relevant column and return it as a numpy array\n",
    "        # set the array type to be string\n",
    "        return np.asarray(df[self.column_name]).astype(np.int)\n",
    "                                                       \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "empty_analyzer = lambda x: x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: TfidfVectorizer\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       none       0.86      0.94      0.90      2225\n",
      "     racism       0.76      0.67      0.71       363\n",
      "     sexism       0.87      0.64      0.74       650\n",
      "\n",
      "avg / total       0.85      0.85      0.84      3238\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_set = df_naacl.sample(n=int(len(df_naacl)), random_state=1965)\n",
    "X = train_test_set[['text', 'feat_dep_unigrams', 'hs_keyword_count', 'similar_hs_keywords']]\n",
    "y = train_test_set['annotation']\n",
    "clf = LinearSVC()\n",
    "\n",
    "# Setup char ngram pipeline\n",
    "char_vect = TfidfVectorizer(ngram_range=(3,5), analyzer='char', stop_words='english')\n",
    "char_tfidf_pipeline = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')),\n",
    "    ('char_vect', char_vect)\n",
    "])\n",
    "\n",
    "# Setup feature tf-idf vectorizer\n",
    "dep_context_vect = TfidfVectorizer(analyzer=empty_analyzer, max_df=0.3)\n",
    "dependency_context_pipeline = Pipeline([\n",
    "    ('dep_extractor', TextListExtractor('feat_dep_unigrams')), # extract names from df\n",
    "    ('dep_vect', dep_context_vect)\n",
    "])\n",
    "\n",
    "\n",
    "hs_keyword_count_pipeline = Pipeline([\n",
    "    ('count_extractor', BooleanExtractor('hs_keyword_count')),\n",
    "    ('identity', Apply(lambda x: x))\n",
    "])\n",
    "\n",
    "# Setup similar hs_keywords vectorizer\n",
    "dep2vec_vect = TfidfVectorizer(analyzer=empty_analyzer)\n",
    "dep2vec_similarity_pipeline = Pipeline([\n",
    "    ('dep2vec_extractor', TextListExtractor('similar_hs_keywords')),\n",
    "    ('dep2vec_vect', dep2vec_vect)\n",
    "])\n",
    "\n",
    "k_features = 200000\n",
    "n_components = 120\n",
    "skb = SelectKBest(chi2, k=k_features)\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('all_features', FeatureUnion([\n",
    "        ('char_tfidf_pipeline', char_tfidf_pipeline),\n",
    "        ('dependency_context_pipeline', dependency_context_pipeline),\n",
    "#         ('hs_keyword_count_pipeline', hs_keyword_count_pipeline),\n",
    "        ('dep2vec_similarity_pipeline', dep2vec_similarity_pipeline)\n",
    "    ])),\n",
    "    ('skb', skb),\n",
    "    ('clf' , clf)\n",
    "])\n",
    "\n",
    "run_experiment(X, y, pipeline, \"TfidfVectorizer\")\n",
    "\n",
    "# https://github.com/michelleful/SingaporeRoadnameOrigins/blob/master/notebooks/04%20Adding%20features%20with%20Pipelines.ipynb\n",
    "# https://github.com/amueller/kaggle_insults/blob/e4abac805be1d1e2b3201a978172bafd36cc01e3/features.py\n",
    "# http://www.markhneedham.com/blog/2015/03/02/python-scikit-learn-training-a-classifier-with-non-numeric-features/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>annotation</th>\n",
       "      <th>avg_token_length</th>\n",
       "      <th>brown_cluster_ids</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>feat_dep_bigrams</th>\n",
       "      <th>feat_dep_trigrams</th>\n",
       "      <th>feat_dep_unigrams</th>\n",
       "      <th>feat_dependency_contexts</th>\n",
       "      <th>feat_pos_dep_rootPos</th>\n",
       "      <th>feat_word_dep_root</th>\n",
       "      <th>feat_word_root_rootparent</th>\n",
       "      <th>has_hs_keywords</th>\n",
       "      <th>hs_keyword_count</th>\n",
       "      <th>hs_keyword_matches</th>\n",
       "      <th>text</th>\n",
       "      <th>unknown_words</th>\n",
       "      <th>unknown_words_count</th>\n",
       "      <th>uppercase_token_count</th>\n",
       "      <th>similar_hs_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>591c2a2065419158a43b8e5a</td>\n",
       "      <td>racism</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[1726, 31978, 116, 218, 250, 4586, 2218, 4906,...</td>\n",
       "      <td>20</td>\n",
       "      <td>[so_said_advmod_RB|drasko_said_nsubj_NNP, dras...</td>\n",
       "      <td>[so_said_advmod_RB|drasko_said_nsubj_NNP|just_...</td>\n",
       "      <td>[so_said_advmod_RB, drasko_said_nsubj_NNP, jus...</td>\n",
       "      <td>[so_said_advmodINV, drasko_said_nsubjINV, just...</td>\n",
       "      <td>[RB_advmod_VBD, NNP_nsubj_VBD, RB_advmod_VBD, ...</td>\n",
       "      <td>[so_advmod_said, drasko_nsubj_said, just_advmo...</td>\n",
       "      <td>[so_said_said, drasko_said_said, just_said_sai...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>So Drasko just said he was impressed the girls...</td>\n",
       "      <td>[drasko]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>591c2a2065419158a43b8e5b</td>\n",
       "      <td>racism</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[90, 2485, 602, 3877]</td>\n",
       "      <td>10</td>\n",
       "      <td>[drasko_drasko_ROOT_NNP|they_cook_nsubj_PRP, t...</td>\n",
       "      <td>[drasko_drasko_ROOT_NNP|they_cook_nsubj_PRP|di...</td>\n",
       "      <td>[drasko_drasko_ROOT_NNP, they_cook_nsubj_PRP, ...</td>\n",
       "      <td>[drasko_cook_ccomp, they_cook_nsubjINV, didn't...</td>\n",
       "      <td>[NNP_ROOT_NNP, PRP_nsubj_VB, MD_aux_VB, VB_cco...</td>\n",
       "      <td>[drasko_ROOT_drasko, they_nsubj_cook, didn't_a...</td>\n",
       "      <td>[drasko_drasko_drasko, they_cook_drasko, didn'...</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>[idiot, bird]</td>\n",
       "      <td>Drasko they didn't cook half a bird you idiot ...</td>\n",
       "      <td>[drasko]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[moron, bint, buffoon, imbecile, rube, lamb, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>591c2a2065419158a43b8e5c</td>\n",
       "      <td>racism</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[1726, 30698, 77, 60, 28]</td>\n",
       "      <td>10</td>\n",
       "      <td>[hopefully_cooks_advmod_RB|someone_cooks_nsubj...</td>\n",
       "      <td>[hopefully_cooks_advmod_RB|someone_cooks_nsubj...</td>\n",
       "      <td>[hopefully_cooks_advmod_RB, someone_cooks_nsub...</td>\n",
       "      <td>[hopefully_cooks_advmodINV, someone_cooks_nsub...</td>\n",
       "      <td>[RB_advmod_VBZ, NN_nsubj_VBZ, VBZ_ROOT_VBZ, NN...</td>\n",
       "      <td>[hopefully_advmod_cooks, someone_nsubj_cooks, ...</td>\n",
       "      <td>[hopefully_cooks_cooks, someone_cooks_cooks, c...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Hopefully someone cooks Drasko in the next ep ...</td>\n",
       "      <td>[drasko, ep]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>591c2a2065419158a43b8e5d</td>\n",
       "      <td>racism</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[28, 1009, 602, 506, 1706, 60, 3494, 966, 212,...</td>\n",
       "      <td>16</td>\n",
       "      <td>[of_born_prep_IN|course_of_pobj_NN, course_of_...</td>\n",
       "      <td>[of_born_prep_IN|course_of_pobj_NN|you_born_ns...</td>\n",
       "      <td>[of_born_prep_IN, course_of_pobj_NN, you_born_...</td>\n",
       "      <td>[of_course_pobj, of_born_prepINV, course_of_po...</td>\n",
       "      <td>[IN_prep_VBN, NN_pobj_IN, PRP_nsubjpass_VBN, V...</td>\n",
       "      <td>[of_prep_born, course_pobj_of, you_nsubjpass_b...</td>\n",
       "      <td>[of_born_born, course_of_born, you_born_born, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>of course you were born in serbia ... you're a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>591c2a2065419158a43b8e5e</td>\n",
       "      <td>racism</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[1530, 28, 19, 853, 189, 6442, 8, 1726, 26282,...</td>\n",
       "      <td>21</td>\n",
       "      <td>[these girls_are_nsubj_NNS|are_are_ROOT_VBP, a...</td>\n",
       "      <td>[these girls_are_nsubj_NNS|are_are_ROOT_VBP|th...</td>\n",
       "      <td>[these girls_are_nsubj_NNS, are_are_ROOT_VBP, ...</td>\n",
       "      <td>[these girls_are_nsubjINV, are_these girls_nsu...</td>\n",
       "      <td>[NNS_nsubj_VBP, VBP_ROOT_VBP, NN_attr_VBP, IN_...</td>\n",
       "      <td>[these girls_nsubj_are, are_ROOT_are, the equi...</td>\n",
       "      <td>[these girls_are_are, are_are_are, the equival...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>These girls are the equivalent of the irritati...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id annotation  avg_token_length  \\\n",
       "0  591c2a2065419158a43b8e5a     racism               4.0   \n",
       "1  591c2a2065419158a43b8e5b     racism               4.0   \n",
       "2  591c2a2065419158a43b8e5c     racism               4.0   \n",
       "3  591c2a2065419158a43b8e5d     racism               4.0   \n",
       "4  591c2a2065419158a43b8e5e     racism               4.0   \n",
       "\n",
       "                                   brown_cluster_ids  comment_length  \\\n",
       "0  [1726, 31978, 116, 218, 250, 4586, 2218, 4906,...              20   \n",
       "1                              [90, 2485, 602, 3877]              10   \n",
       "2                          [1726, 30698, 77, 60, 28]              10   \n",
       "3  [28, 1009, 602, 506, 1706, 60, 3494, 966, 212,...              16   \n",
       "4  [1530, 28, 19, 853, 189, 6442, 8, 1726, 26282,...              21   \n",
       "\n",
       "                                    feat_dep_bigrams  \\\n",
       "0  [so_said_advmod_RB|drasko_said_nsubj_NNP, dras...   \n",
       "1  [drasko_drasko_ROOT_NNP|they_cook_nsubj_PRP, t...   \n",
       "2  [hopefully_cooks_advmod_RB|someone_cooks_nsubj...   \n",
       "3  [of_born_prep_IN|course_of_pobj_NN, course_of_...   \n",
       "4  [these girls_are_nsubj_NNS|are_are_ROOT_VBP, a...   \n",
       "\n",
       "                                   feat_dep_trigrams  \\\n",
       "0  [so_said_advmod_RB|drasko_said_nsubj_NNP|just_...   \n",
       "1  [drasko_drasko_ROOT_NNP|they_cook_nsubj_PRP|di...   \n",
       "2  [hopefully_cooks_advmod_RB|someone_cooks_nsubj...   \n",
       "3  [of_born_prep_IN|course_of_pobj_NN|you_born_ns...   \n",
       "4  [these girls_are_nsubj_NNS|are_are_ROOT_VBP|th...   \n",
       "\n",
       "                                   feat_dep_unigrams  \\\n",
       "0  [so_said_advmod_RB, drasko_said_nsubj_NNP, jus...   \n",
       "1  [drasko_drasko_ROOT_NNP, they_cook_nsubj_PRP, ...   \n",
       "2  [hopefully_cooks_advmod_RB, someone_cooks_nsub...   \n",
       "3  [of_born_prep_IN, course_of_pobj_NN, you_born_...   \n",
       "4  [these girls_are_nsubj_NNS, are_are_ROOT_VBP, ...   \n",
       "\n",
       "                            feat_dependency_contexts  \\\n",
       "0  [so_said_advmodINV, drasko_said_nsubjINV, just...   \n",
       "1  [drasko_cook_ccomp, they_cook_nsubjINV, didn't...   \n",
       "2  [hopefully_cooks_advmodINV, someone_cooks_nsub...   \n",
       "3  [of_course_pobj, of_born_prepINV, course_of_po...   \n",
       "4  [these girls_are_nsubjINV, are_these girls_nsu...   \n",
       "\n",
       "                                feat_pos_dep_rootPos  \\\n",
       "0  [RB_advmod_VBD, NNP_nsubj_VBD, RB_advmod_VBD, ...   \n",
       "1  [NNP_ROOT_NNP, PRP_nsubj_VB, MD_aux_VB, VB_cco...   \n",
       "2  [RB_advmod_VBZ, NN_nsubj_VBZ, VBZ_ROOT_VBZ, NN...   \n",
       "3  [IN_prep_VBN, NN_pobj_IN, PRP_nsubjpass_VBN, V...   \n",
       "4  [NNS_nsubj_VBP, VBP_ROOT_VBP, NN_attr_VBP, IN_...   \n",
       "\n",
       "                                  feat_word_dep_root  \\\n",
       "0  [so_advmod_said, drasko_nsubj_said, just_advmo...   \n",
       "1  [drasko_ROOT_drasko, they_nsubj_cook, didn't_a...   \n",
       "2  [hopefully_advmod_cooks, someone_nsubj_cooks, ...   \n",
       "3  [of_prep_born, course_pobj_of, you_nsubjpass_b...   \n",
       "4  [these girls_nsubj_are, are_ROOT_are, the equi...   \n",
       "\n",
       "                           feat_word_root_rootparent  has_hs_keywords  \\\n",
       "0  [so_said_said, drasko_said_said, just_said_sai...            False   \n",
       "1  [drasko_drasko_drasko, they_cook_drasko, didn'...             True   \n",
       "2  [hopefully_cooks_cooks, someone_cooks_cooks, c...            False   \n",
       "3  [of_born_born, course_of_born, you_born_born, ...            False   \n",
       "4  [these girls_are_are, are_are_are, the equival...            False   \n",
       "\n",
       "   hs_keyword_count hs_keyword_matches  \\\n",
       "0                 0                 []   \n",
       "1                 2      [idiot, bird]   \n",
       "2                 0                 []   \n",
       "3                 0                 []   \n",
       "4                 0                 []   \n",
       "\n",
       "                                                text unknown_words  \\\n",
       "0  So Drasko just said he was impressed the girls...      [drasko]   \n",
       "1  Drasko they didn't cook half a bird you idiot ...      [drasko]   \n",
       "2  Hopefully someone cooks Drasko in the next ep ...  [drasko, ep]   \n",
       "3  of course you were born in serbia ... you're a...            []   \n",
       "4  These girls are the equivalent of the irritati...            []   \n",
       "\n",
       "   unknown_words_count  uppercase_token_count  \\\n",
       "0                    1                      1   \n",
       "1                    1                      0   \n",
       "2                    2                      1   \n",
       "3                    0                      1   \n",
       "4                    0                      1   \n",
       "\n",
       "                                 similar_hs_keywords  \n",
       "0                                                 []  \n",
       "1  [moron, bint, buffoon, imbecile, rube, lamb, l...  \n",
       "2                                                 []  \n",
       "3                                                 []  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_naacl.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
