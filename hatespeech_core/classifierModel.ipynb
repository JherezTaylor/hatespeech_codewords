{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Retrieve parsed collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to DB at mongodb://140.114.79.146:27017 successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>annotation_label</th>\n",
       "      <th>avg_token_length</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>brown_cluster_ids</th>\n",
       "      <th>char_pentagrams</th>\n",
       "      <th>char_quadgrams</th>\n",
       "      <th>char_trigrams</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>conllFormat</th>\n",
       "      <th>...</th>\n",
       "      <th>noun_chunks</th>\n",
       "      <th>pos_dep_rootPos</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>unknown_words</th>\n",
       "      <th>unknown_words_count</th>\n",
       "      <th>uppercase_token_count</th>\n",
       "      <th>word_dep_root</th>\n",
       "      <th>word_root_preRoot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58c659be6541913eb7f119dd</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[Warning penny, penny boards, boards will, wil...</td>\n",
       "      <td>[966, 228, 989, 333, 442, 4618, 602, 19]</td>\n",
       "      <td>[penny, warni, arnin, aggot, board, rning, fag...</td>\n",
       "      <td>[aggo, warn, arni, penn, oard, ards, enny, nin...</td>\n",
       "      <td>[pen, war, nin, mak, enn, you, agg, ggo, fag, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>[1\\twarning\\twarning\\tNOUN\\tNN\\t_\\t0\\tROOT\\t0:...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'root': 'warning', 'text': 'warning'}, {'roo...</td>\n",
       "      <td>[{'pos': 'NN', 'rootPos': 'NN', 'dep': 'ROOT'}...</td>\n",
       "      <td>Warning : penny boards will make you a faggot</td>\n",
       "      <td>[warning, penny, boards, faggot]</td>\n",
       "      <td>[Warning penny boards, penny boards will, boar...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'word': 'warning', 'dep': 'ROOT', 'root': 'w...</td>\n",
       "      <td>[{'preRoot': 'warning', 'word': 'warning', 'ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58c659be6541913eb7f119de</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[Fuck dykes]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[dykes]</td>\n",
       "      <td>[ykes, dyke, fuck]</td>\n",
       "      <td>[fuc, kes, yke, dyk, uck]</td>\n",
       "      <td>2</td>\n",
       "      <td>[1\\tfuck\\tfuck\\tPROPN\\tNNP\\t_\\t2\\tnsubj\\t2:nsu...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'root': 'dykes', 'text': 'fuck'}]</td>\n",
       "      <td>[{'pos': 'NNP', 'rootPos': 'VBZ', 'dep': 'nsub...</td>\n",
       "      <td>Fuck dykes</td>\n",
       "      <td>[fuck, dykes]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'word': 'fuck', 'dep': 'nsubj', 'root': 'dyk...</td>\n",
       "      <td>[{'preRoot': 'dykes', 'word': 'fuck', 'root': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58c659be6541913eb7f119df</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[user_mention user_mention, user_mention user_...</td>\n",
       "      <td>[124, 3690, 966, 2442, 1684, 86]</td>\n",
       "      <td>[efree, aggot, least, jefre, starr, faggo]</td>\n",
       "      <td>[star, east, tarr, aggo, like, dont, jefr, lea...</td>\n",
       "      <td>[ont, ike, ast, lik, ook, agg, ggo, arr, fag, ...</td>\n",
       "      <td>14</td>\n",
       "      <td>[1\\tuser_mention\\tuser_mention\\tNOUN\\tNN\\t_\\t5...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'root': 'look', 'text': 'i'}, {'root': 'like...</td>\n",
       "      <td>[{'pos': 'NN', 'rootPos': 'NN', 'dep': 'compou...</td>\n",
       "      <td>user_mention user_mention user_mention user_me...</td>\n",
       "      <td>[user_mention, user_mention, user_mention, use...</td>\n",
       "      <td>[user_mention user_mention user_mention, user_...</td>\n",
       "      <td>[user_mention, user_mention, user_mention, use...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'word': 'user_mention', 'dep': 'compound', '...</td>\n",
       "      <td>[{'preRoot': 'look', 'word': 'user_mention', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58c659be6541913eb7f119e0</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[user_mention user_mention, user_mention user_...</td>\n",
       "      <td>[228, 228, 1214, 19, 28650, 1014, 981]</td>\n",
       "      <td>[jacki, eeeee, ealou, jealo, ackie, alous, neeee]</td>\n",
       "      <td>[alou, neee, acki, jack, lous, eeee, ealo, cki...</td>\n",
       "      <td>[ack, kie, ous, cki, jea, nee, alo, lou, eal, ...</td>\n",
       "      <td>15</td>\n",
       "      <td>[1\\t\"\\t\"\\tPUNCT\\t``\\t_\\t15\\tpunct\\t15:punct\\t_...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'root': 'neeeee', 'text': 'user_mention'}, {...</td>\n",
       "      <td>[{'pos': 'NN', 'rootPos': 'NNP', 'dep': 'nsubj...</td>\n",
       "      <td>\" user_mention : \" user_mention : user_mention...</td>\n",
       "      <td>[user_mention, user_mention, user_mention, fag...</td>\n",
       "      <td>[user_mention user_mention user_mention, user_...</td>\n",
       "      <td>[user_mention, user_mention, user_mention, nee...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'word': 'user_mention', 'dep': 'nsubj', 'roo...</td>\n",
       "      <td>[{'preRoot': 'neeeee', 'word': 'user_mention',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58c659be6541913eb7f119e1</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[user_mention You, You heard, heard me, me bit...</td>\n",
       "      <td>[858, 26282, 1898, 2485, 148, 12266, 1349, 753...</td>\n",
       "      <td>[talki, texas, nigga, heard, alkin, lking, bit...</td>\n",
       "      <td>[exas, talk, alki, nigg, back, abou, hear, ear...</td>\n",
       "      <td>[itc, alk, lki, nig, kin, tex, hea, you, but, ...</td>\n",
       "      <td>20</td>\n",
       "      <td>[1\\tuser_mention\\tuser_mention\\tNOUN\\tNN\\t_\\t0...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'root': 'user_mention', 'text': 'user_mentio...</td>\n",
       "      <td>[{'pos': 'NN', 'rootPos': 'NN', 'dep': 'ROOT'}...</td>\n",
       "      <td>user_mention You heard me bitch but any way I'...</td>\n",
       "      <td>[user_mention, heard, bitch, way, i'm, th, tex...</td>\n",
       "      <td>[user_mention You heard, You heard me, heard m...</td>\n",
       "      <td>[user_mention]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'word': 'user_mention', 'dep': 'ROOT', 'root...</td>\n",
       "      <td>[{'preRoot': 'user_mention', 'word': 'user_men...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id annotation_label  avg_token_length  \\\n",
       "0  58c659be6541913eb7f119dd    not_offensive               4.0   \n",
       "1  58c659be6541913eb7f119de       hatespeech               4.0   \n",
       "2  58c659be6541913eb7f119df       hatespeech               7.0   \n",
       "3  58c659be6541913eb7f119e0       hatespeech               4.0   \n",
       "4  58c659be6541913eb7f119e1    not_offensive               4.0   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [Warning penny, penny boards, boards will, wil...   \n",
       "1                                       [Fuck dykes]   \n",
       "2  [user_mention user_mention, user_mention user_...   \n",
       "3  [user_mention user_mention, user_mention user_...   \n",
       "4  [user_mention You, You heard, heard me, me bit...   \n",
       "\n",
       "                                   brown_cluster_ids  \\\n",
       "0           [966, 228, 989, 333, 442, 4618, 602, 19]   \n",
       "1                                                 []   \n",
       "2                   [124, 3690, 966, 2442, 1684, 86]   \n",
       "3             [228, 228, 1214, 19, 28650, 1014, 981]   \n",
       "4  [858, 26282, 1898, 2485, 148, 12266, 1349, 753...   \n",
       "\n",
       "                                     char_pentagrams  \\\n",
       "0  [penny, warni, arnin, aggot, board, rning, fag...   \n",
       "1                                            [dykes]   \n",
       "2         [efree, aggot, least, jefre, starr, faggo]   \n",
       "3  [jacki, eeeee, ealou, jealo, ackie, alous, neeee]   \n",
       "4  [talki, texas, nigga, heard, alkin, lking, bit...   \n",
       "\n",
       "                                      char_quadgrams  \\\n",
       "0  [aggo, warn, arni, penn, oard, ards, enny, nin...   \n",
       "1                                 [ykes, dyke, fuck]   \n",
       "2  [star, east, tarr, aggo, like, dont, jefr, lea...   \n",
       "3  [alou, neee, acki, jack, lous, eeee, ealo, cki...   \n",
       "4  [exas, talk, alki, nigg, back, abou, hear, ear...   \n",
       "\n",
       "                                       char_trigrams  comment_length  \\\n",
       "0  [pen, war, nin, mak, enn, you, agg, ggo, fag, ...               9   \n",
       "1                          [fuc, kes, yke, dyk, uck]               2   \n",
       "2  [ont, ike, ast, lik, ook, agg, ggo, arr, fag, ...              14   \n",
       "3  [ack, kie, ous, cki, jea, nee, alo, lou, eal, ...              15   \n",
       "4  [itc, alk, lki, nig, kin, tex, hea, you, but, ...              20   \n",
       "\n",
       "                                         conllFormat  \\\n",
       "0  [1\\twarning\\twarning\\tNOUN\\tNN\\t_\\t0\\tROOT\\t0:...   \n",
       "1  [1\\tfuck\\tfuck\\tPROPN\\tNNP\\t_\\t2\\tnsubj\\t2:nsu...   \n",
       "2  [1\\tuser_mention\\tuser_mention\\tNOUN\\tNN\\t_\\t5...   \n",
       "3  [1\\t\"\\t\"\\tPUNCT\\t``\\t_\\t15\\tpunct\\t15:punct\\t_...   \n",
       "4  [1\\tuser_mention\\tuser_mention\\tNOUN\\tNN\\t_\\t0...   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "1                        ...                           \n",
       "2                        ...                           \n",
       "3                        ...                           \n",
       "4                        ...                           \n",
       "\n",
       "                                         noun_chunks  \\\n",
       "0  [{'root': 'warning', 'text': 'warning'}, {'roo...   \n",
       "1                [{'root': 'dykes', 'text': 'fuck'}]   \n",
       "2  [{'root': 'look', 'text': 'i'}, {'root': 'like...   \n",
       "3  [{'root': 'neeeee', 'text': 'user_mention'}, {...   \n",
       "4  [{'root': 'user_mention', 'text': 'user_mentio...   \n",
       "\n",
       "                                     pos_dep_rootPos  \\\n",
       "0  [{'pos': 'NN', 'rootPos': 'NN', 'dep': 'ROOT'}...   \n",
       "1  [{'pos': 'NNP', 'rootPos': 'VBZ', 'dep': 'nsub...   \n",
       "2  [{'pos': 'NN', 'rootPos': 'NN', 'dep': 'compou...   \n",
       "3  [{'pos': 'NN', 'rootPos': 'NNP', 'dep': 'nsubj...   \n",
       "4  [{'pos': 'NN', 'rootPos': 'NN', 'dep': 'ROOT'}...   \n",
       "\n",
       "                                                text  \\\n",
       "0     Warning : penny boards will make you a faggot    \n",
       "1                                        Fuck dykes    \n",
       "2  user_mention user_mention user_mention user_me...   \n",
       "3  \" user_mention : \" user_mention : user_mention...   \n",
       "4  user_mention You heard me bitch but any way I'...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                   [warning, penny, boards, faggot]   \n",
       "1                                      [fuck, dykes]   \n",
       "2  [user_mention, user_mention, user_mention, use...   \n",
       "3  [user_mention, user_mention, user_mention, fag...   \n",
       "4  [user_mention, heard, bitch, way, i'm, th, tex...   \n",
       "\n",
       "                                            trigrams  \\\n",
       "0  [Warning penny boards, penny boards will, boar...   \n",
       "1                                                 []   \n",
       "2  [user_mention user_mention user_mention, user_...   \n",
       "3  [user_mention user_mention user_mention, user_...   \n",
       "4  [user_mention You heard, You heard me, heard m...   \n",
       "\n",
       "                                       unknown_words unknown_words_count  \\\n",
       "0                                                 []                   0   \n",
       "1                                                 []                   0   \n",
       "2  [user_mention, user_mention, user_mention, use...                   6   \n",
       "3  [user_mention, user_mention, user_mention, nee...                   4   \n",
       "4                                     [user_mention]                   1   \n",
       "\n",
       "  uppercase_token_count                                      word_dep_root  \\\n",
       "0                     0  [{'word': 'warning', 'dep': 'ROOT', 'root': 'w...   \n",
       "1                     0  [{'word': 'fuck', 'dep': 'nsubj', 'root': 'dyk...   \n",
       "2                     0  [{'word': 'user_mention', 'dep': 'compound', '...   \n",
       "3                     0  [{'word': 'user_mention', 'dep': 'nsubj', 'roo...   \n",
       "4                     0  [{'word': 'user_mention', 'dep': 'ROOT', 'root...   \n",
       "\n",
       "                                   word_root_preRoot  \n",
       "0  [{'preRoot': 'warning', 'word': 'warning', 'ro...  \n",
       "1  [{'preRoot': 'dykes', 'word': 'fuck', 'root': ...  \n",
       "2  [{'preRoot': 'look', 'word': 'user_mention', '...  \n",
       "3  [{'preRoot': 'neeeee', 'word': 'user_mention',...  \n",
       "4  [{'preRoot': 'user_mention', 'word': 'user_men...  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import joblib\n",
    "from modules.db import mongo_base\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import FeatureHasher, DictVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from modules.utils.CustomTwokenizer import CustomTwokenizer\n",
    "\n",
    "connection_params = [\"twitter\", \"crowdflower_features\"]\n",
    "client = mongo_base.connect()\n",
    "db_name = connection_params[0]\n",
    "connection_params.insert(0, client)\n",
    "\n",
    "query = {}\n",
    "query[\"filter\"] = {}\n",
    "query[\"projection\"] = {}\n",
    "query[\"limit\"] = 0\n",
    "query[\"skip\"] = 0\n",
    "query[\"no_cursor_timeout\"] = True\n",
    "cursor = mongo_base.finder(connection_params, query, False)\n",
    "df = pd.DataFrame(list(cursor))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/persistence/crowdflower_features.pkl.compressed']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crowdflower_persistence = 'data/persistence/crowdflower_features.pkl.compressed'\n",
    "nlp = spacy.load('en', create_make_doc=CustomTwokenizer)\n",
    "joblib.dump(df, crowdflower_persistence, compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = joblib.load(crowdflower_persistence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>annotation_label</th>\n",
       "      <th>avg_token_length</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>brown_cluster_ids</th>\n",
       "      <th>char_pentagrams</th>\n",
       "      <th>char_quadgrams</th>\n",
       "      <th>char_trigrams</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>conllFormat</th>\n",
       "      <th>...</th>\n",
       "      <th>noun_chunks</th>\n",
       "      <th>pos_dep_rootPos</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>unknown_words</th>\n",
       "      <th>unknown_words_count</th>\n",
       "      <th>uppercase_token_count</th>\n",
       "      <th>word_dep_root</th>\n",
       "      <th>word_root_preRoot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58c659be6541913eb7f119dd</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[Warning penny, penny boards, boards will, wil...</td>\n",
       "      <td>[966, 228, 989, 333, 442, 4618, 602, 19]</td>\n",
       "      <td>[penny, warni, arnin, aggot, board, rning, fag...</td>\n",
       "      <td>[aggo, warn, arni, penn, oard, ards, enny, nin...</td>\n",
       "      <td>[pen, war, nin, mak, enn, you, agg, ggo, fag, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>[1 warning warning NOUN NN _ 0 ROOT 0:ROOT _, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'text': 'warning', 'root': 'warning'}, {'tex...</td>\n",
       "      <td>[{'pos': 'NN', 'dep': 'ROOT', 'rootPos': 'NN'}...</td>\n",
       "      <td>Warning : penny boards will make you a faggot</td>\n",
       "      <td>[warning, penny, boards, faggot]</td>\n",
       "      <td>[Warning penny boards, penny boards will, boar...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'word': 'warning', 'dep': 'ROOT', 'root': 'w...</td>\n",
       "      <td>[{'word': 'warning', 'preRoot': 'warning', 'ro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id annotation_label  avg_token_length  \\\n",
       "0  58c659be6541913eb7f119dd    not_offensive               4.0   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [Warning penny, penny boards, boards will, wil...   \n",
       "\n",
       "                          brown_cluster_ids  \\\n",
       "0  [966, 228, 989, 333, 442, 4618, 602, 19]   \n",
       "\n",
       "                                     char_pentagrams  \\\n",
       "0  [penny, warni, arnin, aggot, board, rning, fag...   \n",
       "\n",
       "                                      char_quadgrams  \\\n",
       "0  [aggo, warn, arni, penn, oard, ards, enny, nin...   \n",
       "\n",
       "                                       char_trigrams  comment_length  \\\n",
       "0  [pen, war, nin, mak, enn, you, agg, ggo, fag, ...               9   \n",
       "\n",
       "                                         conllFormat  \\\n",
       "0  [1 warning warning NOUN NN _ 0 ROOT 0:ROOT _, ...   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "\n",
       "                                         noun_chunks  \\\n",
       "0  [{'text': 'warning', 'root': 'warning'}, {'tex...   \n",
       "\n",
       "                                     pos_dep_rootPos  \\\n",
       "0  [{'pos': 'NN', 'dep': 'ROOT', 'rootPos': 'NN'}...   \n",
       "\n",
       "                                             text  \\\n",
       "0  Warning : penny boards will make you a faggot    \n",
       "\n",
       "                             tokens  \\\n",
       "0  [warning, penny, boards, faggot]   \n",
       "\n",
       "                                            trigrams unknown_words  \\\n",
       "0  [Warning penny boards, penny boards will, boar...            []   \n",
       "\n",
       "  unknown_words_count  uppercase_token_count  \\\n",
       "0                   0                      0   \n",
       "\n",
       "                                       word_dep_root  \\\n",
       "0  [{'word': 'warning', 'dep': 'ROOT', 'root': 'w...   \n",
       "\n",
       "                                   word_root_preRoot  \n",
       "0  [{'word': 'warning', 'preRoot': 'warning', 'ro...  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:1]['dep_bigrams'][0][0]\n",
    "hasher = FeatureHasher(input_type='string', non_negative=True)\n",
    "v = DictVectorizer(sparse=False)\n",
    "# raw_X = hasher.transform(df[0:1]['brown_cluster_ids'])\n",
    "df[0:1]['bigrams'][0][0]\n",
    "D = [{'bigram': 'Warning penny'}, {'bigram': 'penny boards'}, {'bigram': 'boards will'}, {'bigram': 'boards will'}]\n",
    "trans = v.fit_transform(D)\n",
    "v.inverse_transform(trans)\n",
    "v.get_feature_names()\n",
    "# raw_X.toarray()\n",
    "# print(df[0:1]['bigrams'][0])\n",
    "df[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Setup data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# let's pick the same random 10% of the data to train with\n",
    "train_test_set = df.sample(n=int(len(df) / 2), random_state=1965)\n",
    "\n",
    "X = train_test_set['text']\n",
    "y = train_test_set['annotation_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the labels of the test set...\n",
      "7254 documents\n",
      "2 categories\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting the labels of the test set...\")\n",
    "print(\"%d documents\" % len(X))\n",
    "print(\"%d categories\" % len(y.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Setup generic model experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(X, y, pipeline, process_name, num_expts=1):\n",
    "    scores = list()\n",
    "    for i in range(num_expts):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "        model = pipeline.fit(X_train, y_train)  # train the classifier\n",
    "        y_prediction = model.predict(X_test)          # apply the model to the test data\n",
    "        report = classification_report(y_test, y_prediction)\n",
    "        score = accuracy_score(y_prediction, y_test)  # compare the results to the gold standard\n",
    "        scores.append(score)\n",
    "        print(\"Classification Report: \" + process_name)\n",
    "        print(report)\n",
    "        cm = confusion_matrix(y_test, y_prediction)\n",
    "        print(\"Confusion matrix:\")\n",
    "        print(cm)\n",
    "    print(sum(scores) / num_expts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Setup naive baseline classification (countVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: CountVectorizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.46      0.34      0.39       314\n",
      "not_offensive       0.87      0.92      0.89      1500\n",
      "\n",
      "  avg / total       0.80      0.82      0.81      1814\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 107  207]\n",
      " [ 124 1376]]\n",
      "0.817530319735\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "# our two ingredients: the ngram counter and the classifier\n",
    "nm = 5000\n",
    "vect = CountVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "ch2 = SelectKBest(chi2, k=nm)\n",
    "\n",
    "# There are just two steps to our process: extracting the ngrams and\n",
    "# putting them through the classifier. So our Pipeline looks like this:\n",
    "\n",
    "count_pipeline = Pipeline([\n",
    "    ('vect', vect),  # extract ngrams from tweet text\n",
    "    ('kBest', ch2),\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "\n",
    "# Run the classifcation\n",
    "run_experiment(X, y, count_pipeline, \"CountVectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Setup naive baseline classification (hashingVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: HashingVectorizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.58      0.36      0.44       322\n",
      "not_offensive       0.87      0.95      0.91      1492\n",
      "\n",
      "  avg / total       0.82      0.84      0.82      1814\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 115  207]\n",
      " [  82 1410]]\n",
      "0.840683572216\n"
     ]
    }
   ],
   "source": [
    "vect = HashingVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "\n",
    "hashing_pipeline = Pipeline([\n",
    "    ('vect', vect),  # extract ngrams from tweet text\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "run_experiment(X, y, hashing_pipeline, \"HashingVectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Setup tf-idf baseline classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: TfidfVectorizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.53      0.35      0.42       292\n",
      "not_offensive       0.88      0.94      0.91      1522\n",
      "\n",
      "  avg / total       0.83      0.85      0.83      1814\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 101  191]\n",
      " [  89 1433]]\n",
      "0.845644983462\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "\n",
    "tfidf_pipeline = Pipeline([\n",
    "    ('vect', vect),  # extract ngrams from tweet text\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "run_experiment(X, y, tfidf_pipeline, \"TfidfVectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Investigate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report : Random Experiment\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.51      0.07      0.12       289\n",
      "not_offensive       0.85      0.99      0.91      1525\n",
      "\n",
      "  avg / total       0.79      0.84      0.79      1814\n",
      "\n",
      "Confusion matrix:\n",
      "[[  19  270]\n",
      " [  18 1507]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>annotation_label</th>\n",
       "      <th>avg_token_length</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>brown_cluster_ids</th>\n",
       "      <th>char_pentagrams</th>\n",
       "      <th>char_quadgrams</th>\n",
       "      <th>char_trigrams</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>conllFormat</th>\n",
       "      <th>...</th>\n",
       "      <th>noun_chunks</th>\n",
       "      <th>pos_dep_rootPos</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>unknown_words</th>\n",
       "      <th>unknown_words_count</th>\n",
       "      <th>uppercase_token_count</th>\n",
       "      <th>word_dep_root</th>\n",
       "      <th>word_root_preRoot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58c659be6541913eb7f119dd</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[Warning penny, penny boards, boards will, wil...</td>\n",
       "      <td>[966, 228, 989, 333, 442, 4618, 602, 19]</td>\n",
       "      <td>[penny, warni, arnin, aggot, board, rning, fag...</td>\n",
       "      <td>[aggo, warn, arni, penn, oard, ards, enny, nin...</td>\n",
       "      <td>[pen, war, nin, mak, enn, you, agg, ggo, fag, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>[1 warning warning NOUN NN _ 0 ROOT 0:ROOT _, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'text': 'warning', 'root': 'warning'}, {'tex...</td>\n",
       "      <td>[{'pos': 'NN', 'dep': 'ROOT', 'rootPos': 'NN'}...</td>\n",
       "      <td>Warning : penny boards will make you a faggot</td>\n",
       "      <td>[warning, penny, boards, faggot]</td>\n",
       "      <td>[Warning penny boards, penny boards will, boar...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'word': 'warning', 'dep': 'ROOT', 'root': 'w...</td>\n",
       "      <td>[{'word': 'warning', 'preRoot': 'warning', 'ro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id annotation_label  avg_token_length  \\\n",
       "0  58c659be6541913eb7f119dd    not_offensive               4.0   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [Warning penny, penny boards, boards will, wil...   \n",
       "\n",
       "                          brown_cluster_ids  \\\n",
       "0  [966, 228, 989, 333, 442, 4618, 602, 19]   \n",
       "\n",
       "                                     char_pentagrams  \\\n",
       "0  [penny, warni, arnin, aggot, board, rning, fag...   \n",
       "\n",
       "                                      char_quadgrams  \\\n",
       "0  [aggo, warn, arni, penn, oard, ards, enny, nin...   \n",
       "\n",
       "                                       char_trigrams  comment_length  \\\n",
       "0  [pen, war, nin, mak, enn, you, agg, ggo, fag, ...               9   \n",
       "\n",
       "                                         conllFormat  \\\n",
       "0  [1 warning warning NOUN NN _ 0 ROOT 0:ROOT _, ...   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "\n",
       "                                         noun_chunks  \\\n",
       "0  [{'text': 'warning', 'root': 'warning'}, {'tex...   \n",
       "\n",
       "                                     pos_dep_rootPos  \\\n",
       "0  [{'pos': 'NN', 'dep': 'ROOT', 'rootPos': 'NN'}...   \n",
       "\n",
       "                                             text  \\\n",
       "0  Warning : penny boards will make you a faggot    \n",
       "\n",
       "                             tokens  \\\n",
       "0  [warning, penny, boards, faggot]   \n",
       "\n",
       "                                            trigrams unknown_words  \\\n",
       "0  [Warning penny boards, penny boards will, boar...            []   \n",
       "\n",
       "  unknown_words_count  uppercase_token_count  \\\n",
       "0                   0                      0   \n",
       "\n",
       "                                       word_dep_root  \\\n",
       "0  [{'word': 'warning', 'dep': 'ROOT', 'root': 'w...   \n",
       "\n",
       "                                   word_root_preRoot  \n",
       "0  [{'word': 'warning', 'preRoot': 'warning', 'ro...  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As much as I want to ignore this, I shouldn't. The fact that the precision score is\n",
    "# close to the other experiments is troubling\n",
    "\n",
    "X = train_test_set[['hs_keyword_count', 'comment_length', 'unknown_words_count']]\n",
    "y = train_test_set['annotation_label']\n",
    "clf = LinearSVC()\n",
    "\n",
    "# X = X.values.reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "model = clf.fit(X_train, y_train)  # train the classifier\n",
    "y_prediction = model.predict(X_test)          # apply the model to the test data\n",
    "report = classification_report(y_test, y_prediction)\n",
    "print(\"Classification Report : Random Experiment\")\n",
    "print(report)\n",
    "cm = confusion_matrix(y_test, y_prediction)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)\n",
    "df[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report : Random Experiment\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.38      0.14      0.20       303\n",
      "not_offensive       0.85      0.95      0.90      1511\n",
      "\n",
      "  avg / total       0.77      0.82      0.78      1814\n",
      "\n",
      "Confusion matrix:\n",
      "[[  41  262]\n",
      " [  68 1443]]\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "X = train_test_set['word_root_preRoot']\n",
    "y = train_test_set['annotation_label']\n",
    "clf = LinearSVC()\n",
    "\n",
    "# http://www.markhneedham.com/blog/2015/03/02/python-scikit-learn-training-a-classifier-with-non-numeric-features/\n",
    "empty_analyzer = lambda x: x\n",
    "vect = DictVectorizer()\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "# vect = TfidfVectorizer(analyzer=empty_analyzer)\n",
    "# hasher = FeatureHasher(input_type='string', non_negative=True)\n",
    "# X = [[str(res) for res in tmp] for tmp in X]# \n",
    "# X = hasher.transform(X)\n",
    "\n",
    "X = X.tolist()\n",
    "X = vect.fit_transform([item[0] for item in X]).toarray()\n",
    "# X = transformer.fit_transform(X)\n",
    "# vect.vocabulary_\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "model = clf.fit(X_train, y_train)  # train the classifier\n",
    "y_prediction = model.predict(X_test)          # apply the model to the test data\n",
    "report = classification_report(y_test, y_prediction)\n",
    "print(\"Classification Report : Random Experiment\")\n",
    "print(report)\n",
    "cm = confusion_matrix(y_test, y_prediction)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'austrailian': 'scientists amod-INV'},\n",
       " {'scientists': 'austrailian amod'},\n",
       " {'scientists': 'scientists ROOT'},\n",
       " {'discovers': 'discovers ROOT'},\n",
       " {'ten': 'stars nummod-INV'},\n",
       " {'stars': 'ten nummod'},\n",
       " {'stars': 'discovers dobj-INV'},\n",
       " {'discovers': 'stars dobj'},\n",
       " {'with': 'stars prep-INV'},\n",
       " {'stars': 'with prep'},\n",
       " {'telescope': 'with pobj-INV'},\n",
       " {'with': 'telescope pobj'}]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string = \"Austrailian scientists .... discovers ten stars with telescope\"\n",
    "doc = nlp(test_string)\n",
    "\n",
    "# elements = []\n",
    "# dict1 = {\"austrailian\": \"scientists amod\"}\n",
    "# dict2 = {\"scientists\": \"austrailian amod\"}\n",
    "# dict3 = {\"scientists\": \"discovers nsubj\"}\n",
    "\n",
    "# elements.append(dict1)\n",
    "# elements.append(dict2)\n",
    "# elements.append(dict3)\n",
    "\n",
    "# vect = DictVectorizer()\n",
    "# X = vect.fit_transform(elements)\n",
    "# X\n",
    "# vect.inverse_transform(X)\n",
    "\n",
    "def extract_dep_context(doc):\n",
    "    dependency_contexts = []\n",
    "    for word in doc:\n",
    "        if (str(word.head.prefix_).isdigit() or not str(word.head.prefix_).isalpha() or str(word.prefix_).isdigit() or not str(word.prefix_).isalpha() or \".\" in word.text or \".\" in str(word.head)):\n",
    "            pass\n",
    "        elif word.head is word and word.dep_ == \"ROOT\" and not (word.is_punct or word.is_digit or word.like_num):\n",
    "            dependency_contexts.append(\n",
    "                {word.lower_: str(word.head) + \" \" + str(word.dep_)})\n",
    "        elif not (word.is_punct or word.is_digit or word.like_num):\n",
    "            dependency_contexts.append({word.lower_: str(word.head) + \" \" + str(word.dep_)+\"-INV\"})\n",
    "            dependency_contexts.append({word.head.lower_: str(word.lower_) + \" \" + str(word.dep_)})\n",
    "    return dependency_contexts\n",
    "dependency_contexts = extract_dep_context(doc)\n",
    "dependency_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 austrailian austrailian ADJ JJ _ 2 amod 2:amod _',\n",
       " '2 scientists scientist NOUN NNS _ 3 nsubj 3:nsubj _',\n",
       " '3 discovers discover VERB VBZ _ 0 ROOT 0:ROOT _',\n",
       " '4 star star NOUN NN _ 3 dobj 3:dobj _',\n",
       " '5 with with ADP IN _ 4 prep 4:prep _',\n",
       " '6 telescope telescope NOUN NN _ 5 pobj 5:pobj _']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/explosion/spaCy/issues/533#issuecomment-254774296\n",
    "def extract_conll_format(doc):\n",
    "    result = []\n",
    "    conll = []\n",
    "    for sent in doc.sents:\n",
    "        for i, word in enumerate(sent):\n",
    "            if word.head is word:\n",
    "                head_idx = 0\n",
    "            else:\n",
    "                 head_idx = word.head.i + 1\n",
    "            conll.extend((i+1, word.lower_, word.lemma_, word.pos_, word.tag_, \"_\", head_idx, word.dep_, str(head_idx) + \":\"+ word.dep_, \"_\"))\n",
    "            result.append(\" \".join(str(x) for x in conll))\n",
    "            conll = []\n",
    "    return result\n",
    "\n",
    "conll_test = extract_conll_format(doc)\n",
    "conll_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dependency Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"hatespeech_dep_conll\", \"w+\") as conll_file:\n",
    "    for index, row in df.iterrows():\n",
    "        for entry in row[\"conllFormat\"]:\n",
    "            conll_file.write(entry+\"\\n\")\n",
    "        conll_file.write(\"\\n\")\n",
    "conll_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
