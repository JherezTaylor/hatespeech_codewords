{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Retrieve parsed collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from modules.db import mongo_base\n",
    "\n",
    "# connection_params = [\"twitter\", \"crowdflower_features\"]\n",
    "# client = mongo_base.connect()\n",
    "# db_name = connection_params[0]\n",
    "# connection_params.insert(0, client)\n",
    "\n",
    "# query = {}\n",
    "# query[\"filter\"] = {}\n",
    "# query[\"projection\"] = {}\n",
    "# query[\"limit\"] = 0\n",
    "# query[\"skip\"] = 0\n",
    "# query[\"no_cursor_timeout\"] = True\n",
    "# cursor = mongo_base.finder(connection_params, query, False)\n",
    "# df = pd.DataFrame(list(cursor))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "crowdflower_persistence = 'data/persistence/crowdflower_features.pkl.compressed'\n",
    "# joblib.dump(df, crowdflower_persistence, compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>annotation_label</th>\n",
       "      <th>avg_token_length</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>brown_cluster_ids</th>\n",
       "      <th>char_pentagrams</th>\n",
       "      <th>char_quadgrams</th>\n",
       "      <th>char_trigrams</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>dep_bigrams</th>\n",
       "      <th>...</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>hs_keyword_count</th>\n",
       "      <th>hs_keyword_matches</th>\n",
       "      <th>noun_chunks</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>unknown_words</th>\n",
       "      <th>unknown_words_count</th>\n",
       "      <th>uppercase_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58c659be6541913eb7f119dd</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[Warning penny, penny boards, boards will, wil...</td>\n",
       "      <td>[966, 228, 989, 333, 442, 4618, 602, 19]</td>\n",
       "      <td>[penny, warni, arnin, aggot, board, rning, fag...</td>\n",
       "      <td>[aggo, warn, arni, penn, oard, ards, enny, nin...</td>\n",
       "      <td>[pen, war, nin, mak, enn, you, agg, ggo, fag, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>[warning ROOT NN warning | boards compound NN ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[faggot]</td>\n",
       "      <td>[{'text': 'warning', 'root': 'warning'}, {'tex...</td>\n",
       "      <td>Warning : penny boards will make you a faggot</td>\n",
       "      <td>[penny, faggot, warning, boards]</td>\n",
       "      <td>[Warning penny boards, penny boards will, boar...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58c659be6541913eb7f119de</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[Fuck dykes]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[dykes]</td>\n",
       "      <td>[ykes, dyke, fuck]</td>\n",
       "      <td>[fuc, kes, yke, dyk, uck]</td>\n",
       "      <td>2</td>\n",
       "      <td>[dykes nsubj NNP fuck | dykes ROOT VBZ dykes]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'text': 'fuck', 'root': 'dykes'}]</td>\n",
       "      <td>Fuck dykes</td>\n",
       "      <td>[fuck, dykes]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58c659be6541913eb7f119df</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[user_mention user_mention, user_mention user_...</td>\n",
       "      <td>[124, 3690, 966, 2442, 1684, 86]</td>\n",
       "      <td>[efree, aggot, least, jefre, starr, faggo]</td>\n",
       "      <td>[star, east, tarr, aggo, like, dont, jefr, lea...</td>\n",
       "      <td>[ont, ike, ast, lik, ook, agg, ggo, arr, fag, ...</td>\n",
       "      <td>14</td>\n",
       "      <td>[user_mention compound NN user_mention | user_...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[faggot]</td>\n",
       "      <td>[{'text': 'i', 'root': 'look'}, {'text': 'jefr...</td>\n",
       "      <td>user_mention user_mention user_mention user_me...</td>\n",
       "      <td>[faggot, user_mention, jefree, like, starr, do...</td>\n",
       "      <td>[user_mention user_mention user_mention, user_...</td>\n",
       "      <td>[user_mention, user_mention, user_mention, use...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58c659be6541913eb7f119e0</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[user_mention user_mention, user_mention user_...</td>\n",
       "      <td>[228, 228, 1214, 19, 28650, 1014, 981]</td>\n",
       "      <td>[jacki, eeeee, ealou, jealo, ackie, alous, neeee]</td>\n",
       "      <td>[alou, neee, acki, jack, lous, eeee, ealo, cki...</td>\n",
       "      <td>[ack, kie, ous, cki, jea, nee, alo, lou, eal, ...</td>\n",
       "      <td>15</td>\n",
       "      <td>[neeeee nsubj NN user_mention | is dep NN user...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[fag]</td>\n",
       "      <td>[{'text': 'user_mention', 'root': 'neeeee'}, {...</td>\n",
       "      <td>\" user_mention : \" user_mention : user_mention...</td>\n",
       "      <td>[jealous, neeeee, user_mention, jackie, fag]</td>\n",
       "      <td>[user_mention user_mention user_mention, user_...</td>\n",
       "      <td>[user_mention, user_mention, user_mention, nee...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58c659be6541913eb7f119e1</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[user_mention You, You heard, heard me, me bit...</td>\n",
       "      <td>[858, 26282, 1898, 2485, 148, 12266, 1349, 753...</td>\n",
       "      <td>[talki, texas, nigga, heard, alkin, lking, bit...</td>\n",
       "      <td>[exas, talk, alki, nigg, back, abou, hear, ear...</td>\n",
       "      <td>[itc, alk, lki, nig, kin, tex, hea, you, but, ...</td>\n",
       "      <td>20</td>\n",
       "      <td>[user_mention ROOT NN user_mention | heard nsu...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[bitch]</td>\n",
       "      <td>[{'text': 'user_mention', 'root': 'user_mentio...</td>\n",
       "      <td>user_mention You heard me bitch but any way I'...</td>\n",
       "      <td>[i'm, user_mention, ass, way, th, bitch, wtf, ...</td>\n",
       "      <td>[user_mention You heard, You heard me, heard m...</td>\n",
       "      <td>[user_mention]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id annotation_label  avg_token_length  \\\n",
       "0  58c659be6541913eb7f119dd    not_offensive               4.0   \n",
       "1  58c659be6541913eb7f119de       hatespeech               4.0   \n",
       "2  58c659be6541913eb7f119df       hatespeech               7.0   \n",
       "3  58c659be6541913eb7f119e0       hatespeech               4.0   \n",
       "4  58c659be6541913eb7f119e1    not_offensive               4.0   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [Warning penny, penny boards, boards will, wil...   \n",
       "1                                       [Fuck dykes]   \n",
       "2  [user_mention user_mention, user_mention user_...   \n",
       "3  [user_mention user_mention, user_mention user_...   \n",
       "4  [user_mention You, You heard, heard me, me bit...   \n",
       "\n",
       "                                   brown_cluster_ids  \\\n",
       "0           [966, 228, 989, 333, 442, 4618, 602, 19]   \n",
       "1                                                 []   \n",
       "2                   [124, 3690, 966, 2442, 1684, 86]   \n",
       "3             [228, 228, 1214, 19, 28650, 1014, 981]   \n",
       "4  [858, 26282, 1898, 2485, 148, 12266, 1349, 753...   \n",
       "\n",
       "                                     char_pentagrams  \\\n",
       "0  [penny, warni, arnin, aggot, board, rning, fag...   \n",
       "1                                            [dykes]   \n",
       "2         [efree, aggot, least, jefre, starr, faggo]   \n",
       "3  [jacki, eeeee, ealou, jealo, ackie, alous, neeee]   \n",
       "4  [talki, texas, nigga, heard, alkin, lking, bit...   \n",
       "\n",
       "                                      char_quadgrams  \\\n",
       "0  [aggo, warn, arni, penn, oard, ards, enny, nin...   \n",
       "1                                 [ykes, dyke, fuck]   \n",
       "2  [star, east, tarr, aggo, like, dont, jefr, lea...   \n",
       "3  [alou, neee, acki, jack, lous, eeee, ealo, cki...   \n",
       "4  [exas, talk, alki, nigg, back, abou, hear, ear...   \n",
       "\n",
       "                                       char_trigrams  comment_length  \\\n",
       "0  [pen, war, nin, mak, enn, you, agg, ggo, fag, ...               9   \n",
       "1                          [fuc, kes, yke, dyk, uck]               2   \n",
       "2  [ont, ike, ast, lik, ook, agg, ggo, arr, fag, ...              14   \n",
       "3  [ack, kie, ous, cki, jea, nee, alo, lou, eal, ...              15   \n",
       "4  [itc, alk, lki, nig, kin, tex, hea, you, but, ...              20   \n",
       "\n",
       "                                         dep_bigrams          ...           \\\n",
       "0  [warning ROOT NN warning | boards compound NN ...          ...            \n",
       "1      [dykes nsubj NNP fuck | dykes ROOT VBZ dykes]          ...            \n",
       "2  [user_mention compound NN user_mention | user_...          ...            \n",
       "3  [neeeee nsubj NN user_mention | is dep NN user...          ...            \n",
       "4  [user_mention ROOT NN user_mention | heard nsu...          ...            \n",
       "\n",
       "  hashtags hs_keyword_count hs_keyword_matches  \\\n",
       "0       []                1           [faggot]   \n",
       "1       []                0                 []   \n",
       "2       []                1           [faggot]   \n",
       "3       []                1              [fag]   \n",
       "4       []                1            [bitch]   \n",
       "\n",
       "                                         noun_chunks  \\\n",
       "0  [{'text': 'warning', 'root': 'warning'}, {'tex...   \n",
       "1                [{'text': 'fuck', 'root': 'dykes'}]   \n",
       "2  [{'text': 'i', 'root': 'look'}, {'text': 'jefr...   \n",
       "3  [{'text': 'user_mention', 'root': 'neeeee'}, {...   \n",
       "4  [{'text': 'user_mention', 'root': 'user_mentio...   \n",
       "\n",
       "                                                text  \\\n",
       "0     Warning : penny boards will make you a faggot    \n",
       "1                                        Fuck dykes    \n",
       "2  user_mention user_mention user_mention user_me...   \n",
       "3  \" user_mention : \" user_mention : user_mention...   \n",
       "4  user_mention You heard me bitch but any way I'...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                   [penny, faggot, warning, boards]   \n",
       "1                                      [fuck, dykes]   \n",
       "2  [faggot, user_mention, jefree, like, starr, do...   \n",
       "3       [jealous, neeeee, user_mention, jackie, fag]   \n",
       "4  [i'm, user_mention, ass, way, th, bitch, wtf, ...   \n",
       "\n",
       "                                            trigrams  \\\n",
       "0  [Warning penny boards, penny boards will, boar...   \n",
       "1                                                 []   \n",
       "2  [user_mention user_mention user_mention, user_...   \n",
       "3  [user_mention user_mention user_mention, user_...   \n",
       "4  [user_mention You heard, You heard me, heard m...   \n",
       "\n",
       "                                       unknown_words unknown_words_count  \\\n",
       "0                                                 []                   0   \n",
       "1                                                 []                   0   \n",
       "2  [user_mention, user_mention, user_mention, use...                   6   \n",
       "3  [user_mention, user_mention, user_mention, nee...                   4   \n",
       "4                                     [user_mention]                   1   \n",
       "\n",
       "  uppercase_token_count  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = joblib.load(crowdflower_persistence)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample = df[0:5]\n",
    "trigrams = sample['char_trigrams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15, 18, 12, 11,  5, 20,  0,  7,  6,  9, 10,  4, 14,  2,  3,  1, 16,\n",
       "       19, 13,  8, 17])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(sample['char_trigrams'][0])\n",
    "sample_cat = le.transform(sample['char_trigrams'][0])\n",
    "sample_cat\n",
    "# list(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jherez/Dev/thesis-preprocessing/venv/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/home/jherez/Dev/thesis-preprocessing/venv/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x21 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 21 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = OneHotEncoder()\n",
    "ohe.fit(sample_cat)\n",
    "sample_ohe = ohe.transform(sample_cat)\n",
    "sample_ohe\n",
    "# ohe.feature_indices_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/home/jherez/Dev/thesis-preprocessing/venv/lib/python3.5/site-packages/pandas/core/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, categories, ordered, name, fastpath)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jherez/Dev/thesis-preprocessing/venv/lib/python3.5/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mfactorize\u001b[0;34m(values, sort, order, na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec_klass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/src/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_labels (pandas/hashtable.c:15447)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-d72a75c6d19a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_dummies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jherez/Dev/thesis-preprocessing/venv/lib/python3.5/site-packages/pandas/core/reshape.py\u001b[0m in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first)\u001b[0m\n\u001b[1;32m   1097\u001b[0m             dummy = _get_dummies_1d(data[col], prefix=pre, prefix_sep=sep,\n\u001b[1;32m   1098\u001b[0m                                     \u001b[0mdummy_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy_na\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                                     drop_first=drop_first)\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0mwith_dummies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_dummies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jherez/Dev/thesis-preprocessing/venv/lib/python3.5/site-packages/pandas/core/reshape.py\u001b[0m in \u001b[0;36m_get_dummies_1d\u001b[0;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first)\u001b[0m\n\u001b[1;32m   1109\u001b[0m                     sparse=False, drop_first=False):\n\u001b[1;32m   1110\u001b[0m     \u001b[0;31m# Series avoids inconsistent NaN handling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m     \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_factorize_from_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_empty_Frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jherez/Dev/thesis-preprocessing/venv/lib/python3.5/site-packages/pandas/core/categorical.py\u001b[0m in \u001b[0;36m_factorize_from_iterable\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   2038\u001b[0m         \u001b[0mcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2040\u001b[0;31m         \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mordered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2041\u001b[0m         \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0mcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jherez/Dev/thesis-preprocessing/venv/lib/python3.5/site-packages/pandas/core/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, categories, ordered, name, fastpath)\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mordered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                     \u001b[0;31m# raise, as we don't have a sortable data structure and so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jherez/Dev/thesis-preprocessing/venv/lib/python3.5/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mfactorize\u001b[0;34m(values, sort, order, na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhash_klass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_hint\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec_klass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_platform_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/src/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_labels (pandas/hashtable.c:15447)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "sample_dummies = pd.get_dummies(df, columns = ['char_trigrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
