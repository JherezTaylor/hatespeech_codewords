{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve parsed collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import models, similarities\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import joblib\n",
    "from modules.db import mongo_base\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import FeatureHasher, DictVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from modules.utils.CustomTwokenizer import CustomTwokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store feature collection as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection_params = [\"twitter\", \"crowdflower_features\"]\n",
    "# client = mongo_base.connect()\n",
    "# db_name = connection_params[0]\n",
    "# connection_params.insert(0, client)\n",
    "\n",
    "# query = {}\n",
    "# query[\"filter\"] = {}\n",
    "# query[\"projection\"] = {}\n",
    "# query[\"limit\"] = 0\n",
    "# query[\"skip\"] = 0\n",
    "# query[\"no_cursor_timeout\"] = True\n",
    "# cursor = mongo_base.finder(connection_params, query, False)\n",
    "# df = pd.DataFrame(list(cursor))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection_params = [\"twitter\", \"crowdflower_features_emo\"]\n",
    "# client = mongo_base.connect()\n",
    "# db_name = connection_params[0]\n",
    "# connection_params.insert(0, client)\n",
    "\n",
    "# query = {}\n",
    "# query[\"filter\"] = {}\n",
    "# query[\"projection\"] = {\"emotions\":1}\n",
    "# query[\"limit\"] = 0\n",
    "# query[\"skip\"] = 0\n",
    "# query[\"no_cursor_timeout\"] = True\n",
    "# cursor = mongo_base.finder(connection_params, query, False)\n",
    "# df_emo = pd.DataFrame(list(cursor))\n",
    "# df = pd.DataFrame.merge(df, df_emo, on=\"_id\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle the raw feature collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/persistence/df/crowdflower_features_raw.pkl.compressed']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_en_model = \"en_core_web_md\"\n",
    "spacy_glove_model = \"en_vectors_glove_md\"\n",
    "crowdflower_persistence_raw = 'data/persistence/df/crowdflower_features_raw.pkl.compressed'\n",
    "crowdflower_persistence = 'data/persistence/df/crowdflower_features.pkl.compressed'\n",
    "nlp = spacy.load(spacy_en_model, create_make_doc=CustomTwokenizer)\n",
    "# joblib.dump(df, crowdflower_persistence_raw, compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = joblib.load(crowdflower_persistence_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe with classifier features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>text</th>\n",
       "      <th>annotation_label</th>\n",
       "      <th>hs_keyword_matches</th>\n",
       "      <th>hs_keyword_count</th>\n",
       "      <th>unknown_words</th>\n",
       "      <th>unknown_words_count</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>brown_cluster_ids</th>\n",
       "      <th>feat_dependency_contexts</th>\n",
       "      <th>feat_word_dep_root</th>\n",
       "      <th>feat_pos_dep_rootPos</th>\n",
       "      <th>feat_word_root_rootparent</th>\n",
       "      <th>feat_dep_unigrams</th>\n",
       "      <th>feat_dep_bigrams</th>\n",
       "      <th>feat_dep_trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58c659be6541913eb7f119dd</td>\n",
       "      <td>Warning : penny boards will make you a faggot</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>[faggot]</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>[966, 228, 989, 333, 442, 4618, 602, 19]</td>\n",
       "      <td>[warning_:_punct, warning_make_acl, penny_boar...</td>\n",
       "      <td>[warning_ROOT_warning, penny_compound_boards, ...</td>\n",
       "      <td>[NN_ROOT_NN, NN_compound_NNS, NNS_nsubj_VB, MD...</td>\n",
       "      <td>[warning_warning_warning, penny_boards_make, b...</td>\n",
       "      <td>[warning_warning_ROOT_NN, penny_boards_compoun...</td>\n",
       "      <td>[warning_warning_ROOT_NN | penny_boards_compou...</td>\n",
       "      <td>[warning_warning_ROOT_NN | penny_boards_compou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58c659be6541913eb7f119de</td>\n",
       "      <td>Fuck dykes</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>[fuck_dykes_compoundINV, dykes_fuck_compound]</td>\n",
       "      <td>[fuck_compound_dykes, dykes_ROOT_dykes]</td>\n",
       "      <td>[NNP_compound_VBZ, VBZ_ROOT_VBZ]</td>\n",
       "      <td>[fuck_dykes_dykes, dykes_dykes_dykes]</td>\n",
       "      <td>[fuck_dykes_compound_NNP, dykes_dykes_ROOT_VBZ]</td>\n",
       "      <td>[fuck_dykes_compound_NNP | dykes_dykes_ROOT_VBZ]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58c659be6541913eb7f119df</td>\n",
       "      <td>user_mention user_mention user_mention user_me...</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[faggot]</td>\n",
       "      <td>1</td>\n",
       "      <td>[jefree]</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>[124, 3690, 966, 2442, 1684, 86]</td>\n",
       "      <td>[user_mention_user_mention_compoundINV, user_m...</td>\n",
       "      <td>[user_mention_compound_user_mention, user_ment...</td>\n",
       "      <td>[NN_compound_NN, NN_compound_NN, NN_compound_N...</td>\n",
       "      <td>[user_mention_user_mention_user_mention, user_...</td>\n",
       "      <td>[user_mention_user_mention_compound_NN, user_m...</td>\n",
       "      <td>[user_mention_user_mention_compound_NN | user_...</td>\n",
       "      <td>[user_mention_user_mention_compound_NN | user_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58c659be6541913eb7f119e0</td>\n",
       "      <td>\" user_mention : \" user_mention : user_mention...</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[fag]</td>\n",
       "      <td>1</td>\n",
       "      <td>[neeeee]</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>[228, 228, 1214, 19, 28650, 1014, 981]</td>\n",
       "      <td>[user_mention_\"_punct, user_mention_:_punct, u...</td>\n",
       "      <td>[user_mention_ROOT_user_mention, user_mention_...</td>\n",
       "      <td>[NN_ROOT_NN, NN_appos_NN, NN_nsubj_VBZ, VBZ_ac...</td>\n",
       "      <td>[user_mention_user_mention_user_mention, user_...</td>\n",
       "      <td>[user_mention_user_mention_ROOT_NN, user_menti...</td>\n",
       "      <td>[user_mention_user_mention_ROOT_NN | user_ment...</td>\n",
       "      <td>[user_mention_user_mention_ROOT_NN | user_ment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58c659be6541913eb7f119e1</td>\n",
       "      <td>user_mention You heard me bitch but any way I'...</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>[bitch]</td>\n",
       "      <td>1</td>\n",
       "      <td>[nigga]</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>[858, 26282, 1898, 2485, 148, 12266, 1349, 753...</td>\n",
       "      <td>[user_mention_heard_relcl, user_mention_bitch_...</td>\n",
       "      <td>[user_mention_ROOT_user_mention, you_nsubj_hea...</td>\n",
       "      <td>[NN_ROOT_NN, PRP_nsubj_VBD, VBD_relcl_NN, PRP_...</td>\n",
       "      <td>[user_mention_user_mention_user_mention, you_h...</td>\n",
       "      <td>[user_mention_user_mention_ROOT_NN, you_heard_...</td>\n",
       "      <td>[user_mention_user_mention_ROOT_NN | you_heard...</td>\n",
       "      <td>[user_mention_user_mention_ROOT_NN | you_heard...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  58c659be6541913eb7f119dd   \n",
       "1  58c659be6541913eb7f119de   \n",
       "2  58c659be6541913eb7f119df   \n",
       "3  58c659be6541913eb7f119e0   \n",
       "4  58c659be6541913eb7f119e1   \n",
       "\n",
       "                                                text annotation_label  \\\n",
       "0     Warning : penny boards will make you a faggot     not_offensive   \n",
       "1                                        Fuck dykes        hatespeech   \n",
       "2  user_mention user_mention user_mention user_me...       hatespeech   \n",
       "3  \" user_mention : \" user_mention : user_mention...       hatespeech   \n",
       "4  user_mention You heard me bitch but any way I'...    not_offensive   \n",
       "\n",
       "  hs_keyword_matches  hs_keyword_count unknown_words  unknown_words_count  \\\n",
       "0           [faggot]                 1            []                    0   \n",
       "1                 []                 0            []                    0   \n",
       "2           [faggot]                 1      [jefree]                    1   \n",
       "3              [fag]                 1      [neeeee]                    1   \n",
       "4            [bitch]                 1       [nigga]                    1   \n",
       "\n",
       "   comment_length                                  brown_cluster_ids  \\\n",
       "0               9           [966, 228, 989, 333, 442, 4618, 602, 19]   \n",
       "1               2                                                 []   \n",
       "2              14                   [124, 3690, 966, 2442, 1684, 86]   \n",
       "3              15             [228, 228, 1214, 19, 28650, 1014, 981]   \n",
       "4              20  [858, 26282, 1898, 2485, 148, 12266, 1349, 753...   \n",
       "\n",
       "                            feat_dependency_contexts  \\\n",
       "0  [warning_:_punct, warning_make_acl, penny_boar...   \n",
       "1      [fuck_dykes_compoundINV, dykes_fuck_compound]   \n",
       "2  [user_mention_user_mention_compoundINV, user_m...   \n",
       "3  [user_mention_\"_punct, user_mention_:_punct, u...   \n",
       "4  [user_mention_heard_relcl, user_mention_bitch_...   \n",
       "\n",
       "                                  feat_word_dep_root  \\\n",
       "0  [warning_ROOT_warning, penny_compound_boards, ...   \n",
       "1            [fuck_compound_dykes, dykes_ROOT_dykes]   \n",
       "2  [user_mention_compound_user_mention, user_ment...   \n",
       "3  [user_mention_ROOT_user_mention, user_mention_...   \n",
       "4  [user_mention_ROOT_user_mention, you_nsubj_hea...   \n",
       "\n",
       "                                feat_pos_dep_rootPos  \\\n",
       "0  [NN_ROOT_NN, NN_compound_NNS, NNS_nsubj_VB, MD...   \n",
       "1                   [NNP_compound_VBZ, VBZ_ROOT_VBZ]   \n",
       "2  [NN_compound_NN, NN_compound_NN, NN_compound_N...   \n",
       "3  [NN_ROOT_NN, NN_appos_NN, NN_nsubj_VBZ, VBZ_ac...   \n",
       "4  [NN_ROOT_NN, PRP_nsubj_VBD, VBD_relcl_NN, PRP_...   \n",
       "\n",
       "                           feat_word_root_rootparent  \\\n",
       "0  [warning_warning_warning, penny_boards_make, b...   \n",
       "1              [fuck_dykes_dykes, dykes_dykes_dykes]   \n",
       "2  [user_mention_user_mention_user_mention, user_...   \n",
       "3  [user_mention_user_mention_user_mention, user_...   \n",
       "4  [user_mention_user_mention_user_mention, you_h...   \n",
       "\n",
       "                                   feat_dep_unigrams  \\\n",
       "0  [warning_warning_ROOT_NN, penny_boards_compoun...   \n",
       "1    [fuck_dykes_compound_NNP, dykes_dykes_ROOT_VBZ]   \n",
       "2  [user_mention_user_mention_compound_NN, user_m...   \n",
       "3  [user_mention_user_mention_ROOT_NN, user_menti...   \n",
       "4  [user_mention_user_mention_ROOT_NN, you_heard_...   \n",
       "\n",
       "                                    feat_dep_bigrams  \\\n",
       "0  [warning_warning_ROOT_NN | penny_boards_compou...   \n",
       "1   [fuck_dykes_compound_NNP | dykes_dykes_ROOT_VBZ]   \n",
       "2  [user_mention_user_mention_compound_NN | user_...   \n",
       "3  [user_mention_user_mention_ROOT_NN | user_ment...   \n",
       "4  [user_mention_user_mention_ROOT_NN | you_heard...   \n",
       "\n",
       "                                   feat_dep_trigrams  \n",
       "0  [warning_warning_ROOT_NN | penny_boards_compou...  \n",
       "1                                                 []  \n",
       "2  [user_mention_user_mention_compound_NN | user_...  \n",
       "3  [user_mention_user_mention_ROOT_NN | user_ment...  \n",
       "4  [user_mention_user_mention_ROOT_NN | you_heard...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_df = df[['_id', 'text', 'annotation_label', 'hs_keyword_matches', 'hs_keyword_count', 'unknown_words', 'unknown_words_count', 'comment_length', 'brown_cluster_ids', 'feat_dependency_contexts', 'feat_word_dep_root', 'feat_pos_dep_rootPos', 'feat_word_root_rootparent', 'feat_dep_unigrams', 'feat_dep_bigrams', 'feat_dep_trigrams']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/persistence/df/crowdflower_features.pkl.compressed']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joblib.dump(feat_df, crowdflower_persistence, compress=True)\n",
    "feat_df = joblib.load(crowdflower_persistence)\n",
    "feat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>annotation_label</th>\n",
       "      <th>avg_token_length</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>brown_cluster_ids</th>\n",
       "      <th>char_pentagrams</th>\n",
       "      <th>char_quadgrams</th>\n",
       "      <th>char_trigrams</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>conllFormat</th>\n",
       "      <th>...</th>\n",
       "      <th>noun_chunks</th>\n",
       "      <th>pos_dep_rootPos</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>unknown_words</th>\n",
       "      <th>unknown_words_count</th>\n",
       "      <th>uppercase_token_count</th>\n",
       "      <th>word_dep_root</th>\n",
       "      <th>word_root_preRoot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58c659be6541913eb7f119dd</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[Warning penny, penny boards, boards will, wil...</td>\n",
       "      <td>[966, 228, 989, 333, 442, 4618, 602, 19]</td>\n",
       "      <td>[penny, warni, arnin, aggot, board, rning, fag...</td>\n",
       "      <td>[aggo, warn, arni, penn, oard, ards, enny, nin...</td>\n",
       "      <td>[pen, war, nin, mak, enn, you, agg, ggo, fag, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>[1 warning warning NOUN NN _ 0 ROOT 0:ROOT _, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'text': 'warning', 'root': 'warning'}, {'tex...</td>\n",
       "      <td>[{'pos': 'NN', 'dep': 'ROOT', 'rootPos': 'NN'}...</td>\n",
       "      <td>Warning : penny boards will make you a faggot</td>\n",
       "      <td>[warning, penny, boards, faggot]</td>\n",
       "      <td>[Warning penny boards, penny boards will, boar...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'word': 'warning', 'dep': 'ROOT', 'root': 'w...</td>\n",
       "      <td>[{'word': 'warning', 'preRoot': 'warning', 'ro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id annotation_label  avg_token_length  \\\n",
       "0  58c659be6541913eb7f119dd    not_offensive               4.0   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [Warning penny, penny boards, boards will, wil...   \n",
       "\n",
       "                          brown_cluster_ids  \\\n",
       "0  [966, 228, 989, 333, 442, 4618, 602, 19]   \n",
       "\n",
       "                                     char_pentagrams  \\\n",
       "0  [penny, warni, arnin, aggot, board, rning, fag...   \n",
       "\n",
       "                                      char_quadgrams  \\\n",
       "0  [aggo, warn, arni, penn, oard, ards, enny, nin...   \n",
       "\n",
       "                                       char_trigrams  comment_length  \\\n",
       "0  [pen, war, nin, mak, enn, you, agg, ggo, fag, ...               9   \n",
       "\n",
       "                                         conllFormat  \\\n",
       "0  [1 warning warning NOUN NN _ 0 ROOT 0:ROOT _, ...   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "\n",
       "                                         noun_chunks  \\\n",
       "0  [{'text': 'warning', 'root': 'warning'}, {'tex...   \n",
       "\n",
       "                                     pos_dep_rootPos  \\\n",
       "0  [{'pos': 'NN', 'dep': 'ROOT', 'rootPos': 'NN'}...   \n",
       "\n",
       "                                             text  \\\n",
       "0  Warning : penny boards will make you a faggot    \n",
       "\n",
       "                             tokens  \\\n",
       "0  [warning, penny, boards, faggot]   \n",
       "\n",
       "                                            trigrams unknown_words  \\\n",
       "0  [Warning penny boards, penny boards will, boar...            []   \n",
       "\n",
       "  unknown_words_count  uppercase_token_count  \\\n",
       "0                   0                      0   \n",
       "\n",
       "                                       word_dep_root  \\\n",
       "0  [{'word': 'warning', 'dep': 'ROOT', 'root': 'w...   \n",
       "\n",
       "                                   word_root_preRoot  \n",
       "0  [{'word': 'warning', 'preRoot': 'warning', 'ro...  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:1]['dep_bigrams'][0][0]\n",
    "hasher = FeatureHasher(input_type='string', non_negative=True)\n",
    "v = DictVectorizer(sparse=False)\n",
    "# raw_X = hasher.transform(df[0:1]['brown_cluster_ids'])\n",
    "df[0:1]['bigrams'][0][0]\n",
    "D = [{'bigram': 'Warning penny'}, {'bigram': 'penny boards'}, {'bigram': 'boards will'}, {'bigram': 'boards will'}]\n",
    "trans = v.fit_transform(D)\n",
    "v.inverse_transform(trans)\n",
    "v.get_feature_names()\n",
    "# raw_X.toarray()\n",
    "# print(df[0:1]['bigrams'][0])\n",
    "df[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's pick the same random 10% of the data to train with\n",
    "train_test_set = df.sample(n=int(len(df) / 2), random_state=1965)\n",
    "\n",
    "X = train_test_set['text']\n",
    "y = train_test_set['annotation_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the labels of the test set...\n",
      "7254 documents\n",
      "2 categories\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting the labels of the test set...\")\n",
    "print(\"%d documents\" % len(X))\n",
    "print(\"%d categories\" % len(y.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup generic model experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(X, y, pipeline, process_name, num_expts=1):\n",
    "    scores = list()\n",
    "    for i in tqdm(range(num_expts)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "        model = pipeline.fit(X_train, y_train)  # train the classifier\n",
    "        y_prediction = model.predict(X_test)          # apply the model to the test data\n",
    "        report = classification_report(y_test, y_prediction)\n",
    "        score = accuracy_score(y_prediction, y_test)  # compare the results to the gold standard\n",
    "        scores.append(score)\n",
    "        print(\"Classification Report: \" + process_name)\n",
    "        print(report)\n",
    "        cm = confusion_matrix(y_test, y_prediction)\n",
    "        print(\"Confusion matrix:\")\n",
    "        print(cm)\n",
    "    print(sum(scores) / num_expts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup naive baseline classification (countVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: CountVectorizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.43      0.38      0.40       289\n",
      "not_offensive       0.88      0.91      0.90      1525\n",
      "\n",
      "  avg / total       0.81      0.82      0.82      1814\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 109  180]\n",
      " [ 142 1383]]\n",
      "0.822491730981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "# our two ingredients: the ngram counter and the classifier\n",
    "nm = 5000\n",
    "vect = CountVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "ch2 = SelectKBest(chi2, k=nm)\n",
    "\n",
    "# There are just two steps to our process: extracting the ngrams and\n",
    "# putting them through the classifier. So our Pipeline looks like this:\n",
    "\n",
    "count_pipeline = Pipeline([\n",
    "    ('vect', vect),  # extract ngrams from tweet text\n",
    "    ('kBest', ch2),\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "\n",
    "# Run the classifcation\n",
    "run_experiment(X, y, count_pipeline, \"CountVectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup naive baseline classification (hashingVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: HashingVectorizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.58      0.36      0.44       322\n",
      "not_offensive       0.87      0.95      0.91      1492\n",
      "\n",
      "  avg / total       0.82      0.84      0.82      1814\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 115  207]\n",
      " [  82 1410]]\n",
      "0.840683572216\n"
     ]
    }
   ],
   "source": [
    "vect = HashingVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "\n",
    "hashing_pipeline = Pipeline([\n",
    "    ('vect', vect),  # extract ngrams from tweet text\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "run_experiment(X, y, hashing_pipeline, \"HashingVectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup tf-idf baseline classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: TfidfVectorizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.53      0.35      0.42       292\n",
      "not_offensive       0.88      0.94      0.91      1522\n",
      "\n",
      "  avg / total       0.83      0.85      0.83      1814\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 101  191]\n",
      " [  89 1433]]\n",
      "0.845644983462\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "\n",
    "tfidf_pipeline = Pipeline([\n",
    "    ('vect', vect),  # extract ngrams from tweet text\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "run_experiment(X, y, tfidf_pipeline, \"TfidfVectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report : Random Experiment\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.64      0.07      0.13       295\n",
      "not_offensive       0.85      0.99      0.91      1519\n",
      "\n",
      "  avg / total       0.81      0.84      0.79      1814\n",
      "\n",
      "Confusion matrix:\n",
      "[[  21  274]\n",
      " [  12 1507]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>annotation_label</th>\n",
       "      <th>avg_token_length</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>brown_cluster_ids</th>\n",
       "      <th>char_pentagrams</th>\n",
       "      <th>char_quadgrams</th>\n",
       "      <th>char_trigrams</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>conllFormat</th>\n",
       "      <th>...</th>\n",
       "      <th>noun_chunks</th>\n",
       "      <th>pos_dep_rootPos</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>unknown_words</th>\n",
       "      <th>unknown_words_count</th>\n",
       "      <th>uppercase_token_count</th>\n",
       "      <th>word_dep_root</th>\n",
       "      <th>word_root_preRoot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58c659be6541913eb7f119dd</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[Warning penny, penny boards, boards will, wil...</td>\n",
       "      <td>[966, 228, 989, 333, 442, 4618, 602, 19]</td>\n",
       "      <td>[penny, warni, arnin, aggot, board, rning, fag...</td>\n",
       "      <td>[aggo, warn, arni, penn, oard, ards, enny, nin...</td>\n",
       "      <td>[pen, war, nin, mak, enn, you, agg, ggo, fag, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>[1\\twarning\\twarning\\tNOUN\\tNN\\t_\\t0\\tROOT\\t0:...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'root': 'warning', 'text': 'warning'}, {'roo...</td>\n",
       "      <td>[{'dep': 'ROOT', 'rootPos': 'NN', 'pos': 'NN'}...</td>\n",
       "      <td>Warning : penny boards will make you a faggot</td>\n",
       "      <td>[warning, penny, boards, faggot]</td>\n",
       "      <td>[Warning penny boards, penny boards will, boar...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'dep': 'ROOT', 'root': 'warning', 'word': 'w...</td>\n",
       "      <td>[{'root': 'warning', 'preRoot': 'warning', 'wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id annotation_label  avg_token_length  \\\n",
       "0  58c659be6541913eb7f119dd    not_offensive               4.0   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [Warning penny, penny boards, boards will, wil...   \n",
       "\n",
       "                          brown_cluster_ids  \\\n",
       "0  [966, 228, 989, 333, 442, 4618, 602, 19]   \n",
       "\n",
       "                                     char_pentagrams  \\\n",
       "0  [penny, warni, arnin, aggot, board, rning, fag...   \n",
       "\n",
       "                                      char_quadgrams  \\\n",
       "0  [aggo, warn, arni, penn, oard, ards, enny, nin...   \n",
       "\n",
       "                                       char_trigrams  comment_length  \\\n",
       "0  [pen, war, nin, mak, enn, you, agg, ggo, fag, ...               9   \n",
       "\n",
       "                                         conllFormat  \\\n",
       "0  [1\\twarning\\twarning\\tNOUN\\tNN\\t_\\t0\\tROOT\\t0:...   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "\n",
       "                                         noun_chunks  \\\n",
       "0  [{'root': 'warning', 'text': 'warning'}, {'roo...   \n",
       "\n",
       "                                     pos_dep_rootPos  \\\n",
       "0  [{'dep': 'ROOT', 'rootPos': 'NN', 'pos': 'NN'}...   \n",
       "\n",
       "                                             text  \\\n",
       "0  Warning : penny boards will make you a faggot    \n",
       "\n",
       "                             tokens  \\\n",
       "0  [warning, penny, boards, faggot]   \n",
       "\n",
       "                                            trigrams unknown_words  \\\n",
       "0  [Warning penny boards, penny boards will, boar...            []   \n",
       "\n",
       "  unknown_words_count uppercase_token_count  \\\n",
       "0                   0                     0   \n",
       "\n",
       "                                       word_dep_root  \\\n",
       "0  [{'dep': 'ROOT', 'root': 'warning', 'word': 'w...   \n",
       "\n",
       "                                   word_root_preRoot  \n",
       "0  [{'root': 'warning', 'preRoot': 'warning', 'wo...  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As much as I want to ignore this, I shouldn't. The fact that the precision score is\n",
    "# close to the other experiments is troubling\n",
    "\n",
    "X = train_test_set[['hs_keyword_count', 'comment_length', 'unknown_words_count']]\n",
    "y = train_test_set['annotation_label']\n",
    "clf = LinearSVC()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "model = clf.fit(X_train, y_train)  # train the classifier\n",
    "y_prediction = model.predict(X_test)          # apply the model to the test data\n",
    "report = classification_report(y_test, y_prediction)\n",
    "print(\"Classification Report : Random Experiment\")\n",
    "print(report)\n",
    "cm = confusion_matrix(y_test, y_prediction)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)\n",
    "df[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency tuple experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report : Random Experiment\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.38      0.14      0.20       303\n",
      "not_offensive       0.85      0.95      0.90      1511\n",
      "\n",
      "  avg / total       0.77      0.82      0.78      1814\n",
      "\n",
      "Confusion matrix:\n",
      "[[  41  262]\n",
      " [  68 1443]]\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "X = train_test_set['word_root_preRoot']\n",
    "y = train_test_set['annotation_label']\n",
    "clf = LinearSVC()\n",
    "\n",
    "# http://www.markhneedham.com/blog/2015/03/02/python-scikit-learn-training-a-classifier-with-non-numeric-features/\n",
    "empty_analyzer = lambda x: x\n",
    "vect = DictVectorizer()\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "# vect = TfidfVectorizer(analyzer=empty_analyzer)\n",
    "# hasher = FeatureHasher(input_type='string', non_negative=True)\n",
    "# X = [[str(res) for res in tmp] for tmp in X]# \n",
    "# X = hasher.transform(X)\n",
    "\n",
    "X = X.tolist()\n",
    "X = vect.fit_transform([item[0] for item in X]).toarray()\n",
    "# X = transformer.fit_transform(X)\n",
    "# vect.vocabulary_\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "model = clf.fit(X_train, y_train)  # train the classifier\n",
    "y_prediction = model.predict(X_test)          # apply the model to the test data\n",
    "report = classification_report(y_test, y_prediction)\n",
    "print(\"Classification Report : Random Experiment\")\n",
    "print(report)\n",
    "cm = confusion_matrix(y_test, y_prediction)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract dependency contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austrailian_scientists_amodINV',\n",
       " 'scientists_austrailian_amod',\n",
       " 'scientists_discovers_nsubjINV',\n",
       " 'discovers_scientists_nsubj',\n",
       " 'discovers_star_dobj',\n",
       " 'star_with_prep',\n",
       " 'star_discovers_dobjINV',\n",
       " 'with_telescope_pobj',\n",
       " 'with_star_prepINV',\n",
       " 'telescope_with_pobjINV']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string = \"Austrailian scientists discovers star with telescope\"\n",
    "doc = nlp(test_string)\n",
    "\n",
    "# elements = []\n",
    "# dict1 = {\"austrailian\": \"scientists amod\"}\n",
    "# dict2 = {\"scientists\": \"austrailian amod\"}\n",
    "# dict3 = {\"scientists\": \"discovers nsubj\"}\n",
    "\n",
    "# elements.append(dict1)\n",
    "# elements.append(dict2)\n",
    "# elements.append(dict3)\n",
    "\n",
    "# vect = DictVectorizer()\n",
    "# X = vect.fit_transform(elements)\n",
    "# X\n",
    "# vect.inverse_transform(X)\n",
    "\n",
    "def extract_dep_context(doc):\n",
    "    dependency_contexts = []\n",
    "    dependency_contexts_concat = []\n",
    "    for word in doc:\n",
    "        if (not str(word.head.prefix_).isalpha() or not str(word.prefix_).isalpha() or \".\" in word.text or \".\" in str(word.head)):\n",
    "            pass\n",
    "        elif len(list(word.children)) == 0:\n",
    "            dependency_contexts.append({word.lower_: str(word.head.lower_) + \" \" + str(word.dep_) + \"INV\"})\n",
    "            dependency_contexts_concat.append(word.lower_+ \"_\" + word.head.lower_ + \"_\" + str(word.dep_) + \"INV\")\n",
    "        elif len(list(word.children)) >= 1:\n",
    "            for child in word.children:\n",
    "                if child.lower_ != word.lower_:\n",
    "                    dependency_contexts.append({word.lower_: str(child.lower_) + \" \" + str(child.dep_)})\n",
    "                    dependency_contexts_concat.append(word.lower_ + \"_\" + child.lower_ + \"_\" + str(child.dep_))\n",
    "            if word.dep_ != \"ROOT\":\n",
    "                dependency_contexts.append({word.lower_: word.head.lower_ + \" \" + str(word.dep_) + \"INV\"})\n",
    "                dependency_contexts_concat.append(word.lower_ + \"_\" + word.head.lower_ + \"_\" + str(word.dep_) + \"INV\")\n",
    "    return dependency_contexts, dependency_contexts_concat\n",
    "dependency_contexts = extract_dep_context(doc)[1]\n",
    "dependency_contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract conll format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 austrailian austrailian ADJ JJ _ 2 amod 2:amod _',\n",
       " '2 scientists scientist NOUN NNS _ 0 ROOT 0:ROOT _',\n",
       " '3 .... .... PUNCT . _ 2 punct 2:punct _',\n",
       " '1 discovers discover VERB VBZ _ 0 ROOT 0:ROOT _',\n",
       " '2 ten ten NUM CD _ 6 nummod 6:nummod _',\n",
       " '3 stars star NOUN NNS _ 4 dobj 4:dobj _',\n",
       " '4 with with ADP IN _ 6 prep 6:prep _',\n",
       " '5 telescope telescope NOUN NN _ 7 pobj 7:pobj _']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/explosion/spaCy/issues/533#issuecomment-254774296\n",
    "def extract_conll_format(doc):\n",
    "    result = []\n",
    "    conll = []\n",
    "    for sent in doc.sents:\n",
    "        for i, word in enumerate(sent):\n",
    "            if word.head is word:\n",
    "                head_idx = 0\n",
    "            else:\n",
    "                 head_idx = word.head.i + 1\n",
    "            conll.extend((i+1, word.lower_, word.lemma_, word.pos_, word.tag_, \"_\", head_idx, word.dep_, str(head_idx) + \":\"+ word.dep_, \"_\"))\n",
    "            result.append(\" \".join(str(x) for x in conll))\n",
    "            conll = []\n",
    "    return result\n",
    "\n",
    "conll_test = extract_conll_format(doc)\n",
    "conll_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create conll format file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"hatespeech_dep_conll\", \"w+\") as conll_file:\n",
    "    for index, row in df.iterrows():\n",
    "        for entry in row[\"conllFormat\"]:\n",
    "            conll_file.write(entry+\"\\n\")\n",
    "        conll_file.write(\"\\n\")\n",
    "conll_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dependency embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
