{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "\n",
    "This is a staging notebook for experiments related to the classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import models, similarities\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import joblib\n",
    "from modules.db import mongo_base\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import FeatureHasher, DictVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from modules.utils.CustomTwokenizer import CustomTwokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store collection as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_as_df(connection_params, projection):\n",
    "    client = mongo_base.connect()\n",
    "    db_name = connection_params[0]\n",
    "    connection_params.insert(0, client)\n",
    "    query = {}\n",
    "    query[\"filter\"] = {}\n",
    "    query[\"projection\"] = projection\n",
    "    query[\"limit\"] = 0\n",
    "    query[\"skip\"] = 0\n",
    "    query[\"no_cursor_timeout\"] = True\n",
    "    cursor = mongo_base.finder(connection_params, query, False)\n",
    "    df = pd.DataFrame(list(cursor))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge dataframes [CrowdFlower Dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_params_1 = [\"twitter\", \"crowdflower_features\"]\n",
    "connection_params_2 = [\"twitter\", \"crowdflower_features_emo\"]\n",
    "# df = fetch_as_df(connection_params_1, {})\n",
    "# df_emo = fetch_as_df(connection_params_2, {\"emotions\":1})\n",
    "# df = pd.DataFrame.merge(df, df_emo, on=\"_id\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle the raw feature collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en_model = \"en_core_web_md\"\n",
    "spacy_glove_model = \"en_vectors_glove_md\"\n",
    "crowdflower_persistence_raw = 'data/persistence/df/crowdflower_features_raw.pkl.compressed'\n",
    "crowdflower_persistence = 'data/persistence/df/crowdflower_features.pkl.compressed'\n",
    "naacl_2016_persistence = 'data/persistence/df/naacl_2016.pkl.compressed'\n",
    "nlp_2016_persistence = 'data/persistence/df/nlp_2016.pkl.compressed'\n",
    "nlp = spacy.load(spacy_en_model, create_make_doc=CustomTwokenizer)\n",
    "# joblib.dump(df, crowdflower_persistence_raw, compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = joblib.load(crowdflower_persistence_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe with classifier features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feat_df = df[['_id', 'text', 'annotation_label', 'hs_keyword_matches', 'hs_keyword_count', 'unknown_words', 'unknown_words_count', 'comment_length', 'brown_cluster_ids', 'feat_dependency_contexts', 'feat_word_dep_root', 'feat_pos_dep_rootPos', 'feat_word_root_rootparent', 'feat_dep_unigrams', 'feat_dep_bigrams', 'feat_dep_trigrams']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>text</th>\n",
       "      <th>annotation_label</th>\n",
       "      <th>hs_keyword_matches</th>\n",
       "      <th>hs_keyword_count</th>\n",
       "      <th>unknown_words</th>\n",
       "      <th>unknown_words_count</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>brown_cluster_ids</th>\n",
       "      <th>feat_dependency_contexts</th>\n",
       "      <th>feat_word_dep_root</th>\n",
       "      <th>feat_pos_dep_rootPos</th>\n",
       "      <th>feat_word_root_rootparent</th>\n",
       "      <th>feat_dep_unigrams</th>\n",
       "      <th>feat_dep_bigrams</th>\n",
       "      <th>feat_dep_trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58c659be6541913eb7f119dd</td>\n",
       "      <td>Warning : penny boards will make you a faggot</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>[faggot]</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>[966, 228, 442, 4618, 602, 19]</td>\n",
       "      <td>[warning_:_punct, warning_make_acl, penny boar...</td>\n",
       "      <td>[warning_ROOT_warning, penny boards_nsubj_make...</td>\n",
       "      <td>[NN_ROOT_NN, NNS_nsubj_VB, MD_aux_VB, VB_acl_N...</td>\n",
       "      <td>[warning_warning_warning, penny boards_make_wa...</td>\n",
       "      <td>[warning_warning_ROOT_NN, penny boards_make_ns...</td>\n",
       "      <td>[warning_warning_ROOT_NN|penny boards_make_nsu...</td>\n",
       "      <td>[warning_warning_ROOT_NN|penny boards_make_nsu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58c659be6541913eb7f119de</td>\n",
       "      <td>Fuck dykes</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>[fuck_dykes_compoundINV, dykes_fuck_compound]</td>\n",
       "      <td>[fuck_compound_dykes, dykes_ROOT_dykes]</td>\n",
       "      <td>[NNP_compound_VBZ, VBZ_ROOT_VBZ]</td>\n",
       "      <td>[fuck_dykes_dykes, dykes_dykes_dykes]</td>\n",
       "      <td>[fuck_dykes_compound_NNP, dykes_dykes_ROOT_VBZ]</td>\n",
       "      <td>[fuck_dykes_compound_NNP|dykes_dykes_ROOT_VBZ]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58c659be6541913eb7f119df</td>\n",
       "      <td>user_mention user_mention user_mention user_me...</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[faggot]</td>\n",
       "      <td>1</td>\n",
       "      <td>[jefree]</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>[124, 3690, 966, 2442, 1684]</td>\n",
       "      <td>[user_mention user_mention user_mention user_m...</td>\n",
       "      <td>[user_mention user_mention user_mention user_m...</td>\n",
       "      <td>[NN_ROOT_NN, IN_advmod_JJS, JJS_advmod_VBP, PR...</td>\n",
       "      <td>[user_mention user_mention user_mention user_m...</td>\n",
       "      <td>[user_mention user_mention user_mention user_m...</td>\n",
       "      <td>[user_mention user_mention user_mention user_m...</td>\n",
       "      <td>[user_mention user_mention user_mention user_m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58c659be6541913eb7f119e0</td>\n",
       "      <td>\" user_mention : \" user_mention : user_mention...</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[fag]</td>\n",
       "      <td>1</td>\n",
       "      <td>[neeeee]</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>[228, 228, 1214, 981]</td>\n",
       "      <td>[user_mention_is_nsubjINV, is_user_mention_nsu...</td>\n",
       "      <td>[\" user_mention_ROOT_\" user_mention, user_ment...</td>\n",
       "      <td>[NN_ROOT_NN, NN_appos_NN, NN_nsubj_VBZ, VBZ_ac...</td>\n",
       "      <td>[\" user_mention_\" user_mention_\" user_mention,...</td>\n",
       "      <td>[\" user_mention_\" user_mention_ROOT_NN, user_m...</td>\n",
       "      <td>[\" user_mention_\" user_mention_ROOT_NN|user_me...</td>\n",
       "      <td>[\" user_mention_\" user_mention_ROOT_NN|user_me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58c659be6541913eb7f119e1</td>\n",
       "      <td>user_mention You heard me bitch but any way I'...</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>[bitch]</td>\n",
       "      <td>1</td>\n",
       "      <td>[nigga]</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>[858, 26282, 1898, 2485, 148, 12266, 1349, 753...</td>\n",
       "      <td>[user_mention_heard_relcl, user_mention_bitch_...</td>\n",
       "      <td>[user_mention_ROOT_user_mention, you_nsubj_hea...</td>\n",
       "      <td>[NN_ROOT_NN, PRP_nsubj_VBD, VBD_relcl_NN, PRP_...</td>\n",
       "      <td>[user_mention_user_mention_user_mention, you_h...</td>\n",
       "      <td>[user_mention_user_mention_ROOT_NN, you_heard_...</td>\n",
       "      <td>[user_mention_user_mention_ROOT_NN|you_heard_n...</td>\n",
       "      <td>[user_mention_user_mention_ROOT_NN|you_heard_n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  58c659be6541913eb7f119dd   \n",
       "1  58c659be6541913eb7f119de   \n",
       "2  58c659be6541913eb7f119df   \n",
       "3  58c659be6541913eb7f119e0   \n",
       "4  58c659be6541913eb7f119e1   \n",
       "\n",
       "                                                text annotation_label  \\\n",
       "0     Warning : penny boards will make you a faggot     not_offensive   \n",
       "1                                        Fuck dykes        hatespeech   \n",
       "2  user_mention user_mention user_mention user_me...       hatespeech   \n",
       "3  \" user_mention : \" user_mention : user_mention...       hatespeech   \n",
       "4  user_mention You heard me bitch but any way I'...    not_offensive   \n",
       "\n",
       "  hs_keyword_matches  hs_keyword_count unknown_words  unknown_words_count  \\\n",
       "0           [faggot]                 1            []                    0   \n",
       "1                 []                 0            []                    0   \n",
       "2           [faggot]                 1      [jefree]                    1   \n",
       "3              [fag]                 1      [neeeee]                    1   \n",
       "4            [bitch]                 1       [nigga]                    1   \n",
       "\n",
       "   comment_length                                  brown_cluster_ids  \\\n",
       "0               9                     [966, 228, 442, 4618, 602, 19]   \n",
       "1               2                                                 []   \n",
       "2              14                       [124, 3690, 966, 2442, 1684]   \n",
       "3              15                              [228, 228, 1214, 981]   \n",
       "4              20  [858, 26282, 1898, 2485, 148, 12266, 1349, 753...   \n",
       "\n",
       "                            feat_dependency_contexts  \\\n",
       "0  [warning_:_punct, warning_make_acl, penny boar...   \n",
       "1      [fuck_dykes_compoundINV, dykes_fuck_compound]   \n",
       "2  [user_mention user_mention user_mention user_m...   \n",
       "3  [user_mention_is_nsubjINV, is_user_mention_nsu...   \n",
       "4  [user_mention_heard_relcl, user_mention_bitch_...   \n",
       "\n",
       "                                  feat_word_dep_root  \\\n",
       "0  [warning_ROOT_warning, penny boards_nsubj_make...   \n",
       "1            [fuck_compound_dykes, dykes_ROOT_dykes]   \n",
       "2  [user_mention user_mention user_mention user_m...   \n",
       "3  [\" user_mention_ROOT_\" user_mention, user_ment...   \n",
       "4  [user_mention_ROOT_user_mention, you_nsubj_hea...   \n",
       "\n",
       "                                feat_pos_dep_rootPos  \\\n",
       "0  [NN_ROOT_NN, NNS_nsubj_VB, MD_aux_VB, VB_acl_N...   \n",
       "1                   [NNP_compound_VBZ, VBZ_ROOT_VBZ]   \n",
       "2  [NN_ROOT_NN, IN_advmod_JJS, JJS_advmod_VBP, PR...   \n",
       "3  [NN_ROOT_NN, NN_appos_NN, NN_nsubj_VBZ, VBZ_ac...   \n",
       "4  [NN_ROOT_NN, PRP_nsubj_VBD, VBD_relcl_NN, PRP_...   \n",
       "\n",
       "                           feat_word_root_rootparent  \\\n",
       "0  [warning_warning_warning, penny boards_make_wa...   \n",
       "1              [fuck_dykes_dykes, dykes_dykes_dykes]   \n",
       "2  [user_mention user_mention user_mention user_m...   \n",
       "3  [\" user_mention_\" user_mention_\" user_mention,...   \n",
       "4  [user_mention_user_mention_user_mention, you_h...   \n",
       "\n",
       "                                   feat_dep_unigrams  \\\n",
       "0  [warning_warning_ROOT_NN, penny boards_make_ns...   \n",
       "1    [fuck_dykes_compound_NNP, dykes_dykes_ROOT_VBZ]   \n",
       "2  [user_mention user_mention user_mention user_m...   \n",
       "3  [\" user_mention_\" user_mention_ROOT_NN, user_m...   \n",
       "4  [user_mention_user_mention_ROOT_NN, you_heard_...   \n",
       "\n",
       "                                    feat_dep_bigrams  \\\n",
       "0  [warning_warning_ROOT_NN|penny boards_make_nsu...   \n",
       "1     [fuck_dykes_compound_NNP|dykes_dykes_ROOT_VBZ]   \n",
       "2  [user_mention user_mention user_mention user_m...   \n",
       "3  [\" user_mention_\" user_mention_ROOT_NN|user_me...   \n",
       "4  [user_mention_user_mention_ROOT_NN|you_heard_n...   \n",
       "\n",
       "                                   feat_dep_trigrams  \n",
       "0  [warning_warning_ROOT_NN|penny boards_make_nsu...  \n",
       "1                                                 []  \n",
       "2  [user_mention user_mention user_mention user_m...  \n",
       "3  [\" user_mention_\" user_mention_ROOT_NN|user_me...  \n",
       "4  [user_mention_user_mention_ROOT_NN|you_heard_n...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joblib.dump(feat_df, crowdflower_persistence, compress=True)\n",
    "feat_df = joblib.load(crowdflower_persistence)\n",
    "feat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch NAACL_SRW_2016 and NLP+CSS_2016 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to DB at mongodb://140.114.79.146:27017 successfully\n",
      "Connected to DB at mongodb://140.114.79.146:27017 successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/persistence/df/nlp_2016.pkl.compressed']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connection_params_3 = [\"twitter\", \"NAACL_SRW_2016\"]\n",
    "# connection_params_4 = [\"twitter\", \"NLP_CSS_2016_expert\"]\n",
    "# df_naacl = fetch_as_df(connection_params_3, {})\n",
    "# df_nlp = fetch_as_df(connection_params_4, {})\n",
    "# joblib.dump(df_naacl, naacl_2016_persistence, compress=True)\n",
    "# joblib.dump(df_nlp, nlp_2016_persistence, compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>annotation</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>591c29f465419158a43b735d</td>\n",
       "      <td>neither</td>\n",
       "      <td>597576902212063232</td>\n",
       "      <td>Cisco had to deal with a fat cash payout to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>591c29f465419158a43b735e</td>\n",
       "      <td>neither</td>\n",
       "      <td>565586175864610817</td>\n",
       "      <td>@MadamPlumpette I'm decent at editing, no worr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>591c29f465419158a43b735f</td>\n",
       "      <td>neither</td>\n",
       "      <td>563881580209246209</td>\n",
       "      <td>@girlziplocked will read. gotta go afk for a b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>591c29f465419158a43b7360</td>\n",
       "      <td>neither</td>\n",
       "      <td>595380689534656512</td>\n",
       "      <td>guys. show me the data. show me your github. t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>591c29f465419158a43b7361</td>\n",
       "      <td>neither</td>\n",
       "      <td>563757610327748608</td>\n",
       "      <td>@tpw_rules nothings broken. I was just driving...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id annotation              id_str  \\\n",
       "0  591c29f465419158a43b735d    neither  597576902212063232   \n",
       "1  591c29f465419158a43b735e    neither  565586175864610817   \n",
       "2  591c29f465419158a43b735f    neither  563881580209246209   \n",
       "3  591c29f465419158a43b7360    neither  595380689534656512   \n",
       "4  591c29f465419158a43b7361    neither  563757610327748608   \n",
       "\n",
       "                                                text  \n",
       "0  Cisco had to deal with a fat cash payout to th...  \n",
       "1  @MadamPlumpette I'm decent at editing, no worr...  \n",
       "2  @girlziplocked will read. gotta go afk for a b...  \n",
       "3  guys. show me the data. show me your github. t...  \n",
       "4  @tpw_rules nothings broken. I was just driving...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_naacl = joblib.load(naacl_2016_persistence)\n",
    "df_nlp = joblib.load(nlp_2016_persistence)\n",
    "df_nlp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's pick the same random 10% of the data to train with\n",
    "train_test_set = feat_df.sample(n=int(len(feat_df) / 2), random_state=1965)\n",
    "\n",
    "X = train_test_set['text']\n",
    "y = train_test_set['annotation_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the labels of the test set...\n",
      "14508 documents\n",
      "2 categories\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting the labels of the test set...\")\n",
    "print(\"%d documents\" % len(X))\n",
    "print(\"%d categories\" % len(y.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup generic model experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(X, y, pipeline, process_name, num_expts=1):\n",
    "    scores = list()\n",
    "    for i in tqdm(range(num_expts)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, train_size=0.80)\n",
    "        model = pipeline.fit(X_train, y_train)  # train the classifier\n",
    "        y_prediction = model.predict(X_test)          # apply the model to the test data\n",
    "        report = classification_report(y_test, y_prediction)\n",
    "        score = accuracy_score(y_prediction, y_test)  # compare the results to the gold standard\n",
    "        scores.append(score)\n",
    "        print(\"Classification Report: \" + process_name)\n",
    "        print(report)\n",
    "        cm = confusion_matrix(y_test, y_prediction)\n",
    "        print(\"Confusion matrix:\")\n",
    "        print(cm)\n",
    "    print(sum(scores) / num_expts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup naive baseline classification (countVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: CountVectorizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.45      0.33      0.38       245\n",
      "not_offensive       0.87      0.92      0.89      1206\n",
      "\n",
      "  avg / total       0.80      0.82      0.81      1451\n",
      "\n",
      "Confusion matrix:\n",
      "[[  80  165]\n",
      " [  96 1110]]\n",
      "0.820124052378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# our two ingredients: the ngram counter and the classifier\n",
    "nm = 5000\n",
    "vect = CountVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "ch2 = SelectKBest(chi2, k=nm)\n",
    "\n",
    "# There are just two steps to our process: extracting the ngrams and\n",
    "# putting them through the classifier. So our Pipeline looks like this:\n",
    "\n",
    "count_pipeline = Pipeline([\n",
    "    ('vect', vect),  # extract ngrams from tweet text\n",
    "    ('kBest', ch2),\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "\n",
    "# Run the classifcation\n",
    "run_experiment(X, y, count_pipeline, \"CountVectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare against NAACL 2016 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: CountVectorizer\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       none       0.85      0.89      0.87      1013\n",
      "     racism       0.67      0.60      0.64       149\n",
      "     sexism       0.70      0.63      0.66       289\n",
      "\n",
      "avg / total       0.80      0.81      0.80      1451\n",
      "\n",
      "Confusion matrix:\n",
      "[[897  42  74]\n",
      " [ 55  90   4]\n",
      " [105   2 182]]\n",
      "0.805651274983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_set = df_naacl.sample(n=int(len(feat_df) / 2), random_state=1965)\n",
    "\n",
    "X_naacl = train_test_set['text']\n",
    "y_naacl = train_test_set['annotation']\n",
    "\n",
    "nm = 5000\n",
    "vect = CountVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "ch2 = SelectKBest(chi2, k=nm)\n",
    "\n",
    "# There are just two steps to our process: extracting the ngrams and\n",
    "# putting them through the classifier. So our Pipeline looks like this:\n",
    "\n",
    "count_pipeline = Pipeline([\n",
    "    ('vect', vect),  # extract ngrams from tweet text\n",
    "    ('kBest', ch2),\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "\n",
    "# Run the classifcation\n",
    "run_experiment(X_naacl, y_naacl, count_pipeline, \"CountVectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup naive baseline classification (hashingVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: HashingVectorizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.60      0.39      0.47       522\n",
      "not_offensive       0.87      0.94      0.91      2380\n",
      "\n",
      "  avg / total       0.82      0.84      0.83      2902\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 201  321]\n",
      " [ 136 2244]]\n",
      "0.842522398346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vect = HashingVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "\n",
    "hashing_pipeline = Pipeline([\n",
    "    ('vect', vect),  # extract ngrams from tweet text\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "run_experiment(X, y, hashing_pipeline, \"HashingVectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup tf-idf baseline classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: TfidfVectorizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.57      0.40      0.47       477\n",
      "not_offensive       0.89      0.94      0.91      2425\n",
      "\n",
      "  avg / total       0.84      0.85      0.84      2902\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 190  287]\n",
      " [ 146 2279]]\n",
      "0.850792556857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "\n",
    "tfidf_pipeline = Pipeline([\n",
    "    ('vect', vect),  # extract ngrams from tweet text\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "run_experiment(X, y, tfidf_pipeline, \"TfidfVectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency tuple experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: FeatureHasher\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.42      0.13      0.20       253\n",
      "not_offensive       0.84      0.96      0.90      1198\n",
      "\n",
      "  avg / total       0.77      0.82      0.77      1451\n",
      "\n",
      "Confusion matrix:\n",
      "[[  33  220]\n",
      " [  46 1152]]\n",
      "0.816678152998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X = train_test_set['feat_word_root_rootparent']\n",
    "y = train_test_set['annotation_label']\n",
    "\n",
    "hasher = FeatureHasher(input_type='string', non_negative=True)\n",
    "clf = LinearSVC()\n",
    "\n",
    "featurer_hasher_pipeline = Pipeline([\n",
    "    ('hasher', hasher),  # extract ngrams from tweet text\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "run_experiment(X, y, featurer_hasher_pipeline, \"FeatureHasher\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency context experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "class TextExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Adapted from code by @zacstewart \n",
    "       https://github.com/zacstewart/kaggle_seeclickfix/blob/master/estimator.py\n",
    "       Also see Zac Stewart's excellent blogpost on pipelines:\n",
    "       http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "       \"\"\"\n",
    "    \n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def transform(self, df):\n",
    "        # select the relevant column and return it as a numpy array\n",
    "        # set the array type to be string\n",
    "        return df[self.column_name].tolist()\n",
    "#         return np.asarray(df[self.column_name]).astype(str)\n",
    "        \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "\n",
    "class Apply(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies a function f element-wise to the numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fn):\n",
    "        self.fn = np.vectorize(fn)\n",
    "        \n",
    "    def transform(self, data):\n",
    "        # note: reshaping is necessary because otherwise sklearn\n",
    "        # interprets 1-d array as a single sample\n",
    "        return self.fn(data.reshape(data.size, 1))\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "class BooleanExtractor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def transform(self, df):\n",
    "        # select the relevant column and return it as a numpy array\n",
    "        # set the array type to be string\n",
    "        return np.asarray(df[self.column_name]).astype(np.int)\n",
    "                                                       \n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: TfidfVectorizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.51      0.27      0.35       235\n",
      "not_offensive       0.87      0.95      0.91      1216\n",
      "\n",
      "  avg / total       0.81      0.84      0.82      1451\n",
      "\n",
      "Confusion matrix:\n",
      "[[  64  171]\n",
      " [  62 1154]]\n",
      "0.839421088904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "X = train_test_set[['feat_dependency_contexts', 'hs_keyword_count']]\n",
    "y = train_test_set['annotation_label']\n",
    "clf = LinearSVC()\n",
    "empty_analyzer = lambda x: x\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "vect = TfidfVectorizer(analyzer=empty_analyzer)\n",
    "\n",
    "dependency_context_pipeline = Pipeline([\n",
    "    ('dep_extractor', TextExtractor('feat_dependency_contexts')), # extract names from df\n",
    "    ('vect', vect)\n",
    "])\n",
    "\n",
    "hs_keyword_count_pipeline = Pipeline([\n",
    "    ('count_extractor', BooleanExtractor('hs_keyword_count')),\n",
    "    ('identity', Apply(lambda x: x))\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('all_features', FeatureUnion([\n",
    "        ('dependency_context_pipeline', dependency_context_pipeline), # all text features\n",
    "        ('hs_keyword_count_pipeline', hs_keyword_count_pipeline),\n",
    "    ])),\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "\n",
    "run_experiment(X, y, pipeline, \"TfidfVectorizer\")\n",
    "\n",
    "# https://github.com/michelleful/SingaporeRoadnameOrigins/blob/master/notebooks/04%20Adding%20features%20with%20Pipelines.ipynb\n",
    "# https://github.com/amueller/kaggle_insults/blob/e4abac805be1d1e2b3201a978172bafd36cc01e3/features.py\n",
    "# http://www.markhneedham.com/blog/2015/03/02/python-scikit-learn-training-a-classifier-with-non-numeric-features/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
