{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Retrieve parsed collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import joblib\n",
    "from modules.db import mongo_base\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import FeatureHasher, DictVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from modules.utils.CustomTwokenizer import CustomTwokenizer\n",
    "\n",
    "# connection_params = [\"twitter\", \"crowdflower_features\"]\n",
    "# client = mongo_base.connect()\n",
    "# db_name = connection_params[0]\n",
    "# connection_params.insert(0, client)\n",
    "\n",
    "# query = {}\n",
    "# query[\"filter\"] = {}\n",
    "# query[\"projection\"] = {}\n",
    "# query[\"limit\"] = 0\n",
    "# query[\"skip\"] = 0\n",
    "# query[\"no_cursor_timeout\"] = True\n",
    "# cursor = mongo_base.finder(connection_params, query, False)\n",
    "# df = pd.DataFrame(list(cursor))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "crowdflower_persistence = 'data/persistence/crowdflower_features.pkl.compressed'\n",
    "nlp = spacy.load('en', create_make_doc=CustomTwokenizer)\n",
    "# joblib.dump(df, crowdflower_persistence, compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = joblib.load(crowdflower_persistence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>annotation_label</th>\n",
       "      <th>avg_token_length</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>brown_cluster_ids</th>\n",
       "      <th>char_pentagrams</th>\n",
       "      <th>char_quadgrams</th>\n",
       "      <th>char_trigrams</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>dep_bigrams</th>\n",
       "      <th>...</th>\n",
       "      <th>noun_chunks</th>\n",
       "      <th>pos_dep_rootPos</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>unknown_words</th>\n",
       "      <th>unknown_words_count</th>\n",
       "      <th>uppercase_token_count</th>\n",
       "      <th>word_dep_root</th>\n",
       "      <th>word_root_preRoot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58c659be6541913eb7f119dd</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[Warning penny, penny boards, boards will, wil...</td>\n",
       "      <td>[966, 228, 989, 333, 442, 4618, 602, 19]</td>\n",
       "      <td>[penny, warni, arnin, aggot, board, rning, fag...</td>\n",
       "      <td>[aggo, warn, arni, penn, oard, ards, enny, nin...</td>\n",
       "      <td>[pen, war, nin, mak, enn, you, agg, ggo, fag, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>[warning ROOT NN warning | boards compound NN ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'root': 'warning', 'text': 'warning'}, {'roo...</td>\n",
       "      <td>[{'rootPos': 'NN', 'dep': 'ROOT', 'pos': 'NN'}...</td>\n",
       "      <td>Warning : penny boards will make you a faggot</td>\n",
       "      <td>[penny, faggot, warning, boards]</td>\n",
       "      <td>[Warning penny boards, penny boards will, boar...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'root': 'warning', 'dep': 'ROOT', 'word': 'w...</td>\n",
       "      <td>[{'root': 'warning', 'preRoot': 'warning', 'wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id annotation_label  avg_token_length  \\\n",
       "0  58c659be6541913eb7f119dd    not_offensive               4.0   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [Warning penny, penny boards, boards will, wil...   \n",
       "\n",
       "                          brown_cluster_ids  \\\n",
       "0  [966, 228, 989, 333, 442, 4618, 602, 19]   \n",
       "\n",
       "                                     char_pentagrams  \\\n",
       "0  [penny, warni, arnin, aggot, board, rning, fag...   \n",
       "\n",
       "                                      char_quadgrams  \\\n",
       "0  [aggo, warn, arni, penn, oard, ards, enny, nin...   \n",
       "\n",
       "                                       char_trigrams  comment_length  \\\n",
       "0  [pen, war, nin, mak, enn, you, agg, ggo, fag, ...               9   \n",
       "\n",
       "                                         dep_bigrams  \\\n",
       "0  [warning ROOT NN warning | boards compound NN ...   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "\n",
       "                                         noun_chunks  \\\n",
       "0  [{'root': 'warning', 'text': 'warning'}, {'roo...   \n",
       "\n",
       "                                     pos_dep_rootPos  \\\n",
       "0  [{'rootPos': 'NN', 'dep': 'ROOT', 'pos': 'NN'}...   \n",
       "\n",
       "                                             text  \\\n",
       "0  Warning : penny boards will make you a faggot    \n",
       "\n",
       "                             tokens  \\\n",
       "0  [penny, faggot, warning, boards]   \n",
       "\n",
       "                                            trigrams unknown_words  \\\n",
       "0  [Warning penny boards, penny boards will, boar...            []   \n",
       "\n",
       "   unknown_words_count uppercase_token_count  \\\n",
       "0                    0                     0   \n",
       "\n",
       "                                       word_dep_root  \\\n",
       "0  [{'root': 'warning', 'dep': 'ROOT', 'word': 'w...   \n",
       "\n",
       "                                   word_root_preRoot  \n",
       "0  [{'root': 'warning', 'preRoot': 'warning', 'wo...  \n",
       "\n",
       "[1 rows x 28 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:1]['dep_bigrams'][0][0]\n",
    "hasher = FeatureHasher(input_type='string', non_negative=True)\n",
    "v = DictVectorizer(sparse=False)\n",
    "# raw_X = hasher.transform(df[0:1]['brown_cluster_ids'])\n",
    "df[0:1]['bigrams'][0][0]\n",
    "D = [{'bigram': 'Warning penny'}, {'bigram': 'penny boards'}, {'bigram': 'boards will'}, {'bigram': 'boards will'}]\n",
    "trans = v.fit_transform(D)\n",
    "v.inverse_transform(trans)\n",
    "v.get_feature_names()\n",
    "# raw_X.toarray()\n",
    "# print(df[0:1]['bigrams'][0])\n",
    "df[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Setup data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# let's pick the same random 10% of the data to train with\n",
    "train_test_set = df.sample(n=int(len(df) / 2), random_state=1965)\n",
    "\n",
    "X = train_test_set['text']\n",
    "y = train_test_set['annotation_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the labels of the test set...\n",
      "7254 documents\n",
      "2 categories\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting the labels of the test set...\")\n",
    "print(\"%d documents\" % len(X))\n",
    "print(\"%d categories\" % len(y.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Setup generic model experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(X, y, pipeline, process_name, num_expts=1):\n",
    "    scores = list()\n",
    "    for i in range(num_expts):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "        model = pipeline.fit(X_train, y_train)  # train the classifier\n",
    "        y_prediction = model.predict(X_test)          # apply the model to the test data\n",
    "        report = classification_report(y_test, y_prediction)\n",
    "        score = accuracy_score(y_prediction, y_test)  # compare the results to the gold standard\n",
    "        scores.append(score)\n",
    "        print(\"Classification Report: \" + process_name)\n",
    "        print(report)\n",
    "        cm = confusion_matrix(y_test, y_prediction)\n",
    "        print(\"Confusion matrix:\")\n",
    "        print(cm)\n",
    "    print(sum(scores) / num_expts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Setup naive baseline classification (countVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: CountVectorizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.41      0.37      0.39       285\n",
      "not_offensive       0.89      0.90      0.89      1529\n",
      "\n",
      "  avg / total       0.81      0.82      0.81      1814\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 106  179]\n",
      " [ 150 1379]]\n",
      "0.818632855568\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "# our two ingredients: the ngram counter and the classifier\n",
    "nm = 5000\n",
    "vect = CountVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "ch2 = SelectKBest(chi2, k=nm)\n",
    "\n",
    "# There are just two steps to our process: extracting the ngrams and\n",
    "# putting them through the classifier. So our Pipeline looks like this:\n",
    "\n",
    "count_pipeline = Pipeline([\n",
    "    ('vect', vect),  # extract ngrams from tweet text\n",
    "    ('kBest', ch2),\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "\n",
    "# Run the classifcation\n",
    "run_experiment(X, y, count_pipeline, \"CountVectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Setup naive baseline classification (hashingVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: HashingVectorizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.53      0.36      0.43       302\n",
      "not_offensive       0.88      0.94      0.91      1512\n",
      "\n",
      "  avg / total       0.82      0.84      0.83      1814\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 108  194]\n",
      " [  94 1418]]\n",
      "0.841234840132\n"
     ]
    }
   ],
   "source": [
    "vect = HashingVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "\n",
    "hashing_pipeline = Pipeline([\n",
    "    ('vect', vect),  # extract ngrams from tweet text\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "run_experiment(X, y, hashing_pipeline, \"HashingVectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Setup tf-idf baseline classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: TfidfVectorizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.55      0.28      0.37       311\n",
      "not_offensive       0.86      0.95      0.91      1503\n",
      "\n",
      "  avg / total       0.81      0.84      0.81      1814\n",
      "\n",
      "Confusion matrix:\n",
      "[[  87  224]\n",
      " [  72 1431]]\n",
      "0.836824696803\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "\n",
    "tfidf_pipeline = Pipeline([\n",
    "    ('vect', vect),  # extract ngrams from tweet text\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "run_experiment(X, y, tfidf_pipeline, \"TfidfVectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Investigate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report : Random Experiment\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.62      0.11      0.18       281\n",
      "not_offensive       0.86      0.99      0.92      1533\n",
      "\n",
      "  avg / total       0.82      0.85      0.80      1814\n",
      "\n",
      "Confusion matrix:\n",
      "[[  30  251]\n",
      " [  18 1515]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>annotation_label</th>\n",
       "      <th>avg_token_length</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>brown_cluster_ids</th>\n",
       "      <th>char_pentagrams</th>\n",
       "      <th>char_quadgrams</th>\n",
       "      <th>char_trigrams</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>dep_bigrams</th>\n",
       "      <th>...</th>\n",
       "      <th>noun_chunks</th>\n",
       "      <th>pos_dep_rootPos</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>unknown_words</th>\n",
       "      <th>unknown_words_count</th>\n",
       "      <th>uppercase_token_count</th>\n",
       "      <th>word_dep_root</th>\n",
       "      <th>word_root_preRoot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58c659be6541913eb7f119dd</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[Warning penny, penny boards, boards will, wil...</td>\n",
       "      <td>[966, 228, 989, 333, 442, 4618, 602, 19]</td>\n",
       "      <td>[penny, warni, arnin, aggot, board, rning, fag...</td>\n",
       "      <td>[aggo, warn, arni, penn, oard, ards, enny, nin...</td>\n",
       "      <td>[pen, war, nin, mak, enn, you, agg, ggo, fag, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>[warning ROOT NN warning | boards compound NN ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'root': 'warning', 'text': 'warning'}, {'roo...</td>\n",
       "      <td>[{'rootPos': 'NN', 'dep': 'ROOT', 'pos': 'NN'}...</td>\n",
       "      <td>Warning : penny boards will make you a faggot</td>\n",
       "      <td>[penny, faggot, warning, boards]</td>\n",
       "      <td>[Warning penny boards, penny boards will, boar...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'root': 'warning', 'dep': 'ROOT', 'word': 'w...</td>\n",
       "      <td>[{'root': 'warning', 'preRoot': 'warning', 'wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id annotation_label  avg_token_length  \\\n",
       "0  58c659be6541913eb7f119dd    not_offensive               4.0   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [Warning penny, penny boards, boards will, wil...   \n",
       "\n",
       "                          brown_cluster_ids  \\\n",
       "0  [966, 228, 989, 333, 442, 4618, 602, 19]   \n",
       "\n",
       "                                     char_pentagrams  \\\n",
       "0  [penny, warni, arnin, aggot, board, rning, fag...   \n",
       "\n",
       "                                      char_quadgrams  \\\n",
       "0  [aggo, warn, arni, penn, oard, ards, enny, nin...   \n",
       "\n",
       "                                       char_trigrams  comment_length  \\\n",
       "0  [pen, war, nin, mak, enn, you, agg, ggo, fag, ...               9   \n",
       "\n",
       "                                         dep_bigrams  \\\n",
       "0  [warning ROOT NN warning | boards compound NN ...   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "\n",
       "                                         noun_chunks  \\\n",
       "0  [{'root': 'warning', 'text': 'warning'}, {'roo...   \n",
       "\n",
       "                                     pos_dep_rootPos  \\\n",
       "0  [{'rootPos': 'NN', 'dep': 'ROOT', 'pos': 'NN'}...   \n",
       "\n",
       "                                             text  \\\n",
       "0  Warning : penny boards will make you a faggot    \n",
       "\n",
       "                             tokens  \\\n",
       "0  [penny, faggot, warning, boards]   \n",
       "\n",
       "                                            trigrams unknown_words  \\\n",
       "0  [Warning penny boards, penny boards will, boar...            []   \n",
       "\n",
       "   unknown_words_count uppercase_token_count  \\\n",
       "0                    0                     0   \n",
       "\n",
       "                                       word_dep_root  \\\n",
       "0  [{'root': 'warning', 'dep': 'ROOT', 'word': 'w...   \n",
       "\n",
       "                                   word_root_preRoot  \n",
       "0  [{'root': 'warning', 'preRoot': 'warning', 'wo...  \n",
       "\n",
       "[1 rows x 28 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As much as I want to ignore this, I shouldn't. The fact that the precision score is\n",
    "# close to the other experiments is troubling\n",
    "\n",
    "X = train_test_set[['hs_keyword_count', 'comment_length', 'unknown_words_count']]\n",
    "y = train_test_set['annotation_label']\n",
    "clf = LinearSVC()\n",
    "\n",
    "# X = X.values.reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "model = clf.fit(X_train, y_train)  # train the classifier\n",
    "y_prediction = model.predict(X_test)          # apply the model to the test data\n",
    "report = classification_report(y_test, y_prediction)\n",
    "print(\"Classification Report : Random Experiment\")\n",
    "print(report)\n",
    "cm = confusion_matrix(y_test, y_prediction)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)\n",
    "df[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report : Random Experiment\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.41      0.17      0.24       309\n",
      "not_offensive       0.85      0.95      0.90      1505\n",
      "\n",
      "  avg / total       0.77      0.82      0.78      1814\n",
      "\n",
      "Confusion matrix:\n",
      "[[  51  258]\n",
      " [  73 1432]]\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "X = train_test_set['word_root_preRoot']\n",
    "y = train_test_set['annotation_label']\n",
    "clf = LinearSVC()\n",
    "\n",
    "# http://www.markhneedham.com/blog/2015/03/02/python-scikit-learn-training-a-classifier-with-non-numeric-features/\n",
    "empty_analyzer = lambda x: x\n",
    "vect = DictVectorizer()\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "# vect = TfidfVectorizer(analyzer=empty_analyzer)\n",
    "# hasher = FeatureHasher(input_type='string', non_negative=True)\n",
    "# X = [[str(res) for res in tmp] for tmp in X]# \n",
    "# X = hasher.transform(X)\n",
    "\n",
    "X = X.tolist()\n",
    "X = vect.fit_transform([item[0] for item in X]).toarray()\n",
    "# X = transformer.fit_transform(X)\n",
    "# vect.vocabulary_\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "model = clf.fit(X_train, y_train)  # train the classifier\n",
    "y_prediction = model.predict(X_test)          # apply the model to the test data\n",
    "report = classification_report(y_test, y_prediction)\n",
    "print(\"Classification Report : Random Experiment\")\n",
    "print(report)\n",
    "cm = confusion_matrix(y_test, y_prediction)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user_mention great news for people who want to see women be sick and die Utah be very proud of your Byzantine ideas ']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'dep': 'nummod', 'root': 'news', 'word': 'user_mention'},\n",
       "  {'dep': 'amod', 'root': 'news', 'word': 'great'},\n",
       "  {'dep': 'nsubj', 'root': 'be', 'word': 'news'},\n",
       "  {'dep': 'prep', 'root': 'news', 'word': 'for'},\n",
       "  {'dep': 'pobj', 'root': 'for', 'word': 'people'},\n",
       "  {'dep': 'nsubj', 'root': 'want', 'word': 'who'},\n",
       "  {'dep': 'relcl', 'root': 'people', 'word': 'want'},\n",
       "  {'dep': 'aux', 'root': 'see', 'word': 'to'},\n",
       "  {'dep': 'xcomp', 'root': 'want', 'word': 'see'},\n",
       "  {'dep': 'nsubj', 'root': 'be', 'word': 'women'},\n",
       "  {'dep': 'ccomp', 'root': 'see', 'word': 'be'},\n",
       "  {'dep': 'acomp', 'root': 'be', 'word': 'sick'},\n",
       "  {'dep': 'cc', 'root': 'be', 'word': 'and'},\n",
       "  {'dep': 'conj', 'root': 'be', 'word': 'die'},\n",
       "  {'dep': 'nsubj', 'root': 'be', 'word': 'utah'},\n",
       "  {'dep': 'ROOT', 'root': 'be', 'word': 'be'},\n",
       "  {'dep': 'advmod', 'root': 'proud', 'word': 'very'},\n",
       "  {'dep': 'acomp', 'root': 'be', 'word': 'proud'},\n",
       "  {'dep': 'prep', 'root': 'proud', 'word': 'of'},\n",
       "  {'dep': 'poss', 'root': 'ideas', 'word': 'your'},\n",
       "  {'dep': 'amod', 'root': 'ideas', 'word': 'byzantine'},\n",
       "  {'dep': 'pobj', 'root': 'of', 'word': 'ideas'}]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(list(train_test_set[0:1]['text']))\n",
    "list(train_test_set[0:1]['word_dep_root'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'austrailian': 'amod'},\n",
       " {'scientists': 'nsubj'},\n",
       " {'discovers': 'ROOT'},\n",
       " {'star': 'dobj'},\n",
       " {'with': 'prep'},\n",
       " {'telescope': 'pobj'}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string = \"Austrailian scientists discovers star with telescope\"\n",
    "doc = nlp(test_string)\n",
    "\n",
    "# elements = []\n",
    "# dict1 = {\"austrailian\": \"scientists amod\"}\n",
    "# dict2 = {\"scientists\": \"austrailian amod\"}\n",
    "# dict3 = {\"scientists\": \"discovers nsubj\"}\n",
    "\n",
    "# elements.append(dict1)\n",
    "# elements.append(dict2)\n",
    "# elements.append(dict3)\n",
    "\n",
    "# vect = DictVectorizer()\n",
    "# X = vect.fit_transform(elements)\n",
    "# X\n",
    "# vect.inverse_transform(X)\n",
    "dependency_contexts = []\n",
    "context = {}\n",
    "for word in doc:\n",
    "    dependency_contexts.append({word.lower_: word.dep_})\n",
    "dependency_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 austrailian austrailian ADJ JJ _ 2 amod 2:amod _',\n",
       " '2 scientists scientist NOUN NNS _ 3 nsubj 3:nsubj _',\n",
       " '3 discovers discover VERB VBZ _ 0 ROOT 0:ROOT _',\n",
       " '4 star star NOUN NN _ 3 dobj 3:dobj _',\n",
       " '5 with with ADP IN _ 4 prep 4:prep _',\n",
       " '6 telescope telescope NOUN NN _ 5 pobj 5:pobj _']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/explosion/spaCy/issues/533#issuecomment-254774296\n",
    "def extract_conll_format(doc):\n",
    "    result = []\n",
    "    conll = []\n",
    "    for sent in doc.sents:\n",
    "        for i, word in enumerate(sent):\n",
    "            if word.head is word:\n",
    "                head_idx = 0\n",
    "            else:\n",
    "                 head_idx = word.head.i + 1\n",
    "            conll.extend((i+1, word.lower_, word.lemma_, word.pos_, word.tag_, \"_\", head_idx, word.dep_, str(head_idx) + \":\"+ word.dep_, \"_\"))\n",
    "            result.append(\" \".join(str(x) for x in conll))\n",
    "            conll = []\n",
    "    return result\n",
    "\n",
    "conll_test = extract_conll_format(doc)\n",
    "conll_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'dependency': 'amod',\n",
       "   'lemma': 'austrailian',\n",
       "   'pos': 'JJ',\n",
       "   'pre': 'ian',\n",
       "   'root': 'scientists',\n",
       "   'text': 'austrailian'}],\n",
       " [{'dependency': 'nsubj',\n",
       "   'lemma': 'scientist',\n",
       "   'pos': 'NNS',\n",
       "   'pre': 'sts',\n",
       "   'root': 'discovers',\n",
       "   'text': 'scientists'}],\n",
       " [{'dependency': 'ROOT',\n",
       "   'lemma': 'discover',\n",
       "   'pos': 'VBZ',\n",
       "   'pre': 'ers',\n",
       "   'root': 'discovers',\n",
       "   'text': 'discovers'}],\n",
       " [{'dependency': 'dobj',\n",
       "   'lemma': 'star',\n",
       "   'pos': 'NN',\n",
       "   'pre': 'tar',\n",
       "   'root': 'discovers',\n",
       "   'text': 'star'}],\n",
       " [{'dependency': 'prep',\n",
       "   'lemma': 'with',\n",
       "   'pos': 'IN',\n",
       "   'pre': 'ith',\n",
       "   'root': 'star',\n",
       "   'text': 'with'}],\n",
       " [{'dependency': 'pobj',\n",
       "   'lemma': 'telescope',\n",
       "   'pos': 'NN',\n",
       "   'pre': 'ope',\n",
       "   'root': 'with',\n",
       "   'text': 'telescope'}]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = [[{\"text\": word.lower_, \"lemma\": word.lemma_, \"pos\": word.tag_, \"dependency\": word.dep_, \"root\": word.head.lower_, \"pre\": word.suffix_}] for word in doc]\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
