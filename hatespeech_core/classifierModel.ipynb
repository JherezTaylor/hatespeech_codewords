{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve parsed collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import models, similarities\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import joblib\n",
    "from modules.db import mongo_base\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import FeatureHasher, DictVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from modules.utils.CustomTwokenizer import CustomTwokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store feature collection as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connection_params = [\"twitter\", \"crowdflower_features\"]\n",
    "# client = mongo_base.connect()\n",
    "# db_name = connection_params[0]\n",
    "# connection_params.insert(0, client)\n",
    "\n",
    "# query = {}\n",
    "# query[\"filter\"] = {}\n",
    "# query[\"projection\"] = {}\n",
    "# query[\"limit\"] = 0\n",
    "# query[\"skip\"] = 0\n",
    "# query[\"no_cursor_timeout\"] = True\n",
    "# cursor = mongo_base.finder(connection_params, query, False)\n",
    "# df = pd.DataFrame(list(cursor))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connection_params = [\"twitter\", \"crowdflower_features_emo\"]\n",
    "# client = mongo_base.connect()\n",
    "# db_name = connection_params[0]\n",
    "# connection_params.insert(0, client)\n",
    "\n",
    "# query = {}\n",
    "# query[\"filter\"] = {}\n",
    "# query[\"projection\"] = {\"emotions\":1}\n",
    "# query[\"limit\"] = 0\n",
    "# query[\"skip\"] = 0\n",
    "# query[\"no_cursor_timeout\"] = True\n",
    "# cursor = mongo_base.finder(connection_params, query, False)\n",
    "# df_emo = pd.DataFrame(list(cursor))\n",
    "# df = pd.DataFrame.merge(df, df_emo, on=\"_id\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle the raw feature collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacy_en_model = \"en_core_web_md\"\n",
    "spacy_glove_model = \"en_vectors_glove_md\"\n",
    "crowdflower_persistence_raw = 'data/persistence/df/crowdflower_features_raw.pkl.compressed'\n",
    "crowdflower_persistence = 'data/persistence/df/crowdflower_features.pkl.compressed'\n",
    "nlp = spacy.load(spacy_en_model, create_make_doc=CustomTwokenizer)\n",
    "# joblib.dump(df, crowdflower_persistence_raw, compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = joblib.load(crowdflower_persistence_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe with classifier features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feat_df = df[['_id', 'text', 'annotation_label', 'hs_keyword_matches', 'hs_keyword_count', 'unknown_words', 'unknown_words_count', 'comment_length', 'brown_cluster_ids', 'feat_dependency_contexts', 'feat_word_dep_root', 'feat_pos_dep_rootPos', 'feat_word_root_rootparent', 'feat_dep_unigrams', 'feat_dep_bigrams', 'feat_dep_trigrams']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>text</th>\n",
       "      <th>annotation_label</th>\n",
       "      <th>hs_keyword_matches</th>\n",
       "      <th>hs_keyword_count</th>\n",
       "      <th>unknown_words</th>\n",
       "      <th>unknown_words_count</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>brown_cluster_ids</th>\n",
       "      <th>feat_dependency_contexts</th>\n",
       "      <th>feat_word_dep_root</th>\n",
       "      <th>feat_pos_dep_rootPos</th>\n",
       "      <th>feat_word_root_rootparent</th>\n",
       "      <th>feat_dep_unigrams</th>\n",
       "      <th>feat_dep_bigrams</th>\n",
       "      <th>feat_dep_trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58c659be6541913eb7f119dd</td>\n",
       "      <td>Warning : penny boards will make you a faggot</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>[faggot]</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>[966, 228, 989, 333, 442, 4618, 602, 19]</td>\n",
       "      <td>[warning_:_punct, warning_make_acl, penny_boar...</td>\n",
       "      <td>[warning_ROOT_warning, penny_compound_boards, ...</td>\n",
       "      <td>[NN_ROOT_NN, NN_compound_NNS, NNS_nsubj_VB, MD...</td>\n",
       "      <td>[warning_warning_warning, penny_boards_make, b...</td>\n",
       "      <td>[warning_warning_ROOT_NN, penny_boards_compoun...</td>\n",
       "      <td>[warning_warning_ROOT_NN | penny_boards_compou...</td>\n",
       "      <td>[warning_warning_ROOT_NN | penny_boards_compou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58c659be6541913eb7f119de</td>\n",
       "      <td>Fuck dykes</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>[fuck_dykes_compoundINV, dykes_fuck_compound]</td>\n",
       "      <td>[fuck_compound_dykes, dykes_ROOT_dykes]</td>\n",
       "      <td>[NNP_compound_VBZ, VBZ_ROOT_VBZ]</td>\n",
       "      <td>[fuck_dykes_dykes, dykes_dykes_dykes]</td>\n",
       "      <td>[fuck_dykes_compound_NNP, dykes_dykes_ROOT_VBZ]</td>\n",
       "      <td>[fuck_dykes_compound_NNP | dykes_dykes_ROOT_VBZ]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58c659be6541913eb7f119df</td>\n",
       "      <td>user_mention user_mention user_mention user_me...</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[faggot]</td>\n",
       "      <td>1</td>\n",
       "      <td>[jefree]</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>[124, 3690, 966, 2442, 1684, 86]</td>\n",
       "      <td>[user_mention_user_mention_compoundINV, user_m...</td>\n",
       "      <td>[user_mention_compound_user_mention, user_ment...</td>\n",
       "      <td>[NN_compound_NN, NN_compound_NN, NN_compound_N...</td>\n",
       "      <td>[user_mention_user_mention_user_mention, user_...</td>\n",
       "      <td>[user_mention_user_mention_compound_NN, user_m...</td>\n",
       "      <td>[user_mention_user_mention_compound_NN | user_...</td>\n",
       "      <td>[user_mention_user_mention_compound_NN | user_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58c659be6541913eb7f119e0</td>\n",
       "      <td>\" user_mention : \" user_mention : user_mention...</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[fag]</td>\n",
       "      <td>1</td>\n",
       "      <td>[neeeee]</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>[228, 228, 1214, 19, 28650, 1014, 981]</td>\n",
       "      <td>[user_mention_\"_punct, user_mention_:_punct, u...</td>\n",
       "      <td>[user_mention_ROOT_user_mention, user_mention_...</td>\n",
       "      <td>[NN_ROOT_NN, NN_appos_NN, NN_nsubj_VBZ, VBZ_ac...</td>\n",
       "      <td>[user_mention_user_mention_user_mention, user_...</td>\n",
       "      <td>[user_mention_user_mention_ROOT_NN, user_menti...</td>\n",
       "      <td>[user_mention_user_mention_ROOT_NN | user_ment...</td>\n",
       "      <td>[user_mention_user_mention_ROOT_NN | user_ment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58c659be6541913eb7f119e1</td>\n",
       "      <td>user_mention You heard me bitch but any way I'...</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>[bitch]</td>\n",
       "      <td>1</td>\n",
       "      <td>[nigga]</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>[858, 26282, 1898, 2485, 148, 12266, 1349, 753...</td>\n",
       "      <td>[user_mention_heard_relcl, user_mention_bitch_...</td>\n",
       "      <td>[user_mention_ROOT_user_mention, you_nsubj_hea...</td>\n",
       "      <td>[NN_ROOT_NN, PRP_nsubj_VBD, VBD_relcl_NN, PRP_...</td>\n",
       "      <td>[user_mention_user_mention_user_mention, you_h...</td>\n",
       "      <td>[user_mention_user_mention_ROOT_NN, you_heard_...</td>\n",
       "      <td>[user_mention_user_mention_ROOT_NN | you_heard...</td>\n",
       "      <td>[user_mention_user_mention_ROOT_NN | you_heard...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  58c659be6541913eb7f119dd   \n",
       "1  58c659be6541913eb7f119de   \n",
       "2  58c659be6541913eb7f119df   \n",
       "3  58c659be6541913eb7f119e0   \n",
       "4  58c659be6541913eb7f119e1   \n",
       "\n",
       "                                                text annotation_label  \\\n",
       "0     Warning : penny boards will make you a faggot     not_offensive   \n",
       "1                                        Fuck dykes        hatespeech   \n",
       "2  user_mention user_mention user_mention user_me...       hatespeech   \n",
       "3  \" user_mention : \" user_mention : user_mention...       hatespeech   \n",
       "4  user_mention You heard me bitch but any way I'...    not_offensive   \n",
       "\n",
       "  hs_keyword_matches  hs_keyword_count unknown_words  unknown_words_count  \\\n",
       "0           [faggot]                 1            []                    0   \n",
       "1                 []                 0            []                    0   \n",
       "2           [faggot]                 1      [jefree]                    1   \n",
       "3              [fag]                 1      [neeeee]                    1   \n",
       "4            [bitch]                 1       [nigga]                    1   \n",
       "\n",
       "   comment_length                                  brown_cluster_ids  \\\n",
       "0               9           [966, 228, 989, 333, 442, 4618, 602, 19]   \n",
       "1               2                                                 []   \n",
       "2              14                   [124, 3690, 966, 2442, 1684, 86]   \n",
       "3              15             [228, 228, 1214, 19, 28650, 1014, 981]   \n",
       "4              20  [858, 26282, 1898, 2485, 148, 12266, 1349, 753...   \n",
       "\n",
       "                            feat_dependency_contexts  \\\n",
       "0  [warning_:_punct, warning_make_acl, penny_boar...   \n",
       "1      [fuck_dykes_compoundINV, dykes_fuck_compound]   \n",
       "2  [user_mention_user_mention_compoundINV, user_m...   \n",
       "3  [user_mention_\"_punct, user_mention_:_punct, u...   \n",
       "4  [user_mention_heard_relcl, user_mention_bitch_...   \n",
       "\n",
       "                                  feat_word_dep_root  \\\n",
       "0  [warning_ROOT_warning, penny_compound_boards, ...   \n",
       "1            [fuck_compound_dykes, dykes_ROOT_dykes]   \n",
       "2  [user_mention_compound_user_mention, user_ment...   \n",
       "3  [user_mention_ROOT_user_mention, user_mention_...   \n",
       "4  [user_mention_ROOT_user_mention, you_nsubj_hea...   \n",
       "\n",
       "                                feat_pos_dep_rootPos  \\\n",
       "0  [NN_ROOT_NN, NN_compound_NNS, NNS_nsubj_VB, MD...   \n",
       "1                   [NNP_compound_VBZ, VBZ_ROOT_VBZ]   \n",
       "2  [NN_compound_NN, NN_compound_NN, NN_compound_N...   \n",
       "3  [NN_ROOT_NN, NN_appos_NN, NN_nsubj_VBZ, VBZ_ac...   \n",
       "4  [NN_ROOT_NN, PRP_nsubj_VBD, VBD_relcl_NN, PRP_...   \n",
       "\n",
       "                           feat_word_root_rootparent  \\\n",
       "0  [warning_warning_warning, penny_boards_make, b...   \n",
       "1              [fuck_dykes_dykes, dykes_dykes_dykes]   \n",
       "2  [user_mention_user_mention_user_mention, user_...   \n",
       "3  [user_mention_user_mention_user_mention, user_...   \n",
       "4  [user_mention_user_mention_user_mention, you_h...   \n",
       "\n",
       "                                   feat_dep_unigrams  \\\n",
       "0  [warning_warning_ROOT_NN, penny_boards_compoun...   \n",
       "1    [fuck_dykes_compound_NNP, dykes_dykes_ROOT_VBZ]   \n",
       "2  [user_mention_user_mention_compound_NN, user_m...   \n",
       "3  [user_mention_user_mention_ROOT_NN, user_menti...   \n",
       "4  [user_mention_user_mention_ROOT_NN, you_heard_...   \n",
       "\n",
       "                                    feat_dep_bigrams  \\\n",
       "0  [warning_warning_ROOT_NN | penny_boards_compou...   \n",
       "1   [fuck_dykes_compound_NNP | dykes_dykes_ROOT_VBZ]   \n",
       "2  [user_mention_user_mention_compound_NN | user_...   \n",
       "3  [user_mention_user_mention_ROOT_NN | user_ment...   \n",
       "4  [user_mention_user_mention_ROOT_NN | you_heard...   \n",
       "\n",
       "                                   feat_dep_trigrams  \n",
       "0  [warning_warning_ROOT_NN | penny_boards_compou...  \n",
       "1                                                 []  \n",
       "2  [user_mention_user_mention_compound_NN | user_...  \n",
       "3  [user_mention_user_mention_ROOT_NN | user_ment...  \n",
       "4  [user_mention_user_mention_ROOT_NN | you_heard...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joblib.dump(feat_df, crowdflower_persistence, compress=True)\n",
    "feat_df = joblib.load(crowdflower_persistence)\n",
    "feat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and test dependency2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = models.KeyedVectors.load_word2vec_format('data/persistence/df/dim200vecs_hs_candidates_exp6', binary=False)\n",
    "def most_similar(word, n):\n",
    "    queries = [w for w in word.vocab if not (word.is_oov or word.is_punct or word.like_num or word.is_stop or word.lower_ == \"rt\") and w.has_vector and w.lower_ != word.lower_ and w.is_lower == word.is_lower and w.prob >= -15]\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    cosine_score = [word.similarity(w) for w in by_similarity]\n",
    "    return by_similarity[:n], cosine_score[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bih', 0.8150119781494141),\n",
       " ('bihh', 0.7981271743774414),\n",
       " ('nicca', 0.783886194229126),\n",
       " ('nigglet', 0.7746115922927856),\n",
       " ('niglet', 0.7723572254180908),\n",
       " ('bish', 0.7560478448867798),\n",
       " ('shorty', 0.7535991668701172),\n",
       " ('btch', 0.7485767602920532),\n",
       " ('nigha', 0.7435488104820251),\n",
       " ('kuz', 0.7415286302566528),\n",
       " ('chick', 0.7407236099243164),\n",
       " ('niggah', 0.7359271049499512),\n",
       " ('buh', 0.7343946695327759),\n",
       " ('spears', 0.7337879538536072),\n",
       " ('nigguh', 0.7332638502120972),\n",
       " ('fuckboy', 0.7324934601783752),\n",
       " ('bint', 0.7315614819526672),\n",
       " ('niggar', 0.7313817739486694),\n",
       " ('jiggaboo', 0.7312814593315125),\n",
       " ('gook', 0.7270585894584656)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_word(\"bitch\", topn=20, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare output to standard word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bitches', 0.86865010008690136)\n",
      "('ass', 0.82214490212007696)\n",
      "('fuck', 0.8190275073922777)\n",
      "('fucker', 0.7869555594649309)\n",
      "('fucking', 0.76995593539211227)\n",
      "('whore', 0.76224389909833756)\n",
      "('slut', 0.75505237485006749)\n",
      "('fuckin', 0.75232905563296448)\n",
      "('asshole', 0.74672725629683701)\n",
      "('dick', 0.74224040821416348)\n",
      "('shit', 0.73805058822422542)\n",
      "('fucked', 0.72900084327361248)\n",
      "('suck', 0.70949127926097444)\n",
      "('chick', 0.70751225632834569)\n",
      "('cunt', 0.70370280187626522)\n",
      "('fuckers', 0.6952213116761421)\n",
      "('motherfucker', 0.68982780300108659)\n",
      "('fucks', 0.68973827279171707)\n",
      "('pussy', 0.68905374236115746)\n",
      "('faggot', 0.68810587898398012)\n"
     ]
    }
   ],
   "source": [
    "word = nlp.vocab[u'bitch']\n",
    "gloVe_result = most_similar(word, 20)\n",
    "for res in zip(gloVe_result[0], gloVe_result[1]):\n",
    "    print((res[0].lower_, res[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>annotation_label</th>\n",
       "      <th>avg_token_length</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>brown_cluster_ids</th>\n",
       "      <th>char_pentagrams</th>\n",
       "      <th>char_quadgrams</th>\n",
       "      <th>char_trigrams</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>conllFormat</th>\n",
       "      <th>...</th>\n",
       "      <th>noun_chunks</th>\n",
       "      <th>pos_dep_rootPos</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>unknown_words</th>\n",
       "      <th>unknown_words_count</th>\n",
       "      <th>uppercase_token_count</th>\n",
       "      <th>word_dep_root</th>\n",
       "      <th>word_root_preRoot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58c659be6541913eb7f119dd</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[Warning penny, penny boards, boards will, wil...</td>\n",
       "      <td>[966, 228, 989, 333, 442, 4618, 602, 19]</td>\n",
       "      <td>[penny, warni, arnin, aggot, board, rning, fag...</td>\n",
       "      <td>[aggo, warn, arni, penn, oard, ards, enny, nin...</td>\n",
       "      <td>[pen, war, nin, mak, enn, you, agg, ggo, fag, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>[1 warning warning NOUN NN _ 0 ROOT 0:ROOT _, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'text': 'warning', 'root': 'warning'}, {'tex...</td>\n",
       "      <td>[{'pos': 'NN', 'dep': 'ROOT', 'rootPos': 'NN'}...</td>\n",
       "      <td>Warning : penny boards will make you a faggot</td>\n",
       "      <td>[warning, penny, boards, faggot]</td>\n",
       "      <td>[Warning penny boards, penny boards will, boar...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'word': 'warning', 'dep': 'ROOT', 'root': 'w...</td>\n",
       "      <td>[{'word': 'warning', 'preRoot': 'warning', 'ro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id annotation_label  avg_token_length  \\\n",
       "0  58c659be6541913eb7f119dd    not_offensive               4.0   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [Warning penny, penny boards, boards will, wil...   \n",
       "\n",
       "                          brown_cluster_ids  \\\n",
       "0  [966, 228, 989, 333, 442, 4618, 602, 19]   \n",
       "\n",
       "                                     char_pentagrams  \\\n",
       "0  [penny, warni, arnin, aggot, board, rning, fag...   \n",
       "\n",
       "                                      char_quadgrams  \\\n",
       "0  [aggo, warn, arni, penn, oard, ards, enny, nin...   \n",
       "\n",
       "                                       char_trigrams  comment_length  \\\n",
       "0  [pen, war, nin, mak, enn, you, agg, ggo, fag, ...               9   \n",
       "\n",
       "                                         conllFormat  \\\n",
       "0  [1 warning warning NOUN NN _ 0 ROOT 0:ROOT _, ...   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "\n",
       "                                         noun_chunks  \\\n",
       "0  [{'text': 'warning', 'root': 'warning'}, {'tex...   \n",
       "\n",
       "                                     pos_dep_rootPos  \\\n",
       "0  [{'pos': 'NN', 'dep': 'ROOT', 'rootPos': 'NN'}...   \n",
       "\n",
       "                                             text  \\\n",
       "0  Warning : penny boards will make you a faggot    \n",
       "\n",
       "                             tokens  \\\n",
       "0  [warning, penny, boards, faggot]   \n",
       "\n",
       "                                            trigrams unknown_words  \\\n",
       "0  [Warning penny boards, penny boards will, boar...            []   \n",
       "\n",
       "  unknown_words_count  uppercase_token_count  \\\n",
       "0                   0                      0   \n",
       "\n",
       "                                       word_dep_root  \\\n",
       "0  [{'word': 'warning', 'dep': 'ROOT', 'root': 'w...   \n",
       "\n",
       "                                   word_root_preRoot  \n",
       "0  [{'word': 'warning', 'preRoot': 'warning', 'ro...  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:1]['dep_bigrams'][0][0]\n",
    "hasher = FeatureHasher(input_type='string', non_negative=True)\n",
    "v = DictVectorizer(sparse=False)\n",
    "# raw_X = hasher.transform(df[0:1]['brown_cluster_ids'])\n",
    "df[0:1]['bigrams'][0][0]\n",
    "D = [{'bigram': 'Warning penny'}, {'bigram': 'penny boards'}, {'bigram': 'boards will'}, {'bigram': 'boards will'}]\n",
    "trans = v.fit_transform(D)\n",
    "v.inverse_transform(trans)\n",
    "v.get_feature_names()\n",
    "# raw_X.toarray()\n",
    "# print(df[0:1]['bigrams'][0])\n",
    "df[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's pick the same random 10% of the data to train with\n",
    "train_test_set = feat_df.sample(n=int(len(feat_df) / 2), random_state=1965)\n",
    "\n",
    "X = train_test_set['text']\n",
    "y = train_test_set['annotation_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the labels of the test set...\n",
      "7254 documents\n",
      "2 categories\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting the labels of the test set...\")\n",
    "print(\"%d documents\" % len(X))\n",
    "print(\"%d categories\" % len(y.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup generic model experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(X, y, pipeline, process_name, num_expts=1):\n",
    "    scores = list()\n",
    "    for i in tqdm(range(num_expts)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "        model = pipeline.fit(X_train, y_train)  # train the classifier\n",
    "        y_prediction = model.predict(X_test)          # apply the model to the test data\n",
    "        report = classification_report(y_test, y_prediction)\n",
    "        score = accuracy_score(y_prediction, y_test)  # compare the results to the gold standard\n",
    "        scores.append(score)\n",
    "        print(\"Classification Report: \" + process_name)\n",
    "        print(report)\n",
    "        cm = confusion_matrix(y_test, y_prediction)\n",
    "        print(\"Confusion matrix:\")\n",
    "        print(cm)\n",
    "    print(sum(scores) / num_expts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup naive baseline classification (countVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: CountVectorizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.48      0.34      0.40       308\n",
      "not_offensive       0.87      0.92      0.90      1506\n",
      "\n",
      "  avg / total       0.81      0.83      0.81      1814\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 105  203]\n",
      " [ 114 1392]]\n",
      "0.825248070562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "# our two ingredients: the ngram counter and the classifier\n",
    "nm = 5000\n",
    "vect = CountVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "ch2 = SelectKBest(chi2, k=nm)\n",
    "\n",
    "# There are just two steps to our process: extracting the ngrams and\n",
    "# putting them through the classifier. So our Pipeline looks like this:\n",
    "\n",
    "count_pipeline = Pipeline([\n",
    "    ('vect', vect),  # extract ngrams from tweet text\n",
    "    ('kBest', ch2),\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "\n",
    "# Run the classifcation\n",
    "run_experiment(X, y, count_pipeline, \"CountVectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup naive baseline classification (hashingVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: HashingVectorizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.57      0.29      0.39       344\n",
      "not_offensive       0.85      0.95      0.90      1470\n",
      "\n",
      "  avg / total       0.80      0.82      0.80      1814\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 101  243]\n",
      " [  75 1395]]\n",
      "0.824696802646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vect = HashingVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "\n",
    "hashing_pipeline = Pipeline([\n",
    "    ('vect', vect),  # extract ngrams from tweet text\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "run_experiment(X, y, hashing_pipeline, \"HashingVectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup tf-idf baseline classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: TfidfVectorizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.53      0.32      0.40       290\n",
      "not_offensive       0.88      0.95      0.91      1524\n",
      "\n",
      "  avg / total       0.82      0.85      0.83      1814\n",
      "\n",
      "Confusion matrix:\n",
      "[[  93  197]\n",
      " [  83 1441]]\n",
      "0.845644983462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(3,5), analyzer='char')\n",
    "clf = LinearSVC()\n",
    "\n",
    "tfidf_pipeline = Pipeline([\n",
    "    ('vect', vect),  # extract ngrams from tweet text\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "run_experiment(X, y, tfidf_pipeline, \"TfidfVectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report : Random Experiment\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.56      0.07      0.12       313\n",
      "not_offensive       0.84      0.99      0.91      1501\n",
      "\n",
      "  avg / total       0.79      0.83      0.77      1814\n",
      "\n",
      "Confusion matrix:\n",
      "[[  22  291]\n",
      " [  17 1484]]\n"
     ]
    }
   ],
   "source": [
    "# As much as I want to ignore this, I shouldn't. The fact that the precision score is\n",
    "# close to the other experiments is troubling\n",
    "\n",
    "X = train_test_set[['hs_keyword_count', 'comment_length', 'unknown_words_count']]\n",
    "y = train_test_set['annotation_label']\n",
    "clf = LinearSVC()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "model = clf.fit(X_train, y_train)  # train the classifier\n",
    "y_prediction = model.predict(X_test)          # apply the model to the test data\n",
    "report = classification_report(y_test, y_prediction)\n",
    "print(\"Classification Report : Random Experiment\")\n",
    "print(report)\n",
    "cm = confusion_matrix(y_test, y_prediction)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency tuple experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "X = train_test_set['word_root_preRoot']\n",
    "y = train_test_set['annotation_label']\n",
    "clf = LinearSVC()\n",
    "\n",
    "# http://www.markhneedham.com/blog/2015/03/02/python-scikit-learn-training-a-classifier-with-non-numeric-features/\n",
    "empty_analyzer = lambda x: x\n",
    "vect = DictVectorizer()\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "# vect = TfidfVectorizer(analyzer=empty_analyzer)\n",
    "# hasher = FeatureHasher(input_type='string', non_negative=True)\n",
    "# X = [[str(res) for res in tmp] for tmp in X]# \n",
    "# X = hasher.transform(X)\n",
    "X = X.tolist()\n",
    "X = vect.fit_transform([item[0] for item in X]).toarray()\n",
    "# X = transformer.fit_transform(X)\n",
    "# vect.vocabulary_\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "model = clf.fit(X_train, y_train)  # train the classifier\n",
    "y_prediction = model.predict(X_test)          # apply the model to the test data\n",
    "report = classification_report(y_test, y_prediction)\n",
    "print(\"Classification Report : Random Experiment\")\n",
    "print(report)\n",
    "cm = confusion_matrix(y_test, y_prediction)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency context experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "class TextExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Adapted from code by @zacstewart \n",
    "       https://github.com/zacstewart/kaggle_seeclickfix/blob/master/estimator.py\n",
    "       Also see Zac Stewart's excellent blogpost on pipelines:\n",
    "       http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "       \"\"\"\n",
    "    \n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def transform(self, df):\n",
    "        # select the relevant column and return it as a numpy array\n",
    "        # set the array type to be string\n",
    "        return df[self.column_name].tolist()\n",
    "#         return np.asarray(df[self.column_name]).astype(str)\n",
    "        \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "\n",
    "class Apply(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies a function f element-wise to the numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fn):\n",
    "        self.fn = np.vectorize(fn)\n",
    "        \n",
    "    def transform(self, data):\n",
    "        # note: reshaping is necessary because otherwise sklearn\n",
    "        # interprets 1-d array as a single sample\n",
    "        return self.fn(data.reshape(data.size, 1))\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "class BooleanExtractor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def transform(self, df):\n",
    "        # select the relevant column and return it as a numpy array\n",
    "        # set the array type to be string\n",
    "        return np.asarray(df[self.column_name]).astype(np.int)\n",
    "                                                       \n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: TfidfVectorizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   hatespeech       0.54      0.30      0.38       305\n",
      "not_offensive       0.87      0.95      0.91      1509\n",
      "\n",
      "  avg / total       0.81      0.84      0.82      1814\n",
      "\n",
      "Confusion matrix:\n",
      "[[  90  215]\n",
      " [  76 1433]]\n",
      "0.839581036384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "X = train_test_set[['feat_dependency_contexts', 'hs_keyword_count']]\n",
    "y = train_test_set['annotation_label']\n",
    "clf = LinearSVC()\n",
    "empty_analyzer = lambda x: x\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "vect = TfidfVectorizer(analyzer=empty_analyzer)\n",
    "\n",
    "dependency_context_pipeline = Pipeline([\n",
    "    ('dep_extractor', TextExtractor('feat_dependency_contexts')), # extract names from df\n",
    "    ('vect', vect)\n",
    "])\n",
    "\n",
    "hs_keyword_count_pipeline = Pipeline([\n",
    "    ('count_extractor', BooleanExtractor('hs_keyword_count')),\n",
    "    ('identity', Apply(lambda x: x))\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('all_features', FeatureUnion([\n",
    "        ('dependency_context_pipeline', dependency_context_pipeline), # all text features\n",
    "        ('hs_keyword_count_pipeline', hs_keyword_count_pipeline),\n",
    "    ])),\n",
    "    ('clf' , clf),   # feed the output through a classifier\n",
    "])\n",
    "\n",
    "run_experiment(X, y, pipeline, \"TfidfVectorizer\")\n",
    "\n",
    "# https://github.com/michelleful/SingaporeRoadnameOrigins/blob/master/notebooks/04%20Adding%20features%20with%20Pipelines.ipynb\n",
    "# https://github.com/amueller/kaggle_insults/blob/e4abac805be1d1e2b3201a978172bafd36cc01e3/features.py\n",
    "# http://www.markhneedham.com/blog/2015/03/02/python-scikit-learn-training-a-classifier-with-non-numeric-features/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract dependency contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austrailian_scientists_amodINV',\n",
       " 'scientists_austrailian_amod',\n",
       " 'scientists_discovers_nsubjINV',\n",
       " 'discovers_scientists_nsubj',\n",
       " 'discovers_star_dobj',\n",
       " 'star_with_prep',\n",
       " 'star_discovers_dobjINV',\n",
       " 'with_telescope_pobj',\n",
       " 'with_star_prepINV',\n",
       " 'telescope_with_pobjINV']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string = \"Austrailian scientists discovers star with telescope.\"\n",
    "doc = nlp(test_string)\n",
    "\n",
    "# elements = []\n",
    "# dict1 = {\"austrailian\": \"scientists amod\"}\n",
    "# dict2 = {\"scientists\": \"austrailian amod\"}\n",
    "# dict3 = {\"scientists\": \"discovers nsubj\"}\n",
    "\n",
    "# elements.append(dict1)\n",
    "# elements.append(dict2)\n",
    "# elements.append(dict3)\n",
    "\n",
    "# vect = DictVectorizer()\n",
    "# X = vect.fit_transform(elements)\n",
    "# X\n",
    "# vect.inverse_transform(X)\n",
    "\n",
    "def extract_dep_context(doc):\n",
    "    dependency_contexts = []\n",
    "    dependency_contexts_concat = []\n",
    "    for word in doc:\n",
    "#         if (not str(word.head.prefix_).isalpha() or not str(word.prefix_).isalpha() or \".\" in word.text or \".\" in str(word.head)):\n",
    "#             pass\n",
    "        if word.is_punct:\n",
    "            pass\n",
    "        elif len(list(word.children)) == 0:\n",
    "            dependency_contexts.append({word.lower_: str(word.head.lower_) + \" \" + str(word.dep_) + \"INV\"})\n",
    "            dependency_contexts_concat.append(word.lower_+ \"_\" + word.head.lower_ + \"_\" + str(word.dep_) + \"INV\")\n",
    "        elif len(list(word.children)) >= 1:\n",
    "            for child in word.children:\n",
    "                if child.lower_ != word.lower_ and not child.is_punct:\n",
    "                    dependency_contexts.append({word.lower_: str(child.lower_) + \" \" + str(child.dep_)})\n",
    "                    dependency_contexts_concat.append(word.lower_ + \"_\" + child.lower_ + \"_\" + str(child.dep_))\n",
    "            if word.dep_ != \"ROOT\":\n",
    "                dependency_contexts.append({word.lower_: word.head.lower_ + \" \" + str(word.dep_) + \"INV\"})\n",
    "                dependency_contexts_concat.append(word.lower_ + \"_\" + word.head.lower_ + \"_\" + str(word.dep_) + \"INV\")\n",
    "    return dependency_contexts, dependency_contexts_concat\n",
    "dependency_contexts = extract_dep_context(doc)[1]\n",
    "dependency_contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract conll format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 austrailian austrailian ADJ JJ _ 2 amod 2:amod _',\n",
       " '2 scientists scientist NOUN NNS _ 3 nsubj 3:nsubj _',\n",
       " '3 discovers discover VERB VBZ _ 0 ROOT 0:ROOT _',\n",
       " '4 star star NOUN NN _ 3 dobj 3:dobj _',\n",
       " '5 with with ADP IN _ 4 prep 4:prep _',\n",
       " '6 telescope telescope NOUN NN _ 5 pobj 5:pobj _']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/explosion/spaCy/issues/533#issuecomment-254774296\n",
    "def extract_conll_format(doc):\n",
    "    result = []\n",
    "    conll = []\n",
    "    for sent in doc.sents:\n",
    "        for i, word in enumerate(sent):\n",
    "            if word.head is word:\n",
    "                head_idx = 0\n",
    "            else:\n",
    "                 head_idx = word.head.i + 1\n",
    "            conll.extend((i+1, word.lower_, word.lemma_, word.pos_, word.tag_, \"_\", head_idx, word.dep_, str(head_idx) + \":\"+ word.dep_, \"_\"))\n",
    "            result.append(\" \".join(str(x) for x in conll))\n",
    "            conll = []\n",
    "    return result\n",
    "\n",
    "conll_test = extract_conll_format(doc)\n",
    "conll_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create conll format file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"hatespeech_dep_conll\", \"w+\") as conll_file:\n",
    "    for index, row in df.iterrows():\n",
    "        for entry in row[\"conllFormat\"]:\n",
    "            conll_file.write(entry+\"\\n\")\n",
    "        conll_file.write(\"\\n\")\n",
    "conll_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
