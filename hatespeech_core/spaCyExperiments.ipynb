{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from itertools import chain\n",
    "import joblib\n",
    "import plotly as py\n",
    "import cufflinks as cf\n",
    "import plotly.graph_objs as go\n",
    "from modules.utils import file_ops\n",
    "from modules.utils import EmotionDetection\n",
    "from modules.utils.CustomTwokenizer import CustomTwokenizer\n",
    "from modules.utils import settings\n",
    "from modules.pattern_classifier import SimpleClassifier, PatternVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emotion_api = EmotionDetection.EmotionDetection()\n",
    "py.offline.init_notebook_mode(connected=True)\n",
    "# cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \u001b[93mInfo about model en_core_web_md\u001b[0m\n",
      "\n",
      "    lang               en             \n",
      "    url                https://explosion.ai\n",
      "    description        General-purpose English model, with tagging, parsing, entities and word vectors\n",
      "    source             /home/jherez/Dev/thesis-preprocessing/venv/lib/python3.5/site-packages/en_core_web_md/en_core_web_md-1.2.1\n",
      "    spacy_version      >=1.7.0,<2.0.0 \n",
      "    name               core_web_md    \n",
      "    version            1.2.1          \n",
      "    license            CC BY-SA 3.0   \n",
      "    email              contact@explosion.ai\n",
      "    author             Explosion AI   \n",
      "    link               /home/jherez/Dev/thesis-preprocessing/venv/lib/python3.5/site-packages/spacy/data/en_core_web_md\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spacy_model = \"en_core_web_md\"\n",
    "nlp = spacy.load(spacy_model, create_make_doc=CustomTwokenizer)\n",
    "spacy.info(spacy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Persisted models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cls_persistence = 'data/persistence/simple_classifier_model.pkl.compressed'\n",
    "# pv_persistence = 'data/persistence/pattern_vectorizer.pkl.compressed'\n",
    "# cls = joblib.load(cls_persistence)\n",
    "# pv = joblib.load(pv_persistence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CrowdFlower dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2399, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/datasets/twitter-hate-speech-classifier.csv', encoding='utf-8')\n",
    "contains_hatespeech = df.loc[df['does_this_tweet_contain_hate_speech'] == 'The tweet contains hate speech']\n",
    "contains_hatespeech = contains_hatespeech[['_unit_id', '_unit_state', '_trusted_judgments', 'does_this_tweet_contain_hate_speech:confidence', 'tweet_id', 'tweet_text']]\n",
    "contains_hatespeech.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize and classify tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hatespeech_subsample = contains_hatespeech[['tweet_text']][0:20]\n",
    "hatespeech_vec = pv.transform(hatespeech_subsample['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fuck dykes</td>\n",
       "      <td>[disgust, anticipation, joy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@sizzurp__ @ILIKECATS74 @yoPapi_chulo @brandon...</td>\n",
       "      <td>[fear, sadness, surprise]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"@jayswaggkillah: \"@JacklynAnnn: @jayswaggkill...</td>\n",
       "      <td>[anticipation, surprise, joy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@elaynay your a dirty terrorist and your relig...</td>\n",
       "      <td>[anger, surprise, fear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @ivanrabago_: @_WhitePonyJr_ looking like f...</td>\n",
       "      <td>[fear, joy, surprise]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                         Fuck dykes   \n",
       "1  @sizzurp__ @ILIKECATS74 @yoPapi_chulo @brandon...   \n",
       "2  \"@jayswaggkillah: \"@JacklynAnnn: @jayswaggkill...   \n",
       "3  @elaynay your a dirty terrorist and your relig...   \n",
       "4  RT @ivanrabago_: @_WhitePonyJr_ looking like f...   \n",
       "\n",
       "                        emotions  \n",
       "0   [disgust, anticipation, joy]  \n",
       "1      [fear, sadness, surprise]  \n",
       "2  [anticipation, surprise, joy]  \n",
       "3        [anger, surprise, fear]  \n",
       "4          [fear, joy, surprise]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can return up to 8 emotions\n",
    "result = []\n",
    "HS_GUESS = cls.get_top_classes(hatespeech_vec, ascending=True, n=3)\n",
    "for doc in range(0,len(hatespeech_subsample)):\n",
    "    result.append((hatespeech_subsample['tweet_text'].iloc[doc], HS_GUESS[doc]))\n",
    "result_frame = pd.DataFrame(result, columns=('text', 'emotions'))\n",
    "result_frame[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs_text = result_frame['text'].iloc[12]\n",
    "doc = nlp(hs_text)\n",
    "nlp.vocab.strings[doc[16].orth_]\n",
    "[ent.label_ for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['these', 'DET', 'DT'], ['fucking', 'VERB', 'VBG'], ['skypes', 'NOUN', 'NNS']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string = \"these fucking skypes\"\n",
    "doc = nlp(test_string)\n",
    "[[token.lower_, token.pos_, token.tag_] for token in doc if not(token.is_punct)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most similar words to NASA:\n",
      "ufo\n",
      "hubble\n",
      "apollo\n",
      "yung\n",
      "ang\n"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "nasa = nlp.vocab[u'nasa']\n",
    "\n",
    "# cosine similarity\n",
    "cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "\n",
    "# gather all known words, take only the lowercased versions\n",
    "allWords = list({w for w in nlp.vocab if w.has_vector and w.orth_.islower() and w.lower_ != \"nasa\"})\n",
    "\n",
    "# sort by similarity to NASA\n",
    "allWords.sort(key=lambda w: cosine(w.vector, nasa.vector))\n",
    "allWords.reverse()\n",
    "print(\"Top 10 most similar words to NASA:\")\n",
    "for word in allWords[:5]:\n",
    "    print(word.orth_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'dependency': 'det',\n",
       "   'lemma': 'these',\n",
       "   'pos': 'DT',\n",
       "   'pre': 'ese',\n",
       "   'root': 'skypes',\n",
       "   'text': 'these'}],\n",
       " [{'dependency': 'amod',\n",
       "   'lemma': 'fuck',\n",
       "   'pos': 'VBG',\n",
       "   'pre': 'ing',\n",
       "   'root': 'skypes',\n",
       "   'text': 'fucking'}],\n",
       " [{'dependency': 'ROOT',\n",
       "   'lemma': 'skypes',\n",
       "   'pos': 'NNS',\n",
       "   'pre': 'pes',\n",
       "   'root': 'skypes',\n",
       "   'text': 'skypes'}]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = [[{\"text\": word.lower_, \"lemma\": word.lemma_, \"pos\": word.tag_, \"dependency\": word.dep_, \"root\": word.head.lower_, \"pre\": word.suffix_}] for word in doc]\n",
    "nounphrases = [{\"text\":np.text.lower(),\"root\": np.root.head.text.lower()} for np in doc.noun_chunks]\n",
    "nounphrases\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_ids = [tok.cluster for tok in doc if tok.cluster != 0]\n",
    "tokens = [token for token in doc if not(token.is_stop or token.is_punct or token.lower_ == \"rt\" or token.is_digit)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ufo\n",
      "apollo\n",
      "yung\n",
      "ang\n",
      "ako\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/explosion/spaCy/issues/276\n",
    "from nltk.corpus import words as nltk_words\n",
    "def most_similar(word):\n",
    "    queries = [w for w in word.vocab if not (word.is_oov or word.is_punct or word.like_num or word.is_stop or word.lower_ == \"rt\") and w.has_vector and w.lower_ != word.lower_ and w.is_lower == word.is_lower and w.prob >= -15]\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return by_similarity[:5]\n",
    "\n",
    "def get_keywords(doc):\n",
    "    result = set()\n",
    "    for token in doc.noun_chunks:\n",
    "        if ' ' in token.text:\n",
    "            split = token.text.split(\" \")\n",
    "            for tok in split:\n",
    "                result.add(tok)\n",
    "        else:\n",
    "            get_doc_token = doc[token.start]\n",
    "            if not (get_doc_token.is_oov or get_doc_token.is_punct or get_doc_token.like_num or get_doc_token.is_stop or get_doc_token.lower_ == \"rt\"):\n",
    "                result.add(get_doc_token.lower_)\n",
    "    return result\n",
    "            \n",
    "keywords = get_keywords(doc)\n",
    "# [[w.lower_ for w in most_similar(nlp.vocab[token])] for token in keywords if token != 'the']\n",
    "def count_upper_case_tokens(doc):\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if token.text.isupper() and len(token) != 1:\n",
    "            count += 1\n",
    "    return count\n",
    "count_upper_case_tokens(doc)\n",
    "result = most_similar(nlp.vocab[u'nasa'])\n",
    "for res in result:\n",
    "    print(res.lower_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'disgust'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "test = [1,2,3,4,6]\n",
    "s = pd.Series(test, index=range(0, len(test)))\n",
    "HS_GUESS[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet = [{'lemma': 'warning', 'root': 'warning', 'pos': 'NN', 'dependency': 'ROOT', 'text': 'warning'}, {'lemma': 'penny', 'root': 'boards', 'pos': 'NN', 'dependency': 'compound', 'text': 'penny'}\n",
    ", {'lemma': 'board', 'root': 'make', 'pos': 'NNS', 'dependency': 'nsubj', 'text': 'boards'}, {'lemma': 'will', 'root': 'make', 'pos': 'MD', 'dependency': 'aux', 'text': 'will'}, {'lemma':\n",
    "'make', 'root': 'warning', 'pos': 'VB', 'dependency': 'acl', 'text': 'make'}, {'lemma': 'you', 'root': 'faggot', 'pos': 'PRP', 'dependency': 'nsubj', 'text': 'you'}, {'lemma': 'a', 'root':\n",
    " 'faggot', 'pos': 'DT', 'dependency': 'det', 'text': 'a'}, {'lemma': 'faggot', 'root': 'make', 'pos': 'NN', 'dependency': 'ccomp', 'text': 'faggot'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['warning_warning_ROOT_NN | penny_boards_compound_NN | boards_make_nsubj_NNS',\n",
       " 'penny_boards_compound_NN | boards_make_nsubj_NNS | will_make_aux_MD',\n",
       " 'boards_make_nsubj_NNS | will_make_aux_MD | make_warning_acl_VB',\n",
       " 'will_make_aux_MD | make_warning_acl_VB | you_faggot_nsubj_PRP',\n",
       " 'make_warning_acl_VB | you_faggot_nsubj_PRP | a_faggot_det_DT',\n",
       " 'you_faggot_nsubj_PRP | a_faggot_det_DT | faggot_make_ccomp_NN']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dep_ngrams(dependency_list, length):\n",
    "    dependencies = [[dependency[\"text\"], dependency[\"root\"], dependency[\"dependency\"], dependency[\"pos\"]] for dependency in dependency_list]\n",
    "    for idx, dep in enumerate(dependencies):\n",
    "        dependencies[idx] = \"_\".join(_ele for _ele in dep)\n",
    "    \n",
    "    dependencies = list(map(list, zip(*[dependencies[i:] for i in range(length)])))\n",
    "    for idx, dep in enumerate(dependencies):\n",
    "        dependencies[idx] = \" | \".join(_ele for _ele in dep)\n",
    "    return dependencies\n",
    "test = create_dep_ngrams(tweet, 3)\n",
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
